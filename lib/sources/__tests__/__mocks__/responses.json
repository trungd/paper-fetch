{"https://export.arxiv.org/api/query?id_list=1905.11946&start=0&max_results=1&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1905.11946%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1905.11946&amp;start=0&amp;max_results=1</title>\n  <id>http://arxiv.org/api/DBFa7FfkqKf1NXWs07+dYuYd61E</id>\n  <updated>2022-09-18T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1905.11946v5</id>\n    <updated>2020-09-11T05:08:01Z</updated>\n    <published>2019-05-28T17:05:32Z</published>\n    <title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>\n    <summary>  Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\n</summary>\n    <author>\n      <name>Mingxing Tan</name>\n    </author>\n    <author>\n      <name>Quoc V. Le</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML 2019</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning, 2019</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/1905.11946v5\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.11946v5\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://export.arxiv.org/api/query?id_list=1706.03762&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1706.03762%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1706.03762&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/zUwBFJ+vAUSpXAR7QFveSY/bZos</id>\n  <updated>2022-09-18T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1706.03762v5</id>\n    <updated>2017-12-06T03:30:32Z</updated>\n    <published>2017-06-12T17:57:34Z</published>\n    <title>Attention Is All You Need</title>\n    <summary>  The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n</summary>\n    <author>\n      <name>Ashish Vaswani</name>\n    </author>\n    <author>\n      <name>Noam Shazeer</name>\n    </author>\n    <author>\n      <name>Niki Parmar</name>\n    </author>\n    <author>\n      <name>Jakob Uszkoreit</name>\n    </author>\n    <author>\n      <name>Llion Jones</name>\n    </author>\n    <author>\n      <name>Aidan N. Gomez</name>\n    </author>\n    <author>\n      <name>Lukasz Kaiser</name>\n    </author>\n    <author>\n      <name>Illia Polosukhin</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 5 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1706.03762v5\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1706.03762v5\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://export.arxiv.org/api/query?search_query=imagenet&start=0&max_results=2&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dimagenet%26id_list%3D%26start%3D0%26max_results%3D2\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=imagenet&amp;id_list=&amp;start=0&amp;max_results=2</title>\n  <id>http://arxiv.org/api/y6ZzsrV2bbucibpH84pVrWG/7Lk</id>\n  <updated>2022-09-18T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">4394</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/2205.10660v1</id>\n    <updated>2022-05-21T19:48:28Z</updated>\n    <published>2022-05-21T19:48:28Z</published>\n    <title>Vision Transformers in 2022: An Update on Tiny ImageNet</title>\n    <summary>  The recent advances in image transformers have shown impressive results and\nhave largely closed the gap between traditional CNN architectures. The standard\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\nImageNet-1k. After finetuning, researches will often consider the transfer\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\nTiny ImageNet. This paper offers an update on vision transformers' performance\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\nresult with a validation accuracy of 91.35%. Code is available here:\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\n</summary>\n    <author>\n      <name>Ethan Huynh</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2205.10660v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.10660v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2006.07159v1</id>\n    <updated>2020-06-12T13:17:25Z</updated>\n    <published>2020-06-12T13:17:25Z</published>\n    <title>Are we done with ImageNet?</title>\n    <summary>  Yes, and no. We ask whether recent progress on the ImageNet classification\nbenchmark continues to represent meaningful generalization, or whether the\ncommunity has started to overfit to the idiosyncrasies of its labeling\nprocedure. We therefore develop a significantly more robust procedure for\ncollecting human annotations of the ImageNet validation set. Using these new\nlabels, we reassess the accuracy of recently proposed ImageNet classifiers, and\nfind their gains to be substantially smaller than those reported on the\noriginal labels. Furthermore, we find the original ImageNet labels to no longer\nbe the best predictors of this independently-collected set, indicating that\ntheir usefulness in evaluating vision models may be nearing an end.\nNevertheless, we find our annotation procedure to have largely remedied the\nerrors in the original labels, reinforcing ImageNet as a powerful benchmark for\nfuture research in visual recognition.\n</summary>\n    <author>\n      <name>Lucas Beyer</name>\n    </author>\n    <author>\n      <name>Olivier J. Hénaff</name>\n    </author>\n    <author>\n      <name>Alexander Kolesnikov</name>\n    </author>\n    <author>\n      <name>Xiaohua Zhai</name>\n    </author>\n    <author>\n      <name>Aäron van den Oord</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">All five authors contributed equally. New labels at\n  https://github.com/google-research/reassessed-imagenet</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2006.07159v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.07159v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://export.arxiv.org/api/query?id_list=1706.03762&start=1&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1706.03762%26start%3D1%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1706.03762&amp;start=1&amp;max_results=10</title>\n  <id>http://arxiv.org/api/SubSPlGg5kAUM3z6/LV1d8vbbJw</id>\n  <updated>2022-09-18T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n</feed>\n","https://api.crossref.org/works/10.1016/s0021-9258(19)52451-6":"{\"status\":\"ok\",\"message-type\":\"work\",\"message-version\":\"1.0.0\",\"message\":{\"indexed\":{\"date-parts\":[[2022,9,18]],\"date-time\":\"2022-09-18T22:26:01Z\",\"timestamp\":1663539961054},\"reference-count\":25,\"publisher\":\"Elsevier BV\",\"issue\":\"1\",\"license\":[{\"start\":{\"date-parts\":[[1951,11,1]],\"date-time\":\"1951-11-01T00:00:00Z\",\"timestamp\":-573350400000},\"content-version\":\"tdm\",\"delay-in-days\":0,\"URL\":\"https:\\/\\/www.elsevier.com\\/tdm\\/userlicense\\/1.0\\/\"},{\"start\":{\"date-parts\":[[2020,10,7]],\"date-time\":\"2020-10-07T00:00:00Z\",\"timestamp\":1602028800000},\"content-version\":\"vor\",\"delay-in-days\":25178,\"URL\":\"http:\\/\\/creativecommons.org\\/licenses\\/by\\/4.0\\/\"}],\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"short-container-title\":[\"Journal of Biological Chemistry\"],\"published-print\":{\"date-parts\":[[1951,11]]},\"DOI\":\"10.1016\\/s0021-9258(19)52451-6\",\"type\":\"journal-article\",\"created\":{\"date-parts\":[[2021,1,6]],\"date-time\":\"2021-01-06T16:45:28Z\",\"timestamp\":1609951528000},\"page\":\"265-275\",\"source\":\"Crossref\",\"is-referenced-by-count\":233233,\"title\":[\"PROTEIN MEASUREMENT WITH THE FOLIN PHENOL REAGENT\"],\"prefix\":\"10.1016\",\"volume\":\"193\",\"author\":[{\"given\":\"OliverH.\",\"family\":\"Lowry\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"NiraJ.\",\"family\":\"Rosebrough\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"A. Lewis\",\"family\":\"Farr\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"RoseJ.\",\"family\":\"Randall\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"78\",\"reference\":[{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib1\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"33\",\"DOI\":\"10.1016\\/S0021-9258(18)85898-7\",\"volume\":\"51\",\"author\":\"Wu\",\"year\":\"1922\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib2\",\"first-page\":\"161\",\"volume\":\"1\",\"author\":\"Wu\",\"year\":\"1927\",\"journal-title\":\"Chinese J. Physiol\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib3\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"545\",\"DOI\":\"10.1016\\/S0021-9258(20)78300-6\",\"volume\":\"82\",\"author\":\"Greenberg\",\"year\":\"1929\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib4\",\"first-page\":\"816\",\"volume\":\"18\",\"author\":\"Andersch\",\"year\":\"1933\",\"journal-title\":\"J. Lab. and Clin. Med\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib5\",\"first-page\":\"431\",\"volume\":\"21\",\"author\":\"Greenberg\",\"year\":\"1936\",\"journal-title\":\"J. Lab. and Clin. Med\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib6\",\"first-page\":\"743\",\"volume\":\"21\",\"author\":\"Minot\",\"year\":\"1936\",\"journal-title\":\"J. Lab. and Clin. Med\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib7\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"357\",\"DOI\":\"10.1021\\/i560117a023\",\"volume\":\"15\",\"author\":\"Pressman\",\"year\":\"1943\",\"journal-title\":\"Ind. and Eng. Chem., Anal. Ed\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib8\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"405\",\"DOI\":\"10.1126\\/science.97.2522.405\",\"volume\":\"97\",\"author\":\"Heidelberger\",\"year\":\"1943\",\"journal-title\":\"Science\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib9\",\"unstructured\":\"Kabat, E. A., and Mayer, M. M., Experimental immunochemistry, Springfield, 321 (1948).\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib10\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"825\",\"DOI\":\"10.1016\\/S0021-9258(18)56702-8\",\"volume\":\"180\",\"author\":\"Sutherland\",\"year\":\"1949\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib11\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"627\",\"DOI\":\"10.1016\\/S0021-9258(18)84277-6\",\"volume\":\"73\",\"author\":\"Folin\",\"year\":\"1927\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib12\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"633\",\"DOI\":\"10.1016\\/S0021-9258(17)41291-9\",\"volume\":\"163\",\"author\":\"Lowry\",\"year\":\"1946\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib13\",\"first-page\":\"101\",\"volume\":\"21\",\"author\":\"Levy\",\"year\":\"1945\",\"journal-title\":\"Compt.-rend. trav. Lab. Carlsberg, S\\u00e9rie chim\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib14\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"321\",\"DOI\":\"10.1016\\/S0021-9258(18)43072-4\",\"volume\":\"164\",\"author\":\"Bessey\",\"year\":\"1946\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib15\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"177\",\"DOI\":\"10.1016\\/S0021-9258(17)34996-7\",\"volume\":\"166\",\"author\":\"Bessey\",\"year\":\"1946\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib16\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"239\",\"DOI\":\"10.1016\\/S0021-9258(18)88697-5\",\"volume\":\"12\",\"author\":\"Folin\",\"year\":\"1912\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib17\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"283\",\"DOI\":\"10.1085\\/jgp.19.2.283\",\"volume\":\"19\",\"author\":\"Herriott\",\"year\":\"1935\",\"journal-title\":\"J. Gen. Physiol\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib18\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"642\",\"DOI\":\"10.3181\\/00379727-46-12091\",\"volume\":\"46\",\"author\":\"Herriott\",\"year\":\"1941\",\"journal-title\":\"Proc. Soc. Exp. Biol. and Med\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib19\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"13\",\"DOI\":\"10.1016\\/S0021-9258(18)57051-4\",\"volume\":\"177\",\"author\":\"Mehl\",\"year\":\"1949\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib20\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"755\",\"DOI\":\"10.1016\\/S0021-9258(18)76024-9\",\"volume\":\"99\",\"author\":\"Rising\",\"year\":\"1932\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib21\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"843\",\"DOI\":\"10.1016\\/S0021-9258(18)72955-4\",\"volume\":\"139\",\"author\":\"Hitchings\",\"year\":\"1941\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib22\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"356\",\"DOI\":\"10.1042\\/bj0070356\",\"volume\":\"7\",\"author\":\"Funk\",\"year\":\"1913\",\"journal-title\":\"Biochem. J\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib23\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"597\",\"DOI\":\"10.1016\\/S0021-9258(18)56494-2\",\"volume\":\"182\",\"author\":\"Kunkel\",\"year\":\"1950\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib24\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"373\",\"DOI\":\"10.1016\\/S0021-9258(19)52798-3\",\"volume\":\"159\",\"author\":\"Miller\",\"year\":\"1945\",\"journal-title\":\"J. Biol. Chem\"},{\"key\":\"10.1016\\/S0021-9258(19)52451-6_bib25\",\"unstructured\":\"Weichselbaum, T. E., Am. J. Clin. Path., 16, Tech. Sect., 10, 40 (1946)\"}],\"container-title\":[\"Journal of Biological Chemistry\"],\"original-title\":[],\"language\":\"en\",\"link\":[{\"URL\":\"https:\\/\\/api.elsevier.com\\/content\\/article\\/PII:S0021925819524516?httpAccept=text\\/xml\",\"content-type\":\"text\\/xml\",\"content-version\":\"vor\",\"intended-application\":\"text-mining\"},{\"URL\":\"https:\\/\\/api.elsevier.com\\/content\\/article\\/PII:S0021925819524516?httpAccept=text\\/plain\",\"content-type\":\"text\\/plain\",\"content-version\":\"vor\",\"intended-application\":\"text-mining\"}],\"deposited\":{\"date-parts\":[[2022,1,2]],\"date-time\":\"2022-01-02T20:53:48Z\",\"timestamp\":1641156828000},\"score\":1,\"resource\":{\"primary\":{\"URL\":\"https:\\/\\/linkinghub.elsevier.com\\/retrieve\\/pii\\/S0021925819524516\"}},\"subtitle\":[],\"short-title\":[],\"issued\":{\"date-parts\":[[1951,11]]},\"references-count\":25,\"journal-issue\":{\"issue\":\"1\",\"published-print\":{\"date-parts\":[[1951,11]]}},\"alternative-id\":[\"S0021925819524516\"],\"URL\":\"http:\\/\\/dx.doi.org\\/10.1016\\/s0021-9258(19)52451-6\",\"relation\":{},\"ISSN\":[\"0021-9258\"],\"issn-type\":[{\"value\":\"0021-9258\",\"type\":\"print\"}],\"subject\":[\"Cell Biology\",\"Molecular Biology\",\"Biochemistry\"],\"published\":{\"date-parts\":[[1951,11]]}}}","https://api.crossref.org/works?query=Deep+Residual+Learning+for+Image+Recognition&offset=0&rows=5":"{\"status\":\"ok\",\"message-type\":\"work-list\",\"message-version\":\"1.0.0\",\"message\":{\"facets\":{},\"total-results\":2668574,\"items\":[{\"indexed\":{\"date-parts\":[[2022,9,18]],\"date-time\":\"2022-09-18T22:09:57Z\",\"timestamp\":1663538997857},\"reference-count\":49,\"publisher\":\"IEEE\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2016,6]]},\"DOI\":\"10.1109\\/cvpr.2016.90\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2016,12,13]],\"date-time\":\"2016-12-13T01:38:49Z\",\"timestamp\":1481593129000},\"source\":\"Crossref\",\"is-referenced-by-count\":55937,\"title\":[\"Deep Residual Learning for Image Recognition\"],\"prefix\":\"10.1109\",\"author\":[{\"given\":\"Kaiming\",\"family\":\"He\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"Xiangyu\",\"family\":\"Zhang\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Shaoqing\",\"family\":\"Ren\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Jian\",\"family\":\"Sun\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"263\",\"reference\":[{\"key\":\"ref39\",\"article-title\":\"Overfeat: Integrated recognition, localization and detection using convolutional networks\",\"author\":\"sermanet\",\"year\":\"2014\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref38\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/3-540-49430-8_11\"},{\"key\":\"ref33\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1017\\/CBO9780511812651\"},{\"key\":\"ref32\",\"article-title\":\"Faster R-CNN: Towards real-time object detection with region proposal networks\",\"author\":\"ren\",\"year\":\"2015\",\"journal-title\":\"NIPS\"},{\"key\":\"ref31\",\"article-title\":\"Deep learning made easier by linear transformations in perceptrons\",\"author\":\"raiko\",\"year\":\"2012\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref30\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2007.383266\"},{\"key\":\"ref37\",\"article-title\":\"Accelerated gradient descent by factor-centering decomposition\",\"author\":\"schraudolph\",\"year\":\"1998\",\"journal-title\":\"Technical Report\"},{\"key\":\"ref36\",\"author\":\"saxe\",\"year\":\"2013\",\"journal-title\":\"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\"},{\"key\":\"ref35\",\"author\":\"russakovsky\",\"year\":\"2014\",\"journal-title\":\"Imagenet Large Scale Visual Recognition Challenge\"},{\"key\":\"ref34\",\"article-title\":\"Fitnets: Hints for thin deep nets\",\"author\":\"romero\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref28\",\"article-title\":\"On the number of linear regions of deep neural networks\",\"author\":\"mont\\u00fafar\",\"year\":\"2014\",\"journal-title\":\"NIPS\"},{\"key\":\"ref27\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298965\"},{\"key\":\"ref29\",\"article-title\":\"Rectified linear units improve restricted boltzmann machines\",\"author\":\"nair\",\"year\":\"2010\",\"journal-title\":\"ICML\"},{\"key\":\"ref2\",\"author\":\"bishop\",\"year\":\"1995\",\"journal-title\":\"Neural Networks for Pattern Recognition\"},{\"key\":\"ref1\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/72.279181\"},{\"key\":\"ref20\",\"article-title\":\"Learning multiple layers of features from tiny images\",\"author\":\"krizhevsky\",\"year\":\"2009\",\"journal-title\":\"Tech Report\"},{\"key\":\"ref22\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1989.1.4.541\"},{\"key\":\"ref21\",\"article-title\":\"Imagenet classification with deep convolutional neural networks\",\"author\":\"krizhevsky\",\"year\":\"2012\",\"journal-title\":\"NIPS\"},{\"key\":\"ref24\",\"author\":\"lee\",\"year\":\"2014\",\"journal-title\":\"Deeply-supervised nets\"},{\"key\":\"ref23\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"9\",\"DOI\":\"10.1007\\/3-540-49430-8_2\",\"article-title\":\"Efficient backprop\",\"author\":\"lecun\",\"year\":\"1998\",\"journal-title\":\"Neural Networks Tricks of the Trade\"},{\"key\":\"ref26\",\"article-title\":\"Microsoft COCO: Common objects in context\",\"author\":\"lin\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref25\",\"author\":\"lin\",\"year\":\"2013\",\"journal-title\":\"Network in Network\"},{\"key\":\"ref10\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7299173\"},{\"key\":\"ref11\",\"article-title\":\"Spatial pyramid pooling in deep convolutional networks for visual recognition\",\"author\":\"he\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref40\",\"article-title\":\"Very deep convolutional networks for large-scale image recognition\",\"author\":\"simonyan\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref12\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.123\"},{\"key\":\"ref13\",\"article-title\":\"Improving neural networks by preventing coadaptation of feature detectors\",\"author\":\"hinton\",\"year\":\"2012\"},{\"key\":\"ref14\",\"author\":\"hochreiter\",\"year\":\"1991\",\"journal-title\":\"Untersuchungen zu dynamischen neuronalen Netzen\"},{\"key\":\"ref15\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1997.9.8.1735\"},{\"key\":\"ref16\",\"article-title\":\"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\"author\":\"ioffe\",\"year\":\"2015\",\"journal-title\":\"ICML\"},{\"key\":\"ref17\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2010.57\"},{\"key\":\"ref18\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2011.235\"},{\"key\":\"ref19\",\"author\":\"jia\",\"year\":\"2014\",\"journal-title\":\"Caffe Convolutional Architecture for Fast Feature Embedding\"},{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.5244\\/C.25.76\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1137\\/1.9780898719505\"},{\"key\":\"ref6\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.169\"},{\"key\":\"ref5\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11263-009-0275-4\"},{\"key\":\"ref8\",\"article-title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"author\":\"glorot\",\"year\":\"2010\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref7\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2014.81\"},{\"key\":\"ref49\",\"article-title\":\"Visualizing and understanding convolutional neural networks\",\"author\":\"zeiler\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref9\",\"author\":\"goodfellow\",\"year\":\"2013\",\"journal-title\":\"Maxout Networks\"},{\"key\":\"ref46\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-3-642-42054-2_55\"},{\"key\":\"ref45\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/1179352.1142005\"},{\"key\":\"ref48\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-1-4757-3121-7\"},{\"key\":\"ref47\",\"author\":\"vedaldi\",\"year\":\"2008\",\"journal-title\":\"Vlfeat An Open and Portable Library of Computer Vision Algorithms\"},{\"key\":\"ref42\",\"article-title\":\"Training very deep networks\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"1507 06228\"},{\"key\":\"ref41\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"Highway networks\"},{\"key\":\"ref44\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/34.56188\"},{\"key\":\"ref43\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298594\"}],\"event\":{\"name\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"location\":\"Las Vegas, NV, USA\",\"start\":{\"date-parts\":[[2016,6,27]]},\"end\":{\"date-parts\":[[2016,6,30]]}},\"container-title\":[\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"],\"link\":[{\"URL\":\"http:\\/\\/xplorestaging.ieee.org\\/ielx7\\/7776647\\/7780329\\/07780459.pdf?arnumber=7780459\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2019,9,16]],\"date-time\":\"2019-09-16T12:51:26Z\",\"timestamp\":1568638286000},\"score\":34.511917,\"resource\":{\"primary\":{\"URL\":\"http:\\/\\/ieeexplore.ieee.org\\/document\\/7780459\\/\"}},\"issued\":{\"date-parts\":[[2016,6]]},\"references-count\":49,\"URL\":\"http:\\/\\/dx.doi.org\\/10.1109\\/cvpr.2016.90\",\"published\":{\"date-parts\":[[2016,6]]}},{\"indexed\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:41:59Z\",\"timestamp\":1657831319468},\"reference-count\":0,\"publisher\":\"SCITEPRESS - Science and Technology Publications\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2022]]},\"DOI\":\"10.5220\\/0011270400003277\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:19:02Z\",\"timestamp\":1657829942000},\"source\":\"Crossref\",\"is-referenced-by-count\":0,\"title\":[\"RRConvNet: Recursive-residual Network for Real-life Character Image Recognition\"],\"prefix\":\"10.5220\",\"author\":[{\"given\":\"Tadele\",\"family\":\"Mengiste\",\"sequence\":\"first\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Birhanu\",\"family\":\"Belay\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Bezawork\",\"family\":\"Tilahun\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Tsiyon\",\"family\":\"Worku\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Tesfa\",\"family\":\"Tegegne\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"},{\"name\":\"ICT4D Research Center, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]}],\"member\":\"3171\",\"event\":{\"name\":\"3rd International Conference on Deep Learning Theory and Applications\",\"location\":\"Lisbon, Portugal\",\"start\":{\"date-parts\":[[2022,7,12]]},\"end\":{\"date-parts\":[[2022,7,14]]}},\"container-title\":[\"Proceedings of the 3rd International Conference on Deep Learning Theory and Applications\"],\"original-title\":[\"RRConvNet: Recursive-residual Network for Real-life Character Image Recognition\"],\"deposited\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:19:12Z\",\"timestamp\":1657829952000},\"score\":33.44805,\"resource\":{\"primary\":{\"URL\":\"https:\\/\\/www.scitepress.org\\/DigitalLibrary\\/Link.aspx?doi=10.5220\\/0011270400003277\"}},\"subtitle\":[\"\"],\"issued\":{\"date-parts\":[[2022]]},\"references-count\":0,\"URL\":\"http:\\/\\/dx.doi.org\\/10.5220\\/0011270400003277\",\"published\":{\"date-parts\":[[2022]]}},{\"indexed\":{\"date-parts\":[[2022,6,5]],\"date-time\":\"2022-06-05T17:24:57Z\",\"timestamp\":1654449897045},\"reference-count\":26,\"publisher\":\"IEEE\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2018,6]]},\"DOI\":\"10.1109\\/cvprw.2018.00116\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2018,12,18]],\"date-time\":\"2018-12-18T01:36:46Z\",\"timestamp\":1545097006000},\"source\":\"Crossref\",\"is-referenced-by-count\":13,\"title\":[\"Recursive Deep Residual Learning for Single Image Dehazing\"],\"prefix\":\"10.1109\",\"author\":[{\"given\":\"Yixin\",\"family\":\"Du\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"Xin\",\"family\":\"Li\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"263\",\"reference\":[{\"key\":\"ref10\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"2341\",\"DOI\":\"10.1109\\/TPAMI.2010.168\",\"article-title\":\"Single image haze removal using dark channel prior\",\"volume\":\"33\",\"author\":\"he\",\"year\":\"2011\",\"journal-title\":\"Ieee Transactions on PAMI\"},{\"key\":\"ref11\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2016.90\"},{\"key\":\"ref12\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2016.182\"},{\"key\":\"ref13\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2017.618\"},{\"key\":\"ref14\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2017.511\"},{\"key\":\"ref15\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2013.82\"},{\"key\":\"ref16\",\"first-page\":\"254\",\"article-title\":\"Vision through the atmosphere\",\"author\":\"middleton\",\"year\":\"1957\",\"journal-title\":\"Geo-physik II\\/Geophysics II\"},{\"key\":\"ref17\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"233\",\"DOI\":\"10.1023\\/A:1016328200723\",\"article-title\":\"Vision and the atmosphere\",\"volume\":\"48\",\"author\":\"narasimhan\",\"year\":\"2002\",\"journal-title\":\"International Journal of Computer Vision\"},{\"key\":\"ref18\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11263-011-0508-1\"},{\"key\":\"ref19\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1137\\/040605412\"},{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TCSVT.2008.918444\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIP.2016.2598681\"},{\"key\":\"ref6\",\"first-page\":\"184\",\"article-title\":\"Learning a deep convolutional network for image super-resolution\",\"author\":\"dong\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref5\",\"first-page\":\"576\",\"article-title\":\"Robust image and video dehazing with visual artifact suppression via gradient residual minimization\",\"author\":\"chen\",\"year\":\"2016\",\"journal-title\":\"ECCV\"},{\"key\":\"ref8\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/1360612.1360671\"},{\"key\":\"ref7\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2015.2439281\"},{\"key\":\"ref2\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2016.185\"},{\"key\":\"ref9\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1090\\/S0002-9939-1972-0298500-3\"},{\"key\":\"ref1\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/72.279181\"},{\"key\":\"ref20\",\"first-page\":\"154\",\"article-title\":\"Single image dehazing via multi-scale convolutional neural networks\",\"author\":\"ren\",\"year\":\"2016\",\"journal-title\":\"ECCV\"},{\"key\":\"ref22\",\"first-page\":\"746\",\"article-title\":\"Indoor segmentation and support inference from rgbd images\",\"author\":\"silberman\",\"year\":\"2012\",\"journal-title\":\"ECCV\"},{\"key\":\"ref21\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2003.1211354\"},{\"key\":\"ref24\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIP.2003.819861\"},{\"key\":\"ref23\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2008.4587643\"},{\"key\":\"ref26\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIP.2017.2662206\"},{\"key\":\"ref25\",\"first-page\":\"341\",\"article-title\":\"Image denoising and inpainting with deep neural networks\",\"author\":\"xie\",\"year\":\"2012\",\"journal-title\":\"NIPS\"}],\"event\":{\"name\":\"2018 IEEE\\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"location\":\"Salt Lake City, UT, USA\",\"start\":{\"date-parts\":[[2018,6,18]]},\"end\":{\"date-parts\":[[2018,6,22]]}},\"container-title\":[\"2018 IEEE\\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\"],\"link\":[{\"URL\":\"http:\\/\\/xplorestaging.ieee.org\\/ielx7\\/8575058\\/8575239\\/08575267.pdf?arnumber=8575267\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2022,1,27]],\"date-time\":\"2022-01-27T02:43:22Z\",\"timestamp\":1643251402000},\"score\":32.47081,\"resource\":{\"primary\":{\"URL\":\"https:\\/\\/ieeexplore.ieee.org\\/document\\/8575267\\/\"}},\"issued\":{\"date-parts\":[[2018,6]]},\"references-count\":26,\"URL\":\"http:\\/\\/dx.doi.org\\/10.1109\\/cvprw.2018.00116\",\"published\":{\"date-parts\":[[2018,6]]}},{\"indexed\":{\"date-parts\":[[2022,9,9]],\"date-time\":\"2022-09-09T19:31:47Z\",\"timestamp\":1662751907594},\"reference-count\":12,\"publisher\":\"IEEE\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2017,12]]},\"DOI\":\"10.1109\\/itnec.2017.8284852\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2018,2,8]],\"date-time\":\"2018-02-08T21:47:31Z\",\"timestamp\":1518126451000},\"source\":\"Crossref\",\"is-referenced-by-count\":19,\"title\":[\"FPGA accelerates deep residual learning for image recognition\"],\"prefix\":\"10.1109\",\"author\":[{\"given\":\"Xuelei\",\"family\":\"Li\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"Liangkui\",\"family\":\"Ding\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Li\",\"family\":\"Wang\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Fang\",\"family\":\"Cao\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"263\",\"reference\":[{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1989.1.4.541\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2016.90\"},{\"key\":\"ref10\",\"author\":\"chintala\",\"year\":\"2016\",\"journal-title\":\"convnet-benchmarks\"},{\"key\":\"ref6\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCD.2013.6657019\"},{\"key\":\"ref11\",\"author\":\"vidia\",\"year\":\"2015\",\"journal-title\":\"GPU-Based Deep Learning Inference A Performance and Power Analysis\"},{\"key\":\"ref5\",\"first-page\":\"1\",\"article-title\":\"Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks\",\"author\":\"zhang\",\"year\":\"2016\",\"journal-title\":\"Computer-Aided Design (ICCAD) 2016 IEEE\\/ACM International Conference on\"},{\"key\":\"ref12\",\"author\":\"aydonat\",\"year\":\"2017\",\"journal-title\":\"An OpenCL (TM) Deep Learning Accelerator on Arria 10 FPGA'17\"},{\"key\":\"ref8\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"16\",\"DOI\":\"10.1145\\/2847263.2847276\",\"article-title\":\"Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks\",\"author\":\"suda\",\"year\":\"2016\",\"journal-title\":\"Proceedings of the 2016 ACM\\/SIGDA International Symposium on Field-Programmable Gate Arrays\"},{\"key\":\"ref7\",\"first-page\":\"26\",\"article-title\":\"Going Deeper with Embedded FPGA Platform for Convolutional Neural Network\",\"author\":\"qiu\",\"year\":\"2016\",\"journal-title\":\"ACM\\/SIGDA International Symposium on Field-Programmable Gate Arrays\"},{\"key\":\"ref2\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298594\"},{\"key\":\"ref9\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/2684746.2689060\"},{\"key\":\"ref1\",\"first-page\":\"1097\",\"article-title\":\"ImageNet classification with deep convolutional neural networks\",\"volume\":\"25\",\"author\":\"krizhevsky\",\"year\":\"2012\",\"journal-title\":\"International Conference on Neural Information Processing Systems\"}],\"event\":{\"name\":\"2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\",\"location\":\"Chengdu\",\"start\":{\"date-parts\":[[2017,12,15]]},\"end\":{\"date-parts\":[[2017,12,17]]}},\"container-title\":[\"2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\"],\"link\":[{\"URL\":\"http:\\/\\/xplorestaging.ieee.org\\/ielx7\\/8272884\\/8284746\\/08284852.pdf?arnumber=8284852\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2019,10,10]],\"date-time\":\"2019-10-10T17:09:19Z\",\"timestamp\":1570727359000},\"score\":30.848352,\"resource\":{\"primary\":{\"URL\":\"http:\\/\\/ieeexplore.ieee.org\\/document\\/8284852\\/\"}},\"issued\":{\"date-parts\":[[2017,12]]},\"references-count\":12,\"URL\":\"http:\\/\\/dx.doi.org\\/10.1109\\/itnec.2017.8284852\",\"published\":{\"date-parts\":[[2017,12]]}},{\"indexed\":{\"date-parts\":[[2022,9,8]],\"date-time\":\"2022-09-08T01:43:25Z\",\"timestamp\":1662601405311},\"reference-count\":94,\"publisher\":\"MDPI AG\",\"issue\":\"18\",\"license\":[{\"start\":{\"date-parts\":[[2022,9,7]],\"date-time\":\"2022-09-07T00:00:00Z\",\"timestamp\":1662508800000},\"content-version\":\"vor\",\"delay-in-days\":0,\"URL\":\"https:\\/\\/creativecommons.org\\/licenses\\/by\\/4.0\\/\"}],\"funder\":[{\"DOI\":\"10.13039\\/501100001809\",\"name\":\"National Natural Science Foundation of China\",\"doi-asserted-by\":\"publisher\",\"award\":[\"61902082\"]}],\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"short-container-title\":[\"Applied Sciences\"],\"abstract\":\"<jats:p>Deep Residual Networks have recently been shown to significantly improve the performance of neural networks trained on ImageNet, with results beating all previous methods on this dataset by large margins in the image classification task. However, the meaning of these impressive numbers and their implications for future research are not fully understood yet. In this survey, we will try to explain what Deep Residual Networks are, how they achieve their excellent results, and why their successful implementation in practice represents a significant advance over existing techniques. We also discuss some open questions related to residual learning as well as possible applications of Deep Residual Networks beyond ImageNet. Finally, we discuss some issues that still need to be resolved before deep residual learning can be applied on more complex problems.<\\/jats:p>\",\"DOI\":\"10.3390\\/app12188972\",\"type\":\"journal-article\",\"created\":{\"date-parts\":[[2022,9,8]],\"date-time\":\"2022-09-08T00:52:03Z\",\"timestamp\":1662598323000},\"page\":\"8972\",\"source\":\"Crossref\",\"is-referenced-by-count\":0,\"title\":[\"Deep Residual Learning for Image Recognition: A Survey\"],\"prefix\":\"10.3390\",\"volume\":\"12\",\"author\":[{\"given\":\"Muhammad\",\"family\":\"Shafiq\",\"sequence\":\"first\",\"affiliation\":[]},{\"ORCID\":\"http:\\/\\/orcid.org\\/0000-0001-7546-852X\",\"authenticated-orcid\":false,\"given\":\"Zhaoquan\",\"family\":\"Gu\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"1968\",\"published-online\":{\"date-parts\":[[2022,9,7]]},\"reference\":[{\"key\":\"ref1\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2016.90\"},{\"key\":\"ref2\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.ijar.2017.10.030\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.neunet.2021.05.028\"},{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/isbi.2018.8363713\"},{\"key\":\"ref5\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s10489-021-02769-6\"},{\"key\":\"ref6\",\"first-page\":\"3\",\"article-title\":\"Classification of Trash for Recyclability Status\",\"volume\":\"2016\",\"author\":\"Yang\",\"year\":\"2016\",\"journal-title\":\"CS229Project Rep.\"},{\"key\":\"ref7\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s40747-020-00199-4\"},{\"key\":\"ref8\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2020.2971064\"},{\"key\":\"ref9\",\"series-title\":\"FPGA Acceleration of Convolutional Neural Networks\",\"year\":\"2017\"},{\"key\":\"ref10\",\"first-page\":\"226\",\"article-title\":\"Classification model of \\u2018Toraja\\u2019 arabica coffee fruit ripeness levels using convolution neural network approach\",\"volume\":\"13\",\"author\":\"Michael\",\"year\":\"2021\",\"journal-title\":\"ILKOM J. Ilm.\"},{\"key\":\"ref11\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2020.3019937\"},{\"key\":\"ref12\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/resources8030136\"},{\"key\":\"ref13\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2021.3070754\"},{\"key\":\"ref14\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/electronics10091036\"},{\"key\":\"ref15\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1049\\/bme2.12027\"},{\"key\":\"ref16\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ITSC.2017.8317714\"},{\"key\":\"ref17\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2021.3088914\"},{\"key\":\"ref18\",\"article-title\":\"Deep Residual Learning for Image Recognition Kaiming\",\"author\":\"Sangeetha\",\"year\":\"2006\",\"journal-title\":\"Indian J. Chem.-Sect. B Org. Med. Chem.\"},{\"key\":\"ref19\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2019.2922738\"},{\"key\":\"ref20\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.2991\\/ijndc.k.190710.002\"},{\"key\":\"ref21\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/resources8030149\"},{\"key\":\"ref22\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.rsase.2020.100402\"},{\"key\":\"ref23\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1038\\/s41598-020-77923-0\"},{\"key\":\"ref24\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.neunet.2021.08.026\"},{\"key\":\"ref25\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/s21010268\"},{\"key\":\"ref26\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.30534\\/ijeter\\/2020\\/113872020\"},{\"key\":\"ref27\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TGRS.2021.3062372\"},{\"key\":\"ref28\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACSOS-C51401.2020.00024\"},{\"key\":\"ref29\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TASLP.2016.2602884\"},{\"key\":\"ref30\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIP.2018.2815084\"},{\"key\":\"ref31\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2017.298\"},{\"key\":\"ref32\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIFS.2017.2788002\"},{\"key\":\"ref33\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/COMPSAC.2018.10220\"},{\"key\":\"ref34\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/icdar.2017.149\"},{\"key\":\"ref35\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.7717\\/peerj-cs.621\"},{\"key\":\"ref36\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2020.2995330\"},{\"key\":\"ref37\",\"article-title\":\"Packet-based network traffic classification using deep learning\",\"author\":\"Lim\",\"year\":\"2019\",\"journal-title\":\"Proceedings of the 2019 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)\"},{\"key\":\"ref38\",\"unstructured\":\"https:\\/\\/cyberleninka.ru\\/article\\/n\\/reshenie-zadach-vychislitelnoy-gidrodinamiki-s-primeneniem-tehnologii-nvidia-cuda-articlehead-tehnologiya-nvidia-cuda-v-zadachah\\/viewer\"},{\"key\":\"ref39\",\"series-title\":\"Cuda C Best Practices Guide\",\"year\":\"2015\"},{\"key\":\"ref40\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/s21206933\"},{\"key\":\"ref41\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11227-019-02835-4\"},{\"key\":\"ref42\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPDS.2019.2947511\"},{\"key\":\"ref43\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/MIE.2017.2694578\"},{\"key\":\"ref44\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/AIEA53260.2021.00077\"},{\"key\":\"ref45\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.2139\\/ssrn.4013383\"},{\"key\":\"ref46\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2017.195\"},{\"key\":\"ref47\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TITS.2020.3025875\"},{\"key\":\"ref48\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TIP.2019.2937724\"},{\"key\":\"ref49\",\"unstructured\":\"CIFAR-10 and CIFAR-100 Datasets\\nhttps:\\/\\/www.cs.toronto.edu\\/~kriz\\/cifar.html\"},{\"key\":\"ref50\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.csbj.2021.02.016\"},{\"key\":\"ref51\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/JSTARS.2021.3110842\"},{\"key\":\"ref52\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.energy.2020.118617\"},{\"key\":\"ref53\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/icassp40776.2020.9054548\"},{\"key\":\"ref54\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2020.2982538\"},{\"key\":\"ref55\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1155\\/2020\\/8863617\"},{\"key\":\"ref56\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11235-020-00733-2\"},{\"key\":\"ref57\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2020.3044416\"},{\"key\":\"ref58\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TNNLS.2020.2978386\"},{\"key\":\"ref59\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-3-319-46493-0_39\"},{\"key\":\"ref60\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/APSIPA.2016.7820692\"},{\"key\":\"ref61\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-1-4842-6168-2\"},{\"key\":\"ref62\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-3-030-81847-0_8\"},{\"key\":\"ref63\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1183\\/13993003.00775-2020\"},{\"key\":\"ref64\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.15353\\/vsnl.v3i1.166\"},{\"key\":\"ref65\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.compag.2017.08.005\"},{\"key\":\"ref66\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.2514\\/6.2019-0697\"},{\"key\":\"ref67\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11042-017-4440-4\"},{\"key\":\"ref68\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2021.3089698\"},{\"key\":\"ref69\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.3390\\/cancers14030606\"},{\"key\":\"ref70\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1093\\/mnras\\/stw2672\"},{\"key\":\"ref71\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1186\\/s40537-017-0084-5\"},{\"key\":\"ref72\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1017\\/S1351324916000334\"},{\"key\":\"ref73\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.scs.2020.102177\"},{\"key\":\"ref74\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.cose.2020.101863\"},{\"key\":\"ref75\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/JIOT.2020.3002255\"},{\"key\":\"ref76\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1302\\/0301-620X.101B7.BJJ-2018-1420.R1\"},{\"key\":\"ref77\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ACCESS.2019.2937139\"},{\"key\":\"ref78\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1155\\/2021\\/6694281\"},{\"key\":\"ref79\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.patcog.2017.10.013\"},{\"key\":\"ref80\",\"article-title\":\"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\",\"author\":\"Buolamwini\",\"year\":\"2018\",\"journal-title\":\"Proceedings of the Conference on Fairness, Accountability and Transparency, PMLR\"},{\"key\":\"ref81\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.5220\\/0010877400003116\"},{\"key\":\"ref82\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/3306618.3314287\"},{\"key\":\"ref83\",\"article-title\":\"On calibration of modern neural networks\",\"author\":\"Guo\",\"year\":\"2017\",\"journal-title\":\"Proceedings of the 34th International Conference on Machine Learning, ICML 2017\"},{\"key\":\"ref84\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/IJCNN52387.2021.9534411\"},{\"key\":\"ref85\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.patcog.2018.03.005\"},{\"key\":\"ref86\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s12559-021-09848-3\"},{\"key\":\"ref87\",\"article-title\":\"Review: AlexNet, CaffeNet\\u2014Winner of ILSVRC 2012 (Image Classification)\",\"author\":\"Sik-Ho\",\"year\":\"2018\",\"journal-title\":\"Medium Note\"},{\"key\":\"ref88\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s42452-021-04485-9\"},{\"key\":\"ref89\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.jksuci.2021.05.015\"},{\"key\":\"ref90\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/SLT48900.2021.9383531\"},{\"key\":\"ref91\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/MNET.2011.5772055\"},{\"key\":\"ref92\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1016\\/j.jnca.2017.11.007\"},{\"key\":\"ref93\",\"article-title\":\"The CIFAR-10 dataset\",\"volume\":\"35\",\"author\":\"Feng\",\"year\":\"2007\",\"journal-title\":\"Electr. Eng.\"},{\"key\":\"ref94\",\"series-title\":\"ImageNet Dataset\",\"year\":\"2016\"}],\"container-title\":[\"Applied Sciences\"],\"language\":\"en\",\"link\":[{\"URL\":\"https:\\/\\/www.mdpi.com\\/2076-3417\\/12\\/18\\/8972\\/pdf\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2022,9,8]],\"date-time\":\"2022-09-08T01:18:22Z\",\"timestamp\":1662599902000},\"score\":30.482742,\"resource\":{\"primary\":{\"URL\":\"https:\\/\\/www.mdpi.com\\/2076-3417\\/12\\/18\\/8972\"}},\"issued\":{\"date-parts\":[[2022,9,7]]},\"references-count\":94,\"journal-issue\":{\"issue\":\"18\",\"published-online\":{\"date-parts\":[[2022,9]]}},\"alternative-id\":[\"app12188972\"],\"URL\":\"http:\\/\\/dx.doi.org\\/10.3390\\/app12188972\",\"ISSN\":[\"2076-3417\"],\"issn-type\":[{\"value\":\"2076-3417\",\"type\":\"electronic\"}],\"subject\":[\"Fluid Flow and Transfer Processes\",\"Computer Science Applications\",\"Process Chemistry and Technology\",\"General Engineering\",\"Instrumentation\",\"General Materials Science\"],\"published\":{\"date-parts\":[[2022,9,7]]}}],\"items-per-page\":5,\"query\":{\"start-index\":0,\"search-terms\":\"Deep Residual Learning for Image Recognition\"}}}","https://api.semanticscholar.org/graph/v1/paper/arxiv:1706.03762?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors":"{\"paperId\": \"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"externalIds\": {\"DBLP\": \"conf/nips/VaswaniSPUJGKP17\", \"ArXiv\": \"1706.03762\", \"MAG\": \"2963403868\", \"CorpusId\": 13756489}, \"url\": \"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"title\": \"Attention is All you Need\", \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\", \"venue\": \"NIPS\", \"year\": 2017, \"referenceCount\": 41, \"citationCount\": 39648, \"influentialCitationCount\": 9052, \"isOpenAccess\": false, \"fieldsOfStudy\": [\"Computer Science\"], \"authors\": [{\"authorId\": \"40348417\", \"name\": \"Ashish Vaswani\"}, {\"authorId\": \"1846258\", \"name\": \"Noam M. Shazeer\"}, {\"authorId\": \"3877127\", \"name\": \"Niki Parmar\"}, {\"authorId\": \"39328010\", \"name\": \"Jakob Uszkoreit\"}, {\"authorId\": \"145024664\", \"name\": \"Llion Jones\"}, {\"authorId\": \"19177000\", \"name\": \"Aidan N. Gomez\"}, {\"authorId\": \"40527594\", \"name\": \"Lukasz Kaiser\"}, {\"authorId\": \"3443442\", \"name\": \"Illia Polosukhin\"}]}\n","https://export.arxiv.org/api/query?id_list=1512.03385&start=0&max_results=1&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1512.03385%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1512.03385&amp;start=0&amp;max_results=1</title>\n  <id>http://arxiv.org/api/GKwqZ8aTJNq+pSLl6l9dwdnoIrM</id>\n  <updated>2022-09-18T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1512.03385v1</id>\n    <updated>2015-12-10T19:51:55Z</updated>\n    <published>2015-12-10T19:51:55Z</published>\n    <title>Deep Residual Learning for Image Recognition</title>\n    <summary>  Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.\n</summary>\n    <author>\n      <name>Kaiming He</name>\n    </author>\n    <author>\n      <name>Xiangyu Zhang</name>\n    </author>\n    <author>\n      <name>Shaoqing Ren</name>\n    </author>\n    <author>\n      <name>Jian Sun</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Tech report</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1512.03385v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.03385v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1512.03385?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors":"{\"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"externalIds\": {\"DBLP\": \"conf/cvpr/HeZRS16\", \"MAG\": \"2194775991\", \"ArXiv\": \"1512.03385\", \"DOI\": \"10.1109/cvpr.2016.90\", \"CorpusId\": 206594692}, \"url\": \"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"title\": \"Deep Residual Learning for Image Recognition\", \"abstract\": \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\", \"venue\": \"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\", \"year\": 2015, \"referenceCount\": 55, \"citationCount\": 103174, \"influentialCitationCount\": 22394, \"isOpenAccess\": true, \"fieldsOfStudy\": [\"Computer Science\"], \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}\n","https://api.semanticscholar.org/graph/v1/author/39353098?fields=affiliations,authorId,name,url":"{\"authorId\": \"39353098\", \"url\": \"https://www.semanticscholar.org/author/39353098\", \"name\": \"Kaiming He\", \"affiliations\": [\"Facebook AI Research\"]}\n","https://api.semanticscholar.org/graph/v1/author/1771551?fields=affiliations,authorId,name,url":"{\"authorId\": \"1771551\", \"url\": \"https://www.semanticscholar.org/author/1771551\", \"name\": \"X. Zhang\", \"affiliations\": []}\n","https://api.semanticscholar.org/graph/v1/author/3080683?fields=affiliations,authorId,name,url":"{\"authorId\": \"3080683\", \"url\": \"https://www.semanticscholar.org/author/3080683\", \"name\": \"Shaoqing Ren\", \"affiliations\": []}\n","https://api.crossref.org/works/10.1109/cvpr.2016.90":"{\"status\":\"ok\",\"message-type\":\"work\",\"message-version\":\"1.0.0\",\"message\":{\"indexed\":{\"date-parts\":[[2022,9,18]],\"date-time\":\"2022-09-18T22:09:57Z\",\"timestamp\":1663538997857},\"reference-count\":49,\"publisher\":\"IEEE\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"short-container-title\":[],\"published-print\":{\"date-parts\":[[2016,6]]},\"DOI\":\"10.1109\\/cvpr.2016.90\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2016,12,13]],\"date-time\":\"2016-12-13T01:38:49Z\",\"timestamp\":1481593129000},\"source\":\"Crossref\",\"is-referenced-by-count\":55937,\"title\":[\"Deep Residual Learning for Image Recognition\"],\"prefix\":\"10.1109\",\"author\":[{\"given\":\"Kaiming\",\"family\":\"He\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"Xiangyu\",\"family\":\"Zhang\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Shaoqing\",\"family\":\"Ren\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Jian\",\"family\":\"Sun\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"263\",\"reference\":[{\"key\":\"ref39\",\"article-title\":\"Overfeat: Integrated recognition, localization and detection using convolutional networks\",\"author\":\"sermanet\",\"year\":\"2014\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref38\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/3-540-49430-8_11\"},{\"key\":\"ref33\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1017\\/CBO9780511812651\"},{\"key\":\"ref32\",\"article-title\":\"Faster R-CNN: Towards real-time object detection with region proposal networks\",\"author\":\"ren\",\"year\":\"2015\",\"journal-title\":\"NIPS\"},{\"key\":\"ref31\",\"article-title\":\"Deep learning made easier by linear transformations in perceptrons\",\"author\":\"raiko\",\"year\":\"2012\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref30\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2007.383266\"},{\"key\":\"ref37\",\"article-title\":\"Accelerated gradient descent by factor-centering decomposition\",\"author\":\"schraudolph\",\"year\":\"1998\",\"journal-title\":\"Technical Report\"},{\"key\":\"ref36\",\"author\":\"saxe\",\"year\":\"2013\",\"journal-title\":\"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\"},{\"key\":\"ref35\",\"author\":\"russakovsky\",\"year\":\"2014\",\"journal-title\":\"Imagenet Large Scale Visual Recognition Challenge\"},{\"key\":\"ref34\",\"article-title\":\"Fitnets: Hints for thin deep nets\",\"author\":\"romero\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref28\",\"article-title\":\"On the number of linear regions of deep neural networks\",\"author\":\"mont\\u00fafar\",\"year\":\"2014\",\"journal-title\":\"NIPS\"},{\"key\":\"ref27\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298965\"},{\"key\":\"ref29\",\"article-title\":\"Rectified linear units improve restricted boltzmann machines\",\"author\":\"nair\",\"year\":\"2010\",\"journal-title\":\"ICML\"},{\"key\":\"ref2\",\"author\":\"bishop\",\"year\":\"1995\",\"journal-title\":\"Neural Networks for Pattern Recognition\"},{\"key\":\"ref1\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/72.279181\"},{\"key\":\"ref20\",\"article-title\":\"Learning multiple layers of features from tiny images\",\"author\":\"krizhevsky\",\"year\":\"2009\",\"journal-title\":\"Tech Report\"},{\"key\":\"ref22\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1989.1.4.541\"},{\"key\":\"ref21\",\"article-title\":\"Imagenet classification with deep convolutional neural networks\",\"author\":\"krizhevsky\",\"year\":\"2012\",\"journal-title\":\"NIPS\"},{\"key\":\"ref24\",\"author\":\"lee\",\"year\":\"2014\",\"journal-title\":\"Deeply-supervised nets\"},{\"key\":\"ref23\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"9\",\"DOI\":\"10.1007\\/3-540-49430-8_2\",\"article-title\":\"Efficient backprop\",\"author\":\"lecun\",\"year\":\"1998\",\"journal-title\":\"Neural Networks Tricks of the Trade\"},{\"key\":\"ref26\",\"article-title\":\"Microsoft COCO: Common objects in context\",\"author\":\"lin\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref25\",\"author\":\"lin\",\"year\":\"2013\",\"journal-title\":\"Network in Network\"},{\"key\":\"ref10\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7299173\"},{\"key\":\"ref11\",\"article-title\":\"Spatial pyramid pooling in deep convolutional networks for visual recognition\",\"author\":\"he\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref40\",\"article-title\":\"Very deep convolutional networks for large-scale image recognition\",\"author\":\"simonyan\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref12\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.123\"},{\"key\":\"ref13\",\"article-title\":\"Improving neural networks by preventing coadaptation of feature detectors\",\"author\":\"hinton\",\"year\":\"2012\"},{\"key\":\"ref14\",\"author\":\"hochreiter\",\"year\":\"1991\",\"journal-title\":\"Untersuchungen zu dynamischen neuronalen Netzen\"},{\"key\":\"ref15\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1997.9.8.1735\"},{\"key\":\"ref16\",\"article-title\":\"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\"author\":\"ioffe\",\"year\":\"2015\",\"journal-title\":\"ICML\"},{\"key\":\"ref17\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2010.57\"},{\"key\":\"ref18\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2011.235\"},{\"key\":\"ref19\",\"author\":\"jia\",\"year\":\"2014\",\"journal-title\":\"Caffe Convolutional Architecture for Fast Feature Embedding\"},{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.5244\\/C.25.76\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1137\\/1.9780898719505\"},{\"key\":\"ref6\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.169\"},{\"key\":\"ref5\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11263-009-0275-4\"},{\"key\":\"ref8\",\"article-title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"author\":\"glorot\",\"year\":\"2010\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref7\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2014.81\"},{\"key\":\"ref49\",\"article-title\":\"Visualizing and understanding convolutional neural networks\",\"author\":\"zeiler\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref9\",\"author\":\"goodfellow\",\"year\":\"2013\",\"journal-title\":\"Maxout Networks\"},{\"key\":\"ref46\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-3-642-42054-2_55\"},{\"key\":\"ref45\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/1179352.1142005\"},{\"key\":\"ref48\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-1-4757-3121-7\"},{\"key\":\"ref47\",\"author\":\"vedaldi\",\"year\":\"2008\",\"journal-title\":\"Vlfeat An Open and Portable Library of Computer Vision Algorithms\"},{\"key\":\"ref42\",\"article-title\":\"Training very deep networks\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"1507 06228\"},{\"key\":\"ref41\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"Highway networks\"},{\"key\":\"ref44\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/34.56188\"},{\"key\":\"ref43\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298594\"}],\"event\":{\"name\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"location\":\"Las Vegas, NV, USA\",\"start\":{\"date-parts\":[[2016,6,27]]},\"end\":{\"date-parts\":[[2016,6,30]]}},\"container-title\":[\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"],\"original-title\":[],\"link\":[{\"URL\":\"http:\\/\\/xplorestaging.ieee.org\\/ielx7\\/7776647\\/7780329\\/07780459.pdf?arnumber=7780459\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2019,9,16]],\"date-time\":\"2019-09-16T12:51:26Z\",\"timestamp\":1568638286000},\"score\":1,\"resource\":{\"primary\":{\"URL\":\"http:\\/\\/ieeexplore.ieee.org\\/document\\/7780459\\/\"}},\"subtitle\":[],\"short-title\":[],\"issued\":{\"date-parts\":[[2016,6]]},\"references-count\":49,\"URL\":\"http:\\/\\/dx.doi.org\\/10.1109\\/cvpr.2016.90\",\"relation\":{},\"published\":{\"date-parts\":[[2016,6]]}}}","https://api.crossref.org/works?query=Deep+Residual+Learning+for+Image+Recognition&offset=0&rows=2":"{\"status\":\"ok\",\"message-type\":\"work-list\",\"message-version\":\"1.0.0\",\"message\":{\"facets\":{},\"total-results\":2668574,\"items\":[{\"indexed\":{\"date-parts\":[[2022,9,18]],\"date-time\":\"2022-09-18T22:09:57Z\",\"timestamp\":1663538997857},\"reference-count\":49,\"publisher\":\"IEEE\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2016,6]]},\"DOI\":\"10.1109\\/cvpr.2016.90\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2016,12,13]],\"date-time\":\"2016-12-13T01:38:49Z\",\"timestamp\":1481593129000},\"source\":\"Crossref\",\"is-referenced-by-count\":55937,\"title\":[\"Deep Residual Learning for Image Recognition\"],\"prefix\":\"10.1109\",\"author\":[{\"given\":\"Kaiming\",\"family\":\"He\",\"sequence\":\"first\",\"affiliation\":[]},{\"given\":\"Xiangyu\",\"family\":\"Zhang\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Shaoqing\",\"family\":\"Ren\",\"sequence\":\"additional\",\"affiliation\":[]},{\"given\":\"Jian\",\"family\":\"Sun\",\"sequence\":\"additional\",\"affiliation\":[]}],\"member\":\"263\",\"reference\":[{\"key\":\"ref39\",\"article-title\":\"Overfeat: Integrated recognition, localization and detection using convolutional networks\",\"author\":\"sermanet\",\"year\":\"2014\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref38\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/3-540-49430-8_11\"},{\"key\":\"ref33\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1017\\/CBO9780511812651\"},{\"key\":\"ref32\",\"article-title\":\"Faster R-CNN: Towards real-time object detection with region proposal networks\",\"author\":\"ren\",\"year\":\"2015\",\"journal-title\":\"NIPS\"},{\"key\":\"ref31\",\"article-title\":\"Deep learning made easier by linear transformations in perceptrons\",\"author\":\"raiko\",\"year\":\"2012\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref30\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2007.383266\"},{\"key\":\"ref37\",\"article-title\":\"Accelerated gradient descent by factor-centering decomposition\",\"author\":\"schraudolph\",\"year\":\"1998\",\"journal-title\":\"Technical Report\"},{\"key\":\"ref36\",\"author\":\"saxe\",\"year\":\"2013\",\"journal-title\":\"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\"},{\"key\":\"ref35\",\"author\":\"russakovsky\",\"year\":\"2014\",\"journal-title\":\"Imagenet Large Scale Visual Recognition Challenge\"},{\"key\":\"ref34\",\"article-title\":\"Fitnets: Hints for thin deep nets\",\"author\":\"romero\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref28\",\"article-title\":\"On the number of linear regions of deep neural networks\",\"author\":\"mont\\u00fafar\",\"year\":\"2014\",\"journal-title\":\"NIPS\"},{\"key\":\"ref27\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298965\"},{\"key\":\"ref29\",\"article-title\":\"Rectified linear units improve restricted boltzmann machines\",\"author\":\"nair\",\"year\":\"2010\",\"journal-title\":\"ICML\"},{\"key\":\"ref2\",\"author\":\"bishop\",\"year\":\"1995\",\"journal-title\":\"Neural Networks for Pattern Recognition\"},{\"key\":\"ref1\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/72.279181\"},{\"key\":\"ref20\",\"article-title\":\"Learning multiple layers of features from tiny images\",\"author\":\"krizhevsky\",\"year\":\"2009\",\"journal-title\":\"Tech Report\"},{\"key\":\"ref22\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1989.1.4.541\"},{\"key\":\"ref21\",\"article-title\":\"Imagenet classification with deep convolutional neural networks\",\"author\":\"krizhevsky\",\"year\":\"2012\",\"journal-title\":\"NIPS\"},{\"key\":\"ref24\",\"author\":\"lee\",\"year\":\"2014\",\"journal-title\":\"Deeply-supervised nets\"},{\"key\":\"ref23\",\"doi-asserted-by\":\"crossref\",\"first-page\":\"9\",\"DOI\":\"10.1007\\/3-540-49430-8_2\",\"article-title\":\"Efficient backprop\",\"author\":\"lecun\",\"year\":\"1998\",\"journal-title\":\"Neural Networks Tricks of the Trade\"},{\"key\":\"ref26\",\"article-title\":\"Microsoft COCO: Common objects in context\",\"author\":\"lin\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref25\",\"author\":\"lin\",\"year\":\"2013\",\"journal-title\":\"Network in Network\"},{\"key\":\"ref10\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7299173\"},{\"key\":\"ref11\",\"article-title\":\"Spatial pyramid pooling in deep convolutional networks for visual recognition\",\"author\":\"he\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref40\",\"article-title\":\"Very deep convolutional networks for large-scale image recognition\",\"author\":\"simonyan\",\"year\":\"2015\",\"journal-title\":\"ICLRE\"},{\"key\":\"ref12\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.123\"},{\"key\":\"ref13\",\"article-title\":\"Improving neural networks by preventing coadaptation of feature detectors\",\"author\":\"hinton\",\"year\":\"2012\"},{\"key\":\"ref14\",\"author\":\"hochreiter\",\"year\":\"1991\",\"journal-title\":\"Untersuchungen zu dynamischen neuronalen Netzen\"},{\"key\":\"ref15\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1162\\/neco.1997.9.8.1735\"},{\"key\":\"ref16\",\"article-title\":\"Batch normalization: Accelerating deep network training by reducing internal covariate shift\",\"author\":\"ioffe\",\"year\":\"2015\",\"journal-title\":\"ICML\"},{\"key\":\"ref17\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2010.57\"},{\"key\":\"ref18\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/TPAMI.2011.235\"},{\"key\":\"ref19\",\"author\":\"jia\",\"year\":\"2014\",\"journal-title\":\"Caffe Convolutional Architecture for Fast Feature Embedding\"},{\"key\":\"ref4\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.5244\\/C.25.76\"},{\"key\":\"ref3\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1137\\/1.9780898719505\"},{\"key\":\"ref6\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/ICCV.2015.169\"},{\"key\":\"ref5\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/s11263-009-0275-4\"},{\"key\":\"ref8\",\"article-title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"author\":\"glorot\",\"year\":\"2010\",\"journal-title\":\"AISTATS\"},{\"key\":\"ref7\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2014.81\"},{\"key\":\"ref49\",\"article-title\":\"Visualizing and understanding convolutional neural networks\",\"author\":\"zeiler\",\"year\":\"2014\",\"journal-title\":\"ECCV\"},{\"key\":\"ref9\",\"author\":\"goodfellow\",\"year\":\"2013\",\"journal-title\":\"Maxout Networks\"},{\"key\":\"ref46\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-3-642-42054-2_55\"},{\"key\":\"ref45\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1145\\/1179352.1142005\"},{\"key\":\"ref48\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1007\\/978-1-4757-3121-7\"},{\"key\":\"ref47\",\"author\":\"vedaldi\",\"year\":\"2008\",\"journal-title\":\"Vlfeat An Open and Portable Library of Computer Vision Algorithms\"},{\"key\":\"ref42\",\"article-title\":\"Training very deep networks\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"1507 06228\"},{\"key\":\"ref41\",\"author\":\"srivastava\",\"year\":\"2015\",\"journal-title\":\"Highway networks\"},{\"key\":\"ref44\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/34.56188\"},{\"key\":\"ref43\",\"doi-asserted-by\":\"publisher\",\"DOI\":\"10.1109\\/CVPR.2015.7298594\"}],\"event\":{\"name\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"location\":\"Las Vegas, NV, USA\",\"start\":{\"date-parts\":[[2016,6,27]]},\"end\":{\"date-parts\":[[2016,6,30]]}},\"container-title\":[\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\"],\"link\":[{\"URL\":\"http:\\/\\/xplorestaging.ieee.org\\/ielx7\\/7776647\\/7780329\\/07780459.pdf?arnumber=7780459\",\"content-type\":\"unspecified\",\"content-version\":\"vor\",\"intended-application\":\"similarity-checking\"}],\"deposited\":{\"date-parts\":[[2019,9,16]],\"date-time\":\"2019-09-16T12:51:26Z\",\"timestamp\":1568638286000},\"score\":34.532963,\"resource\":{\"primary\":{\"URL\":\"http:\\/\\/ieeexplore.ieee.org\\/document\\/7780459\\/\"}},\"issued\":{\"date-parts\":[[2016,6]]},\"references-count\":49,\"URL\":\"http:\\/\\/dx.doi.org\\/10.1109\\/cvpr.2016.90\",\"published\":{\"date-parts\":[[2016,6]]}},{\"indexed\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:41:59Z\",\"timestamp\":1657831319468},\"reference-count\":0,\"publisher\":\"SCITEPRESS - Science and Technology Publications\",\"content-domain\":{\"domain\":[],\"crossmark-restriction\":false},\"published-print\":{\"date-parts\":[[2022]]},\"DOI\":\"10.5220\\/0011270400003277\",\"type\":\"proceedings-article\",\"created\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:19:02Z\",\"timestamp\":1657829942000},\"source\":\"Crossref\",\"is-referenced-by-count\":0,\"title\":[\"RRConvNet: Recursive-residual Network for Real-life Character Image Recognition\"],\"prefix\":\"10.5220\",\"author\":[{\"given\":\"Tadele\",\"family\":\"Mengiste\",\"sequence\":\"first\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Birhanu\",\"family\":\"Belay\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Bezawork\",\"family\":\"Tilahun\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Tsiyon\",\"family\":\"Worku\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]},{\"given\":\"Tesfa\",\"family\":\"Tegegne\",\"sequence\":\"additional\",\"affiliation\":[{\"name\":\"Faculty of Computing, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"},{\"name\":\"ICT4D Research Center, Bahir Dar Institute of Technology, Bahir Dar, Ethiopia, --- Select a Country ---\"}]}],\"member\":\"3171\",\"event\":{\"name\":\"3rd International Conference on Deep Learning Theory and Applications\",\"location\":\"Lisbon, Portugal\",\"start\":{\"date-parts\":[[2022,7,12]]},\"end\":{\"date-parts\":[[2022,7,14]]}},\"container-title\":[\"Proceedings of the 3rd International Conference on Deep Learning Theory and Applications\"],\"original-title\":[\"RRConvNet: Recursive-residual Network for Real-life Character Image Recognition\"],\"deposited\":{\"date-parts\":[[2022,7,14]],\"date-time\":\"2022-07-14T20:19:12Z\",\"timestamp\":1657829952000},\"score\":33.44805,\"resource\":{\"primary\":{\"URL\":\"https:\\/\\/www.scitepress.org\\/DigitalLibrary\\/Link.aspx?doi=10.5220\\/0011270400003277\"}},\"subtitle\":[\"\"],\"issued\":{\"date-parts\":[[2022]]},\"references-count\":0,\"URL\":\"http:\\/\\/dx.doi.org\\/10.5220\\/0011270400003277\",\"published\":{\"date-parts\":[[2022]]}}],\"items-per-page\":2,\"query\":{\"start-index\":0,\"search-terms\":\"Deep Residual Learning for Image Recognition\"}}}","http://api.semanticscholar.org/graph/v1/paper/search?query=imagenet&offset=0&limit=10&fields=externalIds%2Curl%2Ctitle%2Cvenue%2Cyear%2CcitationCount%2Cauthors":"{\"total\": 19348, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"citationCount\": 27227, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"externalIds\": {\"ArXiv\": \"1502.01852\", \"MAG\": \"1677182931\", \"DBLP\": \"conf/iccv/HeZRS15\", \"DOI\": \"10.1109/ICCV.2015.123\", \"CorpusId\": 13740328}, \"url\": \"https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"title\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"venue\": \"2015 IEEE International Conference on Computer Vision (ICCV)\", \"year\": 2015, \"citationCount\": 13349, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"externalIds\": {\"MAG\": \"2997031122\", \"DBLP\": \"conf/nips/KrizhevskySH12\", \"DOI\": \"10.1145/3065386\", \"CorpusId\": 195908774}, \"url\": \"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"title\": \"ImageNet classification with deep convolutional neural networks\", \"venue\": \"Commun. ACM\", \"year\": 2012, \"citationCount\": 87095, \"authors\": [{\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}, {\"authorId\": \"1701686\", \"name\": \"Ilya Sutskever\"}, {\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}]}, {\"paperId\": \"0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"externalIds\": {\"DBLP\": \"journals/corr/GoyalDGNWKTJH17\", \"ArXiv\": \"1706.02677\", \"MAG\": \"2622263826\", \"CorpusId\": 13905106}, \"url\": \"https://www.semanticscholar.org/paper/0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"title\": \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"venue\": \"ArXiv\", \"year\": 2017, \"citationCount\": 2387, \"authors\": [{\"authorId\": \"47316088\", \"name\": \"Priya Goyal\"}, {\"authorId\": \"3127283\", \"name\": \"Piotr Doll\\u00e1r\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"34837514\", \"name\": \"P. Noordhuis\"}, {\"authorId\": \"2065373815\", \"name\": \"Lukasz Wesolowski\"}, {\"authorId\": \"1717990\", \"name\": \"Aapo Kyrola\"}, {\"authorId\": \"3609856\", \"name\": \"Andrew Tulloch\"}, {\"authorId\": \"39978391\", \"name\": \"Yangqing Jia\"}, {\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}]}, {\"paperId\": \"d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"externalIds\": {\"MAG\": \"2108598243\", \"DBLP\": \"conf/cvpr/DengDSLL009\", \"DOI\": \"10.1109/CVPR.2009.5206848\", \"CorpusId\": 57246310}, \"url\": \"https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"title\": \"ImageNet: A large-scale hierarchical image database\", \"venue\": \"2009 IEEE Conference on Computer Vision and Pattern Recognition\", \"year\": 2009, \"citationCount\": 36326, \"authors\": [{\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144847596\", \"name\": \"Wei Dong\"}, {\"authorId\": \"2166511\", \"name\": \"R. Socher\"}, {\"authorId\": \"2040091191\", \"name\": \"Li-Jia Li\"}, {\"authorId\": \"94451829\", \"name\": \"K. Li\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"externalIds\": {\"DBLP\": \"conf/cvpr/XieLHL20\", \"MAG\": \"2985963903\", \"ArXiv\": \"1911.04252\", \"DOI\": \"10.1109/cvpr42600.2020.01070\", \"CorpusId\": 207853355}, \"url\": \"https://www.semanticscholar.org/paper/20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"title\": \"Self-Training With Noisy Student Improves ImageNet Classification\", \"venue\": \"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\", \"year\": 2019, \"citationCount\": 1180, \"authors\": [{\"authorId\": \"1912046\", \"name\": \"Qizhe Xie\"}, {\"authorId\": \"144547315\", \"name\": \"E. Hovy\"}, {\"authorId\": \"1707242\", \"name\": \"Minh-Thang Luong\"}, {\"authorId\": \"2827616\", \"name\": \"Quoc V. Le\"}]}, {\"paperId\": \"b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"externalIds\": {\"DBLP\": \"journals/corr/RastegariORF16\", \"ArXiv\": \"1603.05279\", \"MAG\": \"2951978180\", \"DOI\": \"10.1007/978-3-319-46493-0_32\", \"CorpusId\": 14925907}, \"url\": \"https://www.semanticscholar.org/paper/b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"title\": \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\", \"venue\": \"ECCV\", \"year\": 2016, \"citationCount\": 2877, \"authors\": [{\"authorId\": \"143887493\", \"name\": \"Mohammad Rastegari\"}, {\"authorId\": \"2004053\", \"name\": \"Vicente Ordonez\"}, {\"authorId\": \"40497777\", \"name\": \"Joseph Redmon\"}, {\"authorId\": \"143787583\", \"name\": \"Ali Farhadi\"}]}, {\"paperId\": \"dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2101-11986\", \"ArXiv\": \"2101.11986\", \"DOI\": \"10.1109/ICCV48922.2021.00060\", \"CorpusId\": 231719476}, \"url\": \"https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"title\": \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\", \"venue\": \"2021 IEEE/CVF International Conference on Computer Vision (ICCV)\", \"year\": 2021, \"citationCount\": 587, \"authors\": [{\"authorId\": \"2087091296\", \"name\": \"Li Yuan\"}, {\"authorId\": \"2144861793\", \"name\": \"Yunpeng Chen\"}, {\"authorId\": \"143988955\", \"name\": \"Tao Wang\"}, {\"authorId\": \"23476952\", \"name\": \"Weihao Yu\"}, {\"authorId\": \"145356288\", \"name\": \"Yujun Shi\"}, {\"authorId\": \"40983412\", \"name\": \"Francis E. H. Tay\"}, {\"authorId\": \"33221685\", \"name\": \"Jiashi Feng\"}, {\"authorId\": \"143653681\", \"name\": \"Shuicheng Yan\"}]}, {\"paperId\": \"0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-1811-12231\", \"ArXiv\": \"1811.12231\", \"MAG\": \"2902617128\", \"CorpusId\": 54101493}, \"url\": \"https://www.semanticscholar.org/paper/0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"title\": \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\", \"venue\": \"ICLR\", \"year\": 2018, \"citationCount\": 1404, \"authors\": [{\"authorId\": \"1949747\", \"name\": \"Robert Geirhos\"}, {\"authorId\": \"52096618\", \"name\": \"Patricia Rubisch\"}, {\"authorId\": \"40899528\", \"name\": \"Claudio Michaelis\"}, {\"authorId\": \"1731199\", \"name\": \"M. Bethge\"}, {\"authorId\": \"1924112\", \"name\": \"Felix Wichmann\"}, {\"authorId\": \"40634590\", \"name\": \"Wieland Brendel\"}]}, {\"paperId\": \"d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"externalIds\": {\"MAG\": \"2952681253\", \"DBLP\": \"conf/cvpr/HaraKS18\", \"ArXiv\": \"1711.09577\", \"DOI\": \"10.1109/CVPR.2018.00685\", \"CorpusId\": 4539700}, \"url\": \"https://www.semanticscholar.org/paper/d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"title\": \"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\", \"venue\": \"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\", \"year\": 2017, \"citationCount\": 1195, \"authors\": [{\"authorId\": \"2199251\", \"name\": \"Kensho Hara\"}, {\"authorId\": \"1730200\", \"name\": \"Hirokatsu Kataoka\"}, {\"authorId\": \"1732705\", \"name\": \"Y. Satoh\"}]}]}\n","https://export.arxiv.org/api/query?search_query=imagenet&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dimagenet%26id_list%3D%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=imagenet&amp;id_list=&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/wvZQ8wg1Vo6qLNcbflCFhmHjUGY</id>\n  <updated>2022-10-02T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">4425</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/2205.10660v1</id>\n    <updated>2022-05-21T19:48:28Z</updated>\n    <published>2022-05-21T19:48:28Z</published>\n    <title>Vision Transformers in 2022: An Update on Tiny ImageNet</title>\n    <summary>  The recent advances in image transformers have shown impressive results and\nhave largely closed the gap between traditional CNN architectures. The standard\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\nImageNet-1k. After finetuning, researches will often consider the transfer\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\nTiny ImageNet. This paper offers an update on vision transformers' performance\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\nresult with a validation accuracy of 91.35%. Code is available here:\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\n</summary>\n    <author>\n      <name>Ethan Huynh</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2205.10660v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.10660v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2006.07159v1</id>\n    <updated>2020-06-12T13:17:25Z</updated>\n    <published>2020-06-12T13:17:25Z</published>\n    <title>Are we done with ImageNet?</title>\n    <summary>  Yes, and no. We ask whether recent progress on the ImageNet classification\nbenchmark continues to represent meaningful generalization, or whether the\ncommunity has started to overfit to the idiosyncrasies of its labeling\nprocedure. We therefore develop a significantly more robust procedure for\ncollecting human annotations of the ImageNet validation set. Using these new\nlabels, we reassess the accuracy of recently proposed ImageNet classifiers, and\nfind their gains to be substantially smaller than those reported on the\noriginal labels. Furthermore, we find the original ImageNet labels to no longer\nbe the best predictors of this independently-collected set, indicating that\ntheir usefulness in evaluating vision models may be nearing an end.\nNevertheless, we find our annotation procedure to have largely remedied the\nerrors in the original labels, reinforcing ImageNet as a powerful benchmark for\nfuture research in visual recognition.\n</summary>\n    <author>\n      <name>Lucas Beyer</name>\n    </author>\n    <author>\n      <name>Olivier J. Hénaff</name>\n    </author>\n    <author>\n      <name>Alexander Kolesnikov</name>\n    </author>\n    <author>\n      <name>Xiaohua Zhai</name>\n    </author>\n    <author>\n      <name>Aäron van den Oord</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">All five authors contributed equally. New labels at\n  https://github.com/google-research/reassessed-imagenet</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2006.07159v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.07159v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2204.03934v1</id>\n    <updated>2022-04-08T08:55:34Z</updated>\n    <published>2022-04-08T08:55:34Z</published>\n    <title>Does Robustness on ImageNet Transfer to Downstream Tasks?</title>\n    <summary>  As clean ImageNet accuracy nears its ceiling, the research community is\nincreasingly more concerned about robust accuracy under distributional shifts.\nWhile a variety of methods have been proposed to robustify neural networks,\nthese techniques often target models trained on ImageNet classification. At the\nsame time, it is a common practice to use ImageNet pretrained backbones for\ndownstream tasks such as object detection, semantic segmentation, and image\nclassification from different domains. This raises a question: Can these robust\nimage classifiers transfer robustness to downstream tasks? For object detection\nand semantic segmentation, we find that a vanilla Swin Transformer, a variant\nof Vision Transformer tailored for dense prediction tasks, transfers robustness\nbetter than Convolutional Neural Networks that are trained to be robust to the\ncorrupted version of ImageNet. For CIFAR10 classification, we find that models\nthat are robustified for ImageNet do not retain robustness when fully\nfine-tuned. These findings suggest that current robustification techniques tend\nto emphasize ImageNet evaluations. Moreover, network architecture is a strong\nsource of robustness when we consider transfer learning.\n</summary>\n    <author>\n      <name>Yutaro Yamada</name>\n    </author>\n    <author>\n      <name>Mayu Otani</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CVPR 2022</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2204.03934v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2204.03934v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1805.08974v3</id>\n    <updated>2019-06-17T16:25:07Z</updated>\n    <published>2018-05-23T06:12:35Z</published>\n    <title>Do Better ImageNet Models Transfer Better?</title>\n    <summary>  Transfer learning is a cornerstone of computer vision, yet little work has\nbeen done to evaluate the relationship between architecture and transfer. An\nimplicit hypothesis in modern computer vision research is that models that\nperform better on ImageNet necessarily perform better on other vision tasks.\nHowever, this hypothesis has never been systematically tested. Here, we compare\nthe performance of 16 classification networks on 12 image classification\ndatasets. We find that, when networks are used as fixed feature extractors or\nfine-tuned, there is a strong correlation between ImageNet accuracy and\ntransfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting,\nwe find that this relationship is very sensitive to the way in which networks\nare trained on ImageNet; many common forms of regularization slightly improve\nImageNet accuracy but yield penultimate layer features that are much worse for\ntransfer learning. Additionally, we find that, on two small fine-grained image\nclassification datasets, pretraining on ImageNet provides minimal benefits,\nindicating the learned features from ImageNet do not transfer well to\nfine-grained tasks. Together, our results show that ImageNet architectures\ngeneralize well across datasets, but ImageNet features are less general than\npreviously suggested.\n</summary>\n    <author>\n      <name>Simon Kornblith</name>\n    </author>\n    <author>\n      <name>Jonathon Shlens</name>\n    </author>\n    <author>\n      <name>Quoc V. Le</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CVPR 2019 Oral</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1805.08974v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1805.08974v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2101.06871v2</id>\n    <updated>2021-02-21T02:06:43Z</updated>\n    <published>2021-01-18T04:48:24Z</published>\n    <title>CheXtransfer: Performance and Parameter Efficiency of ImageNet Models\n  for Chest X-Ray Interpretation</title>\n    <summary>  Deep learning methods for chest X-ray interpretation typically rely on\npretrained models developed for ImageNet. This paradigm assumes that better\nImageNet architectures perform better on chest X-ray tasks and that\nImageNet-pretrained weights provide a performance boost over random\ninitialization. In this work, we compare the transfer performance and parameter\nefficiency of 16 popular convolutional architectures on a large chest X-ray\ndataset (CheXpert) to investigate these assumptions. First, we find no\nrelationship between ImageNet performance and CheXpert performance for both\nmodels without pretraining and models with pretraining. Second, we find that,\nfor models without pretraining, the choice of model family influences\nperformance more than size within a family for medical imaging tasks. Third, we\nobserve that ImageNet pretraining yields a statistically significant boost in\nperformance across architectures, with a higher boost for smaller\narchitectures. Fourth, we examine whether ImageNet architectures are\nunnecessarily large for CheXpert by truncating final blocks from pretrained\nmodels, and find that we can make models 3.25x more parameter-efficient on\naverage without a statistically significant drop in performance. Our work\ncontributes new experimental evidence about the relation of ImageNet to chest\nx-ray interpretation performance.\n</summary>\n    <author>\n      <name>Alexander Ke</name>\n    </author>\n    <author>\n      <name>William Ellsworth</name>\n    </author>\n    <author>\n      <name>Oishi Banerjee</name>\n    </author>\n    <author>\n      <name>Andrew Y. Ng</name>\n    </author>\n    <author>\n      <name>Pranav Rajpurkar</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3450439.3451867</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3450439.3451867\" rel=\"related\"/>\n    <link href=\"http://arxiv.org/abs/2101.06871v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.06871v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1707.08819v3</id>\n    <updated>2017-08-23T16:06:20Z</updated>\n    <published>2017-07-27T11:22:22Z</published>\n    <title>A Downsampled Variant of ImageNet as an Alternative to the CIFAR\n  datasets</title>\n    <summary>  The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and\nImageNet16$\\times$16) contains exactly the same number of classes and images as\nImageNet, with the only difference that the images are downsampled to\n32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the\nvariants, respectively). Experiments on these downsampled variants are\ndramatically faster than on the original ImageNet and the characteristics of\nthe downsampled datasets with respect to optimal hyperparameters appear to\nremain similar. The proposed datasets and scripts to reproduce our results are\navailable at http://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\n</summary>\n    <author>\n      <name>Patryk Chrabaszcz</name>\n    </author>\n    <author>\n      <name>Ilya Loshchilov</name>\n    </author>\n    <author>\n      <name>Frank Hutter</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1707.08819v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1707.08819v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1709.03439v1</id>\n    <updated>2017-09-11T15:35:05Z</updated>\n    <published>2017-09-11T15:35:05Z</published>\n    <title>Why Do Deep Neural Networks Still Not Recognize These Images?: A\n  Qualitative Analysis on Failure Cases of ImageNet Classification</title>\n    <summary>  In a recent decade, ImageNet has become the most notable and powerful\nbenchmark database in computer vision and machine learning community. As\nImageNet has emerged as a representative benchmark for evaluating the\nperformance of novel deep learning models, its evaluation tends to include only\nquantitative measures such as error rate, rather than qualitative analysis.\nThus, there are few studies that analyze the failure cases of deep learning\nmodels in ImageNet, though there are numerous works analyzing the networks\nthemselves and visualizing them. In this abstract, we qualitatively analyze the\nfailure cases of ImageNet classification results from recent deep learning\nmodel, and categorize these cases according to the certain image patterns.\nThrough this failure analysis, we believe that it can be discovered what the\nfinal challenges are in ImageNet database, which the current deep learning\nmodel is still vulnerable to.\n</summary>\n    <author>\n      <name>Han S. Lee</name>\n    </author>\n    <author>\n      <name>Alex A. Agarwal</name>\n    </author>\n    <author>\n      <name>Junmo Kim</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Poster presented at CVPR 2017 Scene Understanding Workshop</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1709.03439v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1709.03439v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1908.08016v2</id>\n    <updated>2020-06-09T05:17:48Z</updated>\n    <published>2019-08-21T17:36:48Z</published>\n    <title>Testing Robustness Against Unforeseen Adversaries</title>\n    <summary>  Most existing adversarial defenses only measure robustness to L_p adversarial\nattacks. Not only are adversaries unlikely to exclusively create small L_p\nperturbations, adversaries are unlikely to remain fixed. Adversaries adapt and\nevolve their attacks; hence adversarial defenses must be robust to a broad\nrange of unforeseen attacks. We address this discrepancy between research and\nreality by proposing a new evaluation framework called ImageNet-UA. Our\nframework enables the research community to test ImageNet model robustness\nagainst attacks not encountered during training. To create ImageNet-UA's\ndiverse attack suite, we introduce a total of four novel adversarial attacks.\nWe also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf\nrobustness assessments give a narrow account of model robustness. By evaluating\ncurrent defenses with ImageNet-UA, we find they provide little robustness to\nunforeseen attacks. We hope the greater variety and realism of ImageNet-UA\nenables development of more robust defenses which can generalize beyond attacks\nseen during training.\n</summary>\n    <author>\n      <name>Daniel Kang</name>\n    </author>\n    <author>\n      <name>Yi Sun</name>\n    </author>\n    <author>\n      <name>Dan Hendrycks</name>\n    </author>\n    <author>\n      <name>Tom Brown</name>\n    </author>\n    <author>\n      <name>Jacob Steinhardt</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1908.08016v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1908.08016v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2104.10972v4</id>\n    <updated>2021-08-05T15:04:28Z</updated>\n    <published>2021-04-22T10:10:14Z</published>\n    <title>ImageNet-21K Pretraining for the Masses</title>\n    <summary>  ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K\n</summary>\n    <author>\n      <name>Tal Ridnik</name>\n    </author>\n    <author>\n      <name>Emanuel Ben-Baruch</name>\n    </author>\n    <author>\n      <name>Asaf Noy</name>\n    </author>\n    <author>\n      <name>Lihi Zelnik-Manor</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to NeurIPS 2021 (Datasets and Benchmarks)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2104.10972v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2104.10972v4\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2105.05233v4</id>\n    <updated>2021-06-01T17:49:49Z</updated>\n    <published>2021-05-11T17:50:24Z</published>\n    <title>Diffusion Models Beat GANs on Image Synthesis</title>\n    <summary>  We show that diffusion models can achieve image sample quality superior to\nthe current state-of-the-art generative models. We achieve this on\nunconditional image synthesis by finding a better architecture through a series\nof ablations. For conditional image synthesis, we further improve sample\nquality with classifier guidance: a simple, compute-efficient method for\ntrading off diversity for fidelity using gradients from a classifier. We\nachieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet\n256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep\neven with as few as 25 forward passes per sample, all while maintaining better\ncoverage of the distribution. Finally, we find that classifier guidance\ncombines well with upsampling diffusion models, further improving FID to 3.94\non ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our\ncode at https://github.com/openai/guided-diffusion\n</summary>\n    <author>\n      <name>Prafulla Dhariwal</name>\n    </author>\n    <author>\n      <name>Alex Nichol</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Added compute requirements, ImageNet 256$\\times$256 upsampling FID\n  and samples, DDIM guided sampler, fixed typos</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2105.05233v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.05233v4\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1409.0575?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors":"{\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"abstract\": null, \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"referenceCount\": 108, \"citationCount\": 27227, \"influentialCitationCount\": 4153, \"isOpenAccess\": true, \"fieldsOfStudy\": [\"Computer Science\"], \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}\n","https://export.arxiv.org/api/query?id_list=1409.0575&start=0&max_results=1&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1409.0575%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1409.0575&amp;start=0&amp;max_results=1</title>\n  <id>http://arxiv.org/api/f4yLhWAEx5hhI4PpFonj7rhpCjY</id>\n  <updated>2022-10-02T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1409.0575v3</id>\n    <updated>2015-01-30T01:23:59Z</updated>\n    <published>2014-09-01T22:29:38Z</published>\n    <title>ImageNet Large Scale Visual Recognition Challenge</title>\n    <summary>  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in\nobject category classification and detection on hundreds of object categories\nand millions of images. The challenge has been run annually from 2010 to\npresent, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances\nin object recognition that have been possible as a result. We discuss the\nchallenges of collecting large-scale ground truth annotation, highlight key\nbreakthroughs in categorical object recognition, provide a detailed analysis of\nthe current state of the field of large-scale image classification and object\ndetection, and compare the state-of-the-art computer vision accuracy with human\naccuracy. We conclude with lessons learned in the five years of the challenge,\nand propose future directions and improvements.\n</summary>\n    <author>\n      <name>Olga Russakovsky</name>\n    </author>\n    <author>\n      <name>Jia Deng</name>\n    </author>\n    <author>\n      <name>Hao Su</name>\n    </author>\n    <author>\n      <name>Jonathan Krause</name>\n    </author>\n    <author>\n      <name>Sanjeev Satheesh</name>\n    </author>\n    <author>\n      <name>Sean Ma</name>\n    </author>\n    <author>\n      <name>Zhiheng Huang</name>\n    </author>\n    <author>\n      <name>Andrej Karpathy</name>\n    </author>\n    <author>\n      <name>Aditya Khosla</name>\n    </author>\n    <author>\n      <name>Michael Bernstein</name>\n    </author>\n    <author>\n      <name>Alexander C. Berg</name>\n    </author>\n    <author>\n      <name>Li Fei-Fei</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">43 pages, 16 figures. v3 includes additional comparisons with PASCAL\n  VOC (per-category comparisons in Table 3, distribution of localization\n  difficulty in Fig 16), a list of queries used for obtaining object detection\n  images (Appendix C), and some additional references</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1409.0575v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.0575v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"I.4.8; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.semanticscholar.org/graph/v1/author/2192178?fields=affiliations,authorId,name,url":"{\"authorId\": \"2192178\", \"url\": \"https://www.semanticscholar.org/author/2192178\", \"name\": \"Olga Russakovsky\", \"affiliations\": []}\n","https://api.semanticscholar.org/graph/v1/author/153302678?fields=affiliations,authorId,name,url":"{\"authorId\": \"153302678\", \"url\": \"https://www.semanticscholar.org/author/153302678\", \"name\": \"Jia Deng\", \"affiliations\": []}\n","https://api.semanticscholar.org/graph/v1/author/144914140?fields=affiliations,authorId,name,url":"{\"authorId\": \"144914140\", \"url\": \"https://www.semanticscholar.org/author/144914140\", \"name\": \"Hao Su\", \"affiliations\": []}\n","https://api.openreview.net/notes/search?query=diffusion+models+beat+gans+on+image+synthesis&limit=2&offset=0":"{\"notes\":[{\"id\":\"OU98jZWS3x_\",\"original\":\"GvWVaxotcNl\",\"number\":8389,\"cdate\":1621629767342,\"mdate\":null,\"ddate\":null,\"tcdate\":1623678013435,\"tmdate\":1643159152266,\"tddate\":null,\"forum\":\"OU98jZWS3x_\",\"replyto\":null,\"invitation\":\"NeurIPS.cc/2021/Conference/-/Blind_Submission\",\"content\":{\"title\":\"Diffusion Models Beat GANs on Image Synthesis\",\"authorids\":[\"~Prafulla_Dhariwal1\",\"~Alexander_Quinn_Nichol1\"],\"authors\":[\"Prafulla Dhariwal\",\"Alexander Quinn Nichol\"],\"keywords\":[\"generative models\",\"diffusion models\",\"score-based models\",\"denoising diffusion probabilistic models\",\"image generation\",\"neural networks\",\"attention\"],\"TL;DR\":\"We achieve state-of-the-art image generation on ImageNet and several LSUN classes with diffusion models.\",\"abstract\":\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\\\times$128, 4.59 on ImageNet 256$\\\\times$256, and 7.72 on ImageNet 512$\\\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\\\times$256 and 3.85 on ImageNet 512$\\\\times$512.\",\"submission_history\":\"\",\"code_of_conduct\":\"I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.\",\"paperhash\":\"dhariwal|diffusion_models_beat_gans_on_image_synthesis\",\"pdf\":\"/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf\",\"supplementary_material\":\"/attachment/c47d3fa2b246d4d640583c0ee3166c25b0eaef15.pdf\",\"checklist\":\"\",\"code\":\"https://github.com/openai/guided-diffusion\",\"thumbnail\":\"\",\"submission_history_-_venue_and_year\":\"\",\"submission_history_-_improvements_made\":\"\",\"_bibtex\":\"@inproceedings{\\ndhariwal2021diffusion,\\ntitle={Diffusion Models Beat {GAN}s on Image Synthesis},\\nauthor={Prafulla Dhariwal and Alexander Quinn Nichol},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},\\nyear={2021},\\nurl={https://openreview.net/forum?id=OU98jZWS3x_}\\n}\",\"venue\":\"NeurIPS 2021 Spotlight\",\"venueid\":\"NeurIPS.cc/2021/Conference\"},\"signatures\":[\"NeurIPS.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"NeurIPS.cc/2021/Conference\"]},{\"id\":\"AAWuCvzaVt\",\"original\":\"GvWVaxotcNl\",\"number\":1854,\"cdate\":1621629767342,\"ddate\":null,\"tcdate\":1621629767342,\"tmdate\":1643159152266,\"tddate\":null,\"forum\":\"AAWuCvzaVt\",\"replyto\":null,\"invitation\":\"NeurIPS.cc/2021/Conference/-/Blind_Submission\",\"content\":{\"title\":\"Diffusion Models Beat GANs on Image Synthesis\",\"authorids\":[\"~Prafulla_Dhariwal1\",\"~Alexander_Quinn_Nichol1\"],\"authors\":[\"Prafulla Dhariwal\",\"Alexander Quinn Nichol\"],\"keywords\":[\"generative models\",\"diffusion models\",\"score-based models\",\"denoising diffusion probabilistic models\",\"image generation\",\"neural networks\",\"attention\"],\"TL;DR\":\"We achieve state-of-the-art image generation on ImageNet and several LSUN classes with diffusion models.\",\"abstract\":\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\\\times$128, 4.59 on ImageNet 256$\\\\times$256, and 7.72 on ImageNet 512$\\\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\\\times$256 and 3.85 on ImageNet 512$\\\\times$512.\",\"submission_history\":\"\",\"code_of_conduct\":\"I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.\",\"paperhash\":\"dhariwal|diffusion_models_beat_gans_on_image_synthesis\",\"pdf\":\"/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf\",\"supplementary_material\":\"/attachment/c47d3fa2b246d4d640583c0ee3166c25b0eaef15.pdf\",\"checklist\":\"\",\"code\":\"https://github.com/openai/guided-diffusion\",\"thumbnail\":\"\",\"submission_history_-_venue_and_year\":\"\",\"submission_history_-_improvements_made\":\"\",\"_bibtex\":\"@inproceedings{\\ndhariwal2021diffusion,\\ntitle={Diffusion Models Beat {GAN}s on Image Synthesis},\\nauthor={Prafulla Dhariwal and Alexander Quinn Nichol},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},\\nyear={2021},\\nurl={https://openreview.net/forum?id=AAWuCvzaVt}\\n}\",\"venue\":\"NeurIPS 2021 Spotlight\",\"venueid\":\"NeurIPS.cc/2021/Conference\"},\"signatures\":[\"NeurIPS.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"NeurIPS.cc/2021/Conference\"],\"mdate\":null}],\"count\":10000}","https://api.openreview.net/notes/search?query=diffusion+models+beat+gans+on+image+synthesis&limit=5&offset=0":"{\"notes\":[{\"id\":\"OU98jZWS3x_\",\"original\":\"GvWVaxotcNl\",\"number\":8389,\"cdate\":1621629767342,\"mdate\":null,\"ddate\":null,\"tcdate\":1623678013435,\"tmdate\":1643159152266,\"tddate\":null,\"forum\":\"OU98jZWS3x_\",\"replyto\":null,\"invitation\":\"NeurIPS.cc/2021/Conference/-/Blind_Submission\",\"content\":{\"title\":\"Diffusion Models Beat GANs on Image Synthesis\",\"authorids\":[\"~Prafulla_Dhariwal1\",\"~Alexander_Quinn_Nichol1\"],\"authors\":[\"Prafulla Dhariwal\",\"Alexander Quinn Nichol\"],\"keywords\":[\"generative models\",\"diffusion models\",\"score-based models\",\"denoising diffusion probabilistic models\",\"image generation\",\"neural networks\",\"attention\"],\"TL;DR\":\"We achieve state-of-the-art image generation on ImageNet and several LSUN classes with diffusion models.\",\"abstract\":\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\\\times$128, 4.59 on ImageNet 256$\\\\times$256, and 7.72 on ImageNet 512$\\\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\\\times$256 and 3.85 on ImageNet 512$\\\\times$512.\",\"submission_history\":\"\",\"code_of_conduct\":\"I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.\",\"paperhash\":\"dhariwal|diffusion_models_beat_gans_on_image_synthesis\",\"pdf\":\"/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf\",\"supplementary_material\":\"/attachment/c47d3fa2b246d4d640583c0ee3166c25b0eaef15.pdf\",\"checklist\":\"\",\"code\":\"https://github.com/openai/guided-diffusion\",\"thumbnail\":\"\",\"submission_history_-_venue_and_year\":\"\",\"submission_history_-_improvements_made\":\"\",\"_bibtex\":\"@inproceedings{\\ndhariwal2021diffusion,\\ntitle={Diffusion Models Beat {GAN}s on Image Synthesis},\\nauthor={Prafulla Dhariwal and Alexander Quinn Nichol},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},\\nyear={2021},\\nurl={https://openreview.net/forum?id=OU98jZWS3x_}\\n}\",\"venue\":\"NeurIPS 2021 Spotlight\",\"venueid\":\"NeurIPS.cc/2021/Conference\"},\"signatures\":[\"NeurIPS.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"NeurIPS.cc/2021/Conference\"]},{\"id\":\"AAWuCvzaVt\",\"original\":\"GvWVaxotcNl\",\"number\":1854,\"cdate\":1621629767342,\"ddate\":null,\"tcdate\":1621629767342,\"tmdate\":1643159152266,\"tddate\":null,\"forum\":\"AAWuCvzaVt\",\"replyto\":null,\"invitation\":\"NeurIPS.cc/2021/Conference/-/Blind_Submission\",\"content\":{\"title\":\"Diffusion Models Beat GANs on Image Synthesis\",\"authorids\":[\"~Prafulla_Dhariwal1\",\"~Alexander_Quinn_Nichol1\"],\"authors\":[\"Prafulla Dhariwal\",\"Alexander Quinn Nichol\"],\"keywords\":[\"generative models\",\"diffusion models\",\"score-based models\",\"denoising diffusion probabilistic models\",\"image generation\",\"neural networks\",\"attention\"],\"TL;DR\":\"We achieve state-of-the-art image generation on ImageNet and several LSUN classes with diffusion models.\",\"abstract\":\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\\\times$128, 4.59 on ImageNet 256$\\\\times$256, and 7.72 on ImageNet 512$\\\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\\\times$256 and 3.85 on ImageNet 512$\\\\times$512.\",\"submission_history\":\"\",\"code_of_conduct\":\"I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.\",\"paperhash\":\"dhariwal|diffusion_models_beat_gans_on_image_synthesis\",\"pdf\":\"/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf\",\"supplementary_material\":\"/attachment/c47d3fa2b246d4d640583c0ee3166c25b0eaef15.pdf\",\"checklist\":\"\",\"code\":\"https://github.com/openai/guided-diffusion\",\"thumbnail\":\"\",\"submission_history_-_venue_and_year\":\"\",\"submission_history_-_improvements_made\":\"\",\"_bibtex\":\"@inproceedings{\\ndhariwal2021diffusion,\\ntitle={Diffusion Models Beat {GAN}s on Image Synthesis},\\nauthor={Prafulla Dhariwal and Alexander Quinn Nichol},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},\\nyear={2021},\\nurl={https://openreview.net/forum?id=AAWuCvzaVt}\\n}\",\"venue\":\"NeurIPS 2021 Spotlight\",\"venueid\":\"NeurIPS.cc/2021/Conference\"},\"signatures\":[\"NeurIPS.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"NeurIPS.cc/2021/Conference\"],\"mdate\":null},{\"id\":\"-x5WuMO4APy\",\"original\":\"Jm8RA-ylgKV\",\"number\":2749,\"cdate\":1663850120938,\"mdate\":null,\"ddate\":null,\"tcdate\":1663850120938,\"tmdate\":1665678617844,\"tddate\":null,\"forum\":\"-x5WuMO4APy\",\"replyto\":null,\"invitation\":\"ICLR.cc/2023/Conference/-/Blind_Submission\",\"content\":{\"title\":\"FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis\",\"authorids\":[\"ICLR.cc/2023/Conference/Paper2749/Authors\"],\"authors\":[\"Anonymous\"],\"keywords\":[\"Speech synthesis\",\"Neural vocoder\",\"Diffusion probabilistic model\",\"Generative adversarial network\"],\"abstract\":\"FastDiff, as a class of denoising probabilistic models, has recently achieved impressive performances in speech synthesis. It utilizes a noise predictor to learn a tight inference schedule for skipping denoising steps. Despite the successful speedup of FastDiff, there is still room for improvements, e.g., further optimizing the speed-quality trade-off and accelerating DDPMs training procedures. After analyzing GANs and diffusion models in conditional speech synthesis, we find that: GANs produce samples but do not cover the whole distribution, and the coverage degree does not distinctly impact audio quality. Inspired by these observations, we propose to trade off diversity for quality and speed by incorporating GANs into diffusion models, introducing two GAN-empowered modeling perspectives: (1) FastDiff 2 (Diff-GAN), whose denoising distribution is parametrized by conditional GANs; and (2) FastDiff 2 (GAN-Diff), in which the denoising model is treated as a generator in GAN for adversarial training. Unlike the acceleration methods based on skipping the denoising steps, FastDiff 2 provides a principled way to speed up both the training and inference processes. Experimental results demonstrate that both variants of FastDiff 2 enjoy an efficient 4-step sampling process as in FastDiff yet demonstrate a superior sample quality. Audio samples are available at https://FastDiff2.github.io/.\",\"anonymous_url\":\"I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.\",\"no_acknowledgement_section\":\"I certify that there is no acknowledgement section in this submission for double blind review.\",\"code_of_ethics\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics\",\"submission_guidelines\":\"Yes\",\"resubmission\":\"\",\"student_author\":\"\",\"Please_choose_the_closest_area_that_your_submission_falls_into\":\"Applications (eg, speech processing, computer vision, NLP)\",\"paperhash\":\"anonymous|fastdiff_2_dually_incorporating_gans_into_diffusion_models_for_highquality_speech_synthesis\",\"TL;DR\":\"We propose FastDiff 2, a conditional diffusion model to trade off diversity for quality and speed by incorporating GANs into diffusion models.\",\"pdf\":\"/pdf/1df3606905c1c22ce8bab529e0a005a4d990f62e.pdf\",\"_bibtex\":\"@inproceedings{\\nanonymous2023fastdiff,\\ntitle={FastDiff 2: Dually Incorporating {GAN}s into Diffusion Models for High-Quality Speech Synthesis},\\nauthor={Anonymous},\\nbooktitle={Submitted to The Eleventh International Conference on Learning Representations },\\nyear={2023},\\nurl={https://openreview.net/forum?id=-x5WuMO4APy},\\nnote={under review}\\n}\"},\"signatures\":[\"ICLR.cc/2023/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2023/Conference\"]},{\"id\":\"HtoA0oT30jC\",\"original\":\"JAKVQT8kEpD\",\"number\":1884,\"cdate\":1663850015339,\"mdate\":null,\"ddate\":null,\"tcdate\":1663850015339,\"tmdate\":1665678162487,\"tddate\":null,\"forum\":\"HtoA0oT30jC\",\"replyto\":null,\"invitation\":\"ICLR.cc/2023/Conference/-/Blind_Submission\",\"content\":{\"title\":\"Novel View Synthesis with Diffusion Models\",\"authorids\":[\"ICLR.cc/2023/Conference/Paper1884/Authors\"],\"authors\":[\"Anonymous\"],\"keywords\":[\"3D\",\"diffusion\",\"ddpm\",\"novel\",\"view\",\"synthesis\",\"generative\",\"models\"],\"TL;DR\":\"Novel View Synthesis with diffusion models from as few a single image\",\"abstract\":\"We present 3DiM (pronounced \\\"three-dim\\\"), a diffusion model for 3D novel view synthesis from as few as a single image. The core of 3DiM is an image-to-image diffusion model -- 3DiM takes a single reference view and their poses as inputs, and generates a novel view via diffusion. 3DiM can then generate a full 3D consistent scene following our novel stochastic conditioning sampler: the output frames of the scene are generated autoregressively, and during the reverse diffusion process of each individual frame, we select a random conditioning frame from the set of previous frames at each denoising step. We demonstrate that stochastic conditioning yields much more 3D consistent results compared to the naive sampling process which only conditions on a single previous frame. We compare 3DiMs to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM's generated videos from a single view achieve much higher fidelity while being approximately 3D consistent. We also introduce a new evaluation methodology, 3D consistency scoring, to measure the 3D consistency of a generated object by training a neural field on the model's output views. 3DiMs are geometry free, do not rely on hyper-networks or test-time optimization for novel view synthesis, and allow a single model to easily scale to a large number of scenes.\",\"anonymous_url\":\"I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.\",\"no_acknowledgement_section\":\"I certify that there is no acknowledgement section in this submission for double blind review.\",\"code_of_ethics\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics\",\"submission_guidelines\":\"Yes\",\"resubmission\":\"\",\"student_author\":\"\",\"Please_choose_the_closest_area_that_your_submission_falls_into\":\"Generative models\",\"paperhash\":\"anonymous|novel_view_synthesis_with_diffusion_models\",\"pdf\":\"/pdf/f84f4d758029ef24a48d77b9a7d4b9b3020da9fc.pdf\",\"supplementary_material\":\"/attachment/84fb9108922dba109dfb021256ea6aefc394b322.zip\",\"_bibtex\":\"@inproceedings{\\nanonymous2023novel,\\ntitle={Novel View Synthesis with Diffusion Models},\\nauthor={Anonymous},\\nbooktitle={Submitted to The Eleventh International Conference on Learning Representations },\\nyear={2023},\\nurl={https://openreview.net/forum?id=HtoA0oT30jC},\\nnote={under review}\\n}\"},\"signatures\":[\"ICLR.cc/2023/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2023/Conference\"]},{\"id\":\"NsvkBeYEEEK\",\"original\":null,\"number\":2,\"cdate\":1626206172176,\"mdate\":null,\"ddate\":null,\"tcdate\":1626206172176,\"tmdate\":1626206585192,\"tddate\":null,\"forum\":\"AAWuCvzaVt\",\"replyto\":\"AAWuCvzaVt\",\"invitation\":\"NeurIPS.cc/2021/Conference/Paper1854/-/Official_Review\",\"content\":{\"rating\":\"7: Good paper, accept\",\"confidence\":\"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\",\"summary\":\"The authors demonstrate that through architectural improvements, diffusion models can achieve state-of-the-art image sample quality, beating previous GANs that have dominated the image generation task for years. They also demonstrate that the sample quality can be further improved post-training using classifier guidance [1,2].\\n\\n[1] Deep unsupervised learning using nonequilibrium thermodynamics (Sohl-Dickstein et al. 2015)  \\n[2] Score-based generative modeling through stochastic differential equations (Song et al. 2020)\\n\\n\",\"main_review\":\"The main focus of the paper is demonstrating the capabilities of diffusion models for image synthesis. The paper does a great job at conveying this message and is very clearly written. It is interesting to see that after less than a year of research interest around diffusion models, they can beat GANs which have dominated the image synthesis task for years.\\n\\nThe experiments are strong, containing an ablation study on the neural architecture, a study on the effect of classifier guidance on sample quality, as well as a larger-scale study using metrics beyond just FID, including sFID [2] and Precision and Recall [3] where state-of-the-art performance is demonstrated on several image datasets. \\n\\nIn terms of technical innovation, the paper has few contributions. The differences compared to [1] are mostly about scaling up parts of the model and using previously proposed changes, which is studied in the ablation study. This is however fine as it is not the main focus of the paper.\\n\\nOne point of potential improvement could be to include a short discussion / a column in Table 1 on the demands on computation/memory/parameter count as various model components are ablated.\\n\\n[1] Improved Denoising Diffusion Probabilistic Models (Nichol & Dhariwal, 2020)  \\n[2] Generating images with sparse representations (Nash et al., 2021)  \\n[3] Improved Precision and Recall Metric for Assessing Generative Models (Kynkäänniemi et al., 2019)\",\"limitations_and_societal_impact\":\"Limitations and Societal Impacts are adequately discussed in Sections 7 and 8, respectively.\",\"needs_ethics_review\":\"No\",\"ethics_review_area\":[],\"time_spent_reviewing\":\"3\",\"code_of_conduct\":\"While performing my duties as a reviewer (including writing reviews and participating in discussions), I have and will continue to abide by the NeurIPS code of conduct.\"},\"signatures\":[\"NeurIPS.cc/2021/Conference/Paper1854/Reviewer_rkcu\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"NeurIPS.cc/2021/Conference\",\"NeurIPS.cc/2021/Conference/Paper1854/Reviewer_rkcu\"],\"forumContent\":{\"title\":\"Diffusion Models Beat GANs on Image Synthesis\",\"authorids\":[\"~Prafulla_Dhariwal1\",\"~Alexander_Quinn_Nichol1\"],\"authors\":[\"Prafulla Dhariwal\",\"Alexander Quinn Nichol\"],\"keywords\":[\"generative models\",\"diffusion models\",\"score-based models\",\"denoising diffusion probabilistic models\",\"image generation\",\"neural networks\",\"attention\"],\"TL;DR\":\"We achieve state-of-the-art image generation on ImageNet and several LSUN classes with diffusion models.\",\"abstract\":\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\\\times$128, 4.59 on ImageNet 256$\\\\times$256, and 7.72 on ImageNet 512$\\\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\\\times$256 and 3.85 on ImageNet 512$\\\\times$512.\",\"submission_history\":\"\",\"code_of_conduct\":\"I certify that all co-authors of this work have read and commit to adhering to the NeurIPS Statement on Ethics, Fairness, Inclusivity, and Code of Conduct.\",\"paperhash\":\"dhariwal|diffusion_models_beat_gans_on_image_synthesis\",\"pdf\":\"/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf\",\"supplementary_material\":\"/attachment/c47d3fa2b246d4d640583c0ee3166c25b0eaef15.pdf\",\"checklist\":\"\",\"code\":\"https://github.com/openai/guided-diffusion\",\"thumbnail\":\"\",\"submission_history_-_venue_and_year\":\"\",\"submission_history_-_improvements_made\":\"\",\"_bibtex\":\"@inproceedings{\\ndhariwal2021diffusion,\\ntitle={Diffusion Models Beat {GAN}s on Image Synthesis},\\nauthor={Prafulla Dhariwal and Alexander Quinn Nichol},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},\\nyear={2021},\\nurl={https://openreview.net/forum?id=AAWuCvzaVt}\\n}\",\"venue\":\"NeurIPS 2021 Spotlight\",\"venueid\":\"NeurIPS.cc/2021/Conference\"}}],\"count\":10000}","https://api.semanticscholar.org/graph/v1/paper/search?query=imagenet&offset=0&limit=10&fields=externalIds%2Curl%2Ctitle%2Cvenue%2Cyear%2CcitationCount%2Cauthors":"{\"total\": 19938, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"citationCount\": 27809, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"externalIds\": {\"DBLP\": \"journals/corr/HeZR015\", \"MAG\": \"1677182931\", \"ArXiv\": \"1502.01852\", \"DOI\": \"10.1109/ICCV.2015.123\", \"CorpusId\": 13740328}, \"url\": \"https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"title\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"venue\": \"2015 IEEE International Conference on Computer Vision (ICCV)\", \"year\": 2015, \"citationCount\": 13635, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"externalIds\": {\"MAG\": \"2997031122\", \"DBLP\": \"conf/nips/KrizhevskySH12\", \"DOI\": \"10.1145/3065386\", \"CorpusId\": 195908774}, \"url\": \"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"title\": \"ImageNet classification with deep convolutional neural networks\", \"venue\": \"Commun. ACM\", \"year\": 2012, \"citationCount\": 88653, \"authors\": [{\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}, {\"authorId\": \"1701686\", \"name\": \"Ilya Sutskever\"}, {\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}]}, {\"paperId\": \"0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"externalIds\": {\"ArXiv\": \"1706.02677\", \"MAG\": \"2622263826\", \"DBLP\": \"journals/corr/GoyalDGNWKTJH17\", \"CorpusId\": 13905106}, \"url\": \"https://www.semanticscholar.org/paper/0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"title\": \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"venue\": \"ArXiv\", \"year\": 2017, \"citationCount\": 2457, \"authors\": [{\"authorId\": \"47316088\", \"name\": \"Priya Goyal\"}, {\"authorId\": \"3127283\", \"name\": \"Piotr Doll\\u00e1r\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"34837514\", \"name\": \"P. Noordhuis\"}, {\"authorId\": \"2065373815\", \"name\": \"Lukasz Wesolowski\"}, {\"authorId\": \"1717990\", \"name\": \"Aapo Kyrola\"}, {\"authorId\": \"3609856\", \"name\": \"Andrew Tulloch\"}, {\"authorId\": \"39978391\", \"name\": \"Yangqing Jia\"}, {\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}]}, {\"paperId\": \"d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"externalIds\": {\"DBLP\": \"conf/cvpr/DengDSLL009\", \"MAG\": \"2108598243\", \"DOI\": \"10.1109/CVPR.2009.5206848\", \"CorpusId\": 57246310}, \"url\": \"https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"title\": \"ImageNet: A large-scale hierarchical image database\", \"venue\": \"2009 IEEE Conference on Computer Vision and Pattern Recognition\", \"year\": 2009, \"citationCount\": 37488, \"authors\": [{\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144847596\", \"name\": \"Wei Dong\"}, {\"authorId\": \"2166511\", \"name\": \"R. Socher\"}, {\"authorId\": \"2040091191\", \"name\": \"Li-Jia Li\"}, {\"authorId\": \"94451829\", \"name\": \"K. Li\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"externalIds\": {\"DBLP\": \"conf/cvpr/XieLHL20\", \"MAG\": \"2985963903\", \"ArXiv\": \"1911.04252\", \"DOI\": \"10.1109/cvpr42600.2020.01070\", \"CorpusId\": 207853355}, \"url\": \"https://www.semanticscholar.org/paper/20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"title\": \"Self-Training With Noisy Student Improves ImageNet Classification\", \"venue\": \"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\", \"year\": 2019, \"citationCount\": 1263, \"authors\": [{\"authorId\": \"1912046\", \"name\": \"Qizhe Xie\"}, {\"authorId\": \"144547315\", \"name\": \"E. Hovy\"}, {\"authorId\": \"1707242\", \"name\": \"Minh-Thang Luong\"}, {\"authorId\": \"2827616\", \"name\": \"Quoc V. Le\"}]}, {\"paperId\": \"dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2101-11986\", \"ArXiv\": \"2101.11986\", \"DOI\": \"10.1109/ICCV48922.2021.00060\", \"CorpusId\": 231719476}, \"url\": \"https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"title\": \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\", \"venue\": \"2021 IEEE/CVF International Conference on Computer Vision (ICCV)\", \"year\": 2021, \"citationCount\": 644, \"authors\": [{\"authorId\": \"2087091296\", \"name\": \"Li Yuan\"}, {\"authorId\": \"2144861793\", \"name\": \"Yunpeng Chen\"}, {\"authorId\": \"143988955\", \"name\": \"Tao Wang\"}, {\"authorId\": \"23476952\", \"name\": \"Weihao Yu\"}, {\"authorId\": \"145356288\", \"name\": \"Yujun Shi\"}, {\"authorId\": \"40983412\", \"name\": \"Francis E. H. Tay\"}, {\"authorId\": \"33221685\", \"name\": \"Jiashi Feng\"}, {\"authorId\": \"143653681\", \"name\": \"Shuicheng Yan\"}]}, {\"paperId\": \"b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"externalIds\": {\"DBLP\": \"journals/corr/RastegariORF16\", \"ArXiv\": \"1603.05279\", \"MAG\": \"2951978180\", \"DOI\": \"10.1007/978-3-319-46493-0_32\", \"CorpusId\": 14925907}, \"url\": \"https://www.semanticscholar.org/paper/b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"title\": \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\", \"venue\": \"ECCV\", \"year\": 2016, \"citationCount\": 2938, \"authors\": [{\"authorId\": \"143887493\", \"name\": \"Mohammad Rastegari\"}, {\"authorId\": \"2004053\", \"name\": \"Vicente Ordonez\"}, {\"authorId\": \"40497777\", \"name\": \"Joseph Redmon\"}, {\"authorId\": \"143787583\", \"name\": \"Ali Farhadi\"}]}, {\"paperId\": \"0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-1811-12231\", \"ArXiv\": \"1811.12231\", \"MAG\": \"2902617128\", \"CorpusId\": 54101493}, \"url\": \"https://www.semanticscholar.org/paper/0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"title\": \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\", \"venue\": \"ICLR\", \"year\": 2018, \"citationCount\": 1466, \"authors\": [{\"authorId\": \"1949747\", \"name\": \"Robert Geirhos\"}, {\"authorId\": \"52096618\", \"name\": \"Patricia Rubisch\"}, {\"authorId\": \"40899528\", \"name\": \"Claudio Michaelis\"}, {\"authorId\": \"1731199\", \"name\": \"M. Bethge\"}, {\"authorId\": \"1924112\", \"name\": \"Felix Wichmann\"}, {\"authorId\": \"40634590\", \"name\": \"Wieland Brendel\"}]}, {\"paperId\": \"d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"externalIds\": {\"MAG\": \"2952681253\", \"DBLP\": \"conf/cvpr/HaraKS18\", \"ArXiv\": \"1711.09577\", \"DOI\": \"10.1109/CVPR.2018.00685\", \"CorpusId\": 4539700}, \"url\": \"https://www.semanticscholar.org/paper/d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"title\": \"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\", \"venue\": \"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\", \"year\": 2017, \"citationCount\": 1236, \"authors\": [{\"authorId\": \"2199251\", \"name\": \"Kensho Hara\"}, {\"authorId\": \"1730200\", \"name\": \"Hirokatsu Kataoka\"}, {\"authorId\": \"1732705\", \"name\": \"Y. Satoh\"}]}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1409.0575?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors,tldr":"{\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"abstract\": null, \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"referenceCount\": 112, \"citationCount\": 28559, \"influentialCitationCount\": 4447, \"isOpenAccess\": true, \"fieldsOfStudy\": [\"Computer Science\"], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared.\"}, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:2205.10660?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors,tldr":"{\"paperId\": \"d7f0a2e9ebd57c0231937acb973bdc064a7eb625\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2205-10660\", \"ArXiv\": \"2205.10660\", \"DOI\": \"10.48550/arXiv.2205.10660\", \"CorpusId\": 248986770}, \"url\": \"https://www.semanticscholar.org/paper/d7f0a2e9ebd57c0231937acb973bdc064a7eb625\", \"title\": \"Vision Transformers in 2022: An Update on Tiny ImageNet\", \"abstract\": \"The recent advances in image transformers have shown impressive results and have largely closed the gap between traditional CNN architectures. The standard procedure is to train on large datasets like ImageNet-21k and then \\ufb01netune on ImageNet-1k. After \\ufb01netuning, researches will often consider the transfer learning performance on smaller datasets such as CIFAR-10/100 but have left out Tiny ImageNet. This paper offers an update on vision transformers\\u2019 performance on Tiny ImageNet. I include Vision Transformer ( ViT ) , Data Ef\\ufb01cient Image Transformer ( DeiT ), Class Attention in Image Transformer ( CaiT ), and Swin Transformers . In addition, Swin Transformers beats the current state-of-the-art re-sult with a validation accuracy of 91.35% . Code is available here: https: //github.com/ehuynh1106/TinyImageNet-Transformers\", \"venue\": \"ArXiv\", \"year\": 2022, \"referenceCount\": 30, \"citationCount\": 1, \"influentialCitationCount\": 0, \"isOpenAccess\": false, \"fieldsOfStudy\": [\"Computer Science\"], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"An update on vision transformers\\u2019 performance on Tiny ImageNet is offered and Swin Transformers beats the current state-of-the-art re-sult with a validation accuracy of 91.35% .\"}, \"authors\": [{\"authorId\": \"2166052995\", \"name\": \"Ethan Huynh\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1706.03762?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors,tldr":"{\"paperId\": \"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"externalIds\": {\"DBLP\": \"journals/corr/VaswaniSPUJGKP17\", \"ArXiv\": \"1706.03762\", \"MAG\": \"2950858113\", \"CorpusId\": 13756489}, \"url\": \"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"title\": \"Attention is All you Need\", \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\", \"venue\": \"NIPS\", \"year\": 2017, \"referenceCount\": 39, \"citationCount\": 47430, \"influentialCitationCount\": 10606, \"isOpenAccess\": false, \"fieldsOfStudy\": [\"Computer Science\"], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"}, \"authors\": [{\"authorId\": \"40348417\", \"name\": \"Ashish Vaswani\"}, {\"authorId\": \"1846258\", \"name\": \"Noam M. Shazeer\"}, {\"authorId\": \"3877127\", \"name\": \"Niki Parmar\"}, {\"authorId\": \"39328010\", \"name\": \"Jakob Uszkoreit\"}, {\"authorId\": \"145024664\", \"name\": \"Llion Jones\"}, {\"authorId\": \"19177000\", \"name\": \"Aidan N. Gomez\"}, {\"authorId\": \"40527594\", \"name\": \"Lukasz Kaiser\"}, {\"authorId\": \"3443442\", \"name\": \"Illia Polosukhin\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1512.03385?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,authors,tldr":"{\"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"externalIds\": {\"MAG\": \"2194775991\", \"DBLP\": \"journals/corr/HeZRS15\", \"ArXiv\": \"1512.03385\", \"DOI\": \"10.1109/cvpr.2016.90\", \"CorpusId\": 206594692}, \"url\": \"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"title\": \"Deep Residual Learning for Image Recognition\", \"abstract\": \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2015, \"referenceCount\": 54, \"citationCount\": 115203, \"influentialCitationCount\": 25456, \"isOpenAccess\": true, \"fieldsOfStudy\": [\"Computer Science\"], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.\"}, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1706.03762?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"externalIds\": {\"DBLP\": \"journals/corr/VaswaniSPUJGKP17\", \"ArXiv\": \"1706.03762\", \"MAG\": \"2950858113\", \"CorpusId\": 13756489}, \"url\": \"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"title\": \"Attention is All you Need\", \"abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\", \"venue\": \"NIPS\", \"year\": 2017, \"referenceCount\": 39, \"citationCount\": 47440, \"influentialCitationCount\": 10607, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"}, \"authors\": [{\"authorId\": \"40348417\", \"name\": \"Ashish Vaswani\"}, {\"authorId\": \"1846258\", \"name\": \"Noam M. Shazeer\"}, {\"authorId\": \"3877127\", \"name\": \"Niki Parmar\"}, {\"authorId\": \"39328010\", \"name\": \"Jakob Uszkoreit\"}, {\"authorId\": \"145024664\", \"name\": \"Llion Jones\"}, {\"authorId\": \"19177000\", \"name\": \"Aidan N. Gomez\"}, {\"authorId\": \"40527594\", \"name\": \"Lukasz Kaiser\"}, {\"authorId\": \"3443442\", \"name\": \"Illia Polosukhin\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1512.03385?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"externalIds\": {\"MAG\": \"2194775991\", \"DBLP\": \"journals/corr/HeZRS15\", \"ArXiv\": \"1512.03385\", \"DOI\": \"10.1109/cvpr.2016.90\", \"CorpusId\": 206594692}, \"url\": \"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"title\": \"Deep Residual Learning for Image Recognition\", \"abstract\": \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2015, \"referenceCount\": 54, \"citationCount\": 115228, \"influentialCitationCount\": 25470, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"http://arxiv.org/pdf/1512.03385\", \"status\": \"GREEN\"}, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.\"}, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1409.0575?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"abstract\": null, \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"referenceCount\": 112, \"citationCount\": 28560, \"influentialCitationCount\": 4447, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"https://dspace.mit.edu/bitstream/1721.1/104944/1/11263_2015_Article_816.pdf\", \"status\": \"GREEN\"}, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared.\"}, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:2205.10660?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"d7f0a2e9ebd57c0231937acb973bdc064a7eb625\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2205-10660\", \"ArXiv\": \"2205.10660\", \"DOI\": \"10.48550/arXiv.2205.10660\", \"CorpusId\": 248986770}, \"url\": \"https://www.semanticscholar.org/paper/d7f0a2e9ebd57c0231937acb973bdc064a7eb625\", \"title\": \"Vision Transformers in 2022: An Update on Tiny ImageNet\", \"abstract\": \"The recent advances in image transformers have shown impressive results and have largely closed the gap between traditional CNN architectures. The standard procedure is to train on large datasets like ImageNet-21k and then \\ufb01netune on ImageNet-1k. After \\ufb01netuning, researches will often consider the transfer learning performance on smaller datasets such as CIFAR-10/100 but have left out Tiny ImageNet. This paper offers an update on vision transformers\\u2019 performance on Tiny ImageNet. I include Vision Transformer ( ViT ) , Data Ef\\ufb01cient Image Transformer ( DeiT ), Class Attention in Image Transformer ( CaiT ), and Swin Transformers . In addition, Swin Transformers beats the current state-of-the-art re-sult with a validation accuracy of 91.35% . Code is available here: https: //github.com/ehuynh1106/TinyImageNet-Transformers\", \"venue\": \"ArXiv\", \"year\": 2022, \"referenceCount\": 30, \"citationCount\": 1, \"influentialCitationCount\": 0, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}, {\"category\": \"Physics\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"An update on vision transformers\\u2019 performance on Tiny ImageNet is offered and Swin Transformers beats the current state-of-the-art re-sult with a validation accuracy of 91.35% .\"}, \"authors\": [{\"authorId\": \"2166052995\", \"name\": \"Ethan Huynh\"}]}\n","https://export.arxiv.org/api/query?id_list=2205.10660&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2205.10660%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2205.10660&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/zMql+o0EcdZQ4npEvoEgmEVYK38</id>\n  <updated>2022-12-26T00:00:00-05:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/2205.10660v1</id>\n    <updated>2022-05-21T19:48:28Z</updated>\n    <published>2022-05-21T19:48:28Z</published>\n    <title>Vision Transformers in 2022: An Update on Tiny ImageNet</title>\n    <summary>  The recent advances in image transformers have shown impressive results and\nhave largely closed the gap between traditional CNN architectures. The standard\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\nImageNet-1k. After finetuning, researches will often consider the transfer\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\nTiny ImageNet. This paper offers an update on vision transformers' performance\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\nresult with a validation accuracy of 91.35%. Code is available here:\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\n</summary>\n    <author>\n      <name>Ethan Huynh</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2205.10660v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2205.10660v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://export.arxiv.org/api/query?id_list=1409.0575&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1409.0575%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1409.0575&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/0OX7geS+m66HQzgUDBhgGGSuK9Q</id>\n  <updated>2022-12-26T00:00:00-05:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1409.0575v3</id>\n    <updated>2015-01-30T01:23:59Z</updated>\n    <published>2014-09-01T22:29:38Z</published>\n    <title>ImageNet Large Scale Visual Recognition Challenge</title>\n    <summary>  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in\nobject category classification and detection on hundreds of object categories\nand millions of images. The challenge has been run annually from 2010 to\npresent, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances\nin object recognition that have been possible as a result. We discuss the\nchallenges of collecting large-scale ground truth annotation, highlight key\nbreakthroughs in categorical object recognition, provide a detailed analysis of\nthe current state of the field of large-scale image classification and object\ndetection, and compare the state-of-the-art computer vision accuracy with human\naccuracy. We conclude with lessons learned in the five years of the challenge,\nand propose future directions and improvements.\n</summary>\n    <author>\n      <name>Olga Russakovsky</name>\n    </author>\n    <author>\n      <name>Jia Deng</name>\n    </author>\n    <author>\n      <name>Hao Su</name>\n    </author>\n    <author>\n      <name>Jonathan Krause</name>\n    </author>\n    <author>\n      <name>Sanjeev Satheesh</name>\n    </author>\n    <author>\n      <name>Sean Ma</name>\n    </author>\n    <author>\n      <name>Zhiheng Huang</name>\n    </author>\n    <author>\n      <name>Andrej Karpathy</name>\n    </author>\n    <author>\n      <name>Aditya Khosla</name>\n    </author>\n    <author>\n      <name>Michael Bernstein</name>\n    </author>\n    <author>\n      <name>Alexander C. Berg</name>\n    </author>\n    <author>\n      <name>Li Fei-Fei</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">43 pages, 16 figures. v3 includes additional comparisons with PASCAL\n  VOC (per-category comparisons in Table 3, distribution of localization\n  difficulty in Fig 16), a list of queries used for obtaining object detection\n  images (Appendix C), and some additional references</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1409.0575v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1409.0575v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"I.4.8; I.5.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.semanticscholar.org/graph/v1/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"externalIds\": {\"MAG\": \"2194775991\", \"DBLP\": \"journals/corr/HeZRS15\", \"ArXiv\": \"1512.03385\", \"DOI\": \"10.1109/cvpr.2016.90\", \"CorpusId\": 206594692}, \"url\": \"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"title\": \"Deep Residual Learning for Image Recognition\", \"abstract\": \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2015, \"referenceCount\": 54, \"citationCount\": 116241, \"influentialCitationCount\": 25795, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"http://arxiv.org/pdf/1512.03385\", \"status\": \"GREEN\"}, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.\"}, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}\n","https://export.arxiv.org/api/query?id_list=1512.03385&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1512.03385%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1512.03385&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/bTv2n6kf64CUD+SyXk72BSrEV1g</id>\n  <updated>2022-12-27T00:00:00-05:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1512.03385v1</id>\n    <updated>2015-12-10T19:51:55Z</updated>\n    <published>2015-12-10T19:51:55Z</published>\n    <title>Deep Residual Learning for Image Recognition</title>\n    <summary>  Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.\n</summary>\n    <author>\n      <name>Kaiming He</name>\n    </author>\n    <author>\n      <name>Xiangyu Zhang</name>\n    </author>\n    <author>\n      <name>Shaoqing Ren</name>\n    </author>\n    <author>\n      <name>Jian Sun</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Tech report</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1512.03385v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1512.03385v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.openreview.net/notes/search?query=deep+residual+learning+for+image+recognition&limit=5&offset=0":"{\"notes\":[{\"id\":\"gikTEZVsHy3\",\"original\":null,\"number\":361443,\"cdate\":1420070400000,\"mdate\":1663319226636,\"ddate\":null,\"tcdate\":1663319226636,\"tmdate\":1663390364100,\"tddate\":null,\"forum\":\"gikTEZVsHy3\",\"replyto\":null,\"invitation\":\"dblp.org/-/record\",\"content\":{\"venue\":\"CoRR 2015\",\"venueid\":\"dblp.org/journals/CORR/2015\",\"_bibtex\":\"@article{DBLP:journals/corr/HeZRS15,\\n  publtype={informal},\\n  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\\n  title={Deep Residual Learning for Image Recognition},\\n  year={2015},\\n  cdate={1420070400000},\\n  journal={CoRR},\\n  volume={abs/1512.03385},\\n  url={http://arxiv.org/abs/1512.03385}\\n}\\n\",\"authors\":[\"Kaiming He\",\"Xiangyu Zhang\",\"Shaoqing Ren\",\"Jian Sun\"],\"authorids\":[\"https://dblp.org/search/pid/api?q=author:Kaiming_He:\",\"~Xiangyu_Zhang1\",\"https://dblp.org/search/pid/api?q=author:Shaoqing_Ren:\",\"https://dblp.org/search/pid/api?q=author:Jian_Sun_0001:\"],\"html\":\"http://arxiv.org/abs/1512.03385\",\"title\":\"Deep Residual Learning for Image Recognition\",\"paperhash\":\"he|deep_residual_learning_for_image_recognition\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"pdf\":\"http://arxiv.org/pdf/1512.03385v1\"},\"signatures\":[\"~Xiangyu_Zhang1\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"dblp.org\"]},{\"id\":\"BJWb6TWOZr\",\"original\":null,\"number\":59748,\"cdate\":1451606400000,\"ddate\":null,\"tcdate\":1563069880687,\"tmdate\":1668116436643,\"tddate\":null,\"forum\":\"BJWb6TWOZr\",\"replyto\":null,\"invitation\":\"dblp.org/-/record\",\"content\":{\"venue\":\"CVPR 2016\",\"venueid\":\"dblp.org/conf/CVPR/2016\",\"_bibtex\":\"@inproceedings{DBLP:conf/cvpr/HeZRS16,\\n  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\\n  title={Deep Residual Learning for Image Recognition},\\n  year={2016},\\n  cdate={1451606400000},\\n  pages={770-778},\\n  url={https://doi.org/10.1109/CVPR.2016.90},\\n  booktitle={CVPR},\\n  crossref={conf/cvpr/2016}\\n}\\n\",\"authors\":[\"Kaiming He\",\"Xiangyu Zhang\",\"Shaoqing Ren\",\"Jian Sun\"],\"authorids\":[\"~Kaiming_He3\",\"~Xiangyu_Zhang1\",\"~Shaoqing_Ren2\",\"~Jian_Sun6\"],\"html\":\"https://doi.org/10.1109/CVPR.2016.90\",\"title\":\"Deep Residual Learning for Image Recognition\",\"paperhash\":\"he|deep_residual_learning_for_image_recognition\",\"abstract\":\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\"pdf\":\"https://ieeexplore.ieee.org/iel7/7776647/7780329/07780459.pdf\"},\"signatures\":[\"dblp.org\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"dblp.org\"]},{\"id\":\"OHg8aOAERKH\",\"original\":null,\"number\":6050,\"cdate\":1620491149734,\"ddate\":null,\"tcdate\":1620491149734,\"tmdate\":1620491149734,\"tddate\":null,\"forum\":\"OHg8aOAERKH\",\"replyto\":null,\"invitation\":\"OpenReview.net/Archive/-/Direct_Upload\",\"content\":{\"title\":\"Deep Cross Residual Learning for Multitask Visual Recognition\",\"authorids\":[\"~Brendan_Jou1\",\"~Shih-Fu_Chang3\"],\"authors\":[\"Brendan Jou\",\"Shih-Fu Chang\"],\"abstract\":\"Residual learning has recently surfaced as an effective means of constructing very deep neural networks for object recognition. However, current incarnations of residual networks do not allow for the modeling and integration of complex relations between closely coupled recognition tasks or across domains. Such problems are often encountered in multimedia applications involving large-scale content recognition. We propose a novel extension of residual learning for deep networks that enables intuitive learning across multiple related tasks using cross-connections called cross-residuals. These cross-residuals connections can be viewed as a form of innetwork regularization and enables greater network generalization. We show how cross-residual learning (CRL) can be integrated in multitask networks to jointly train and detect visual concepts across several tasks. We present a single multitask cross-residual network with >40% less parameters that is able to achieve competitive, or even better, detection performance on a visual sentiment concept detection problem normally requiring multiple specialized single-task networks. The resulting multitask cross-residual network also achieves better detection performance by about 10.4% over a standard multitask residual network without cross-residuals with even a small amount of cross-task weighting.\",\"paperhash\":\"jou|deep_cross_residual_learning_for_multitask_visual_recognition\"},\"signatures\":[\"~Brendan_Jou1\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"~Brendan_Jou1\"]},{\"id\":\"BkxaXeHYDB\",\"original\":\"Skgrrrxtvr\",\"number\":2228,\"cdate\":1569439781025,\"ddate\":null,\"tcdate\":1569439781025,\"tmdate\":1577198805007,\"tddate\":null,\"forum\":\"BkxaXeHYDB\",\"replyto\":null,\"invitation\":\"ICLR.cc/2020/Conference/-/Withdrawn_Submission\",\"content\":{\"authorids\":[\"g.chrysos@imperial.ac.uk\",\"j.deng16@imperial.ac.uk\",\"i.panagakis@imperial.ac.uk\",\"s.zafeiriou@imperial.ac.uk\"],\"title\":\"Newton Residual Learning\",\"authors\":[\"Grigorios Chrysos\",\"Jiankang Deng\",\"Yannis Panagakis\",\"Stefanos Zafeiriou\"],\"pdf\":\"/pdf/aaaee12824dc4b58ba1116dde8988769c90b33c6.pdf\",\"TL;DR\":\"We demonstrate how residual blocks can be viewed as Gauss-Newton steps; we propose a new residual block that exploits second order information.\",\"abstract\":\"A plethora of computer vision tasks, such as optical flow and image alignment, can be formulated as non-linear optimization problems. Before the resurgence of deep learning, the dominant family for solving such optimization problems was numerical optimization, e.g, Gauss-Newton (GN). More recently, several attempts were made to formulate learnable GN steps as cascade regression architectures. In this paper, we investigate recent machine learning architectures, such as deep neural networks with residual connections, under the above perspective. To this end, we first demonstrate how residual blocks (when considered as discretization of ODEs) can be viewed as GN steps. Then, we go a step further and propose a new residual block, that is reminiscent of Newton's method in numerical optimization and exhibits faster convergence. We thoroughly evaluate the proposed Newton-ResNet by conducting experiments on image and speech classification and image generation, using 4 datasets. All the experiments demonstrate that Newton-ResNet requires less parameters to achieve the same performance with the original ResNet.\",\"keywords\":[\"Residual learning\",\"Resnet\",\"Newton\"],\"paperhash\":\"chrysos|newton_residual_learning\",\"original_pdf\":\"/attachment/aaf48096a90a714ee1ca9048335731a2e68e5b32.pdf\"},\"signatures\":[\"ICLR.cc/2020/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2020/Conference\"]},{\"id\":\"AJq_KW43Di\",\"original\":null,\"number\":96859,\"cdate\":1514764800000,\"ddate\":null,\"tcdate\":1582488392251,\"tmdate\":1583361963419,\"tddate\":null,\"forum\":\"AJq_KW43Di\",\"replyto\":null,\"invitation\":\"dblp.org/-/record\",\"content\":{\"venue\":\"Comput. Vis. Image Underst. 2018\",\"venueid\":\"dblp.org/journals/CVIU/2018\",\"_bibtex\":\"@article{DBLP:journals/cviu/PhamKCZV18,\\n  author={Huy-Hieu Pham and Louahdi Khoudour and Alain Crouzil and Pablo Zegers and Sergio A. Velastin},\\n  title={Exploiting deep residual networks for human action recognition from skeletal data},\\n  year={2018},\\n  cdate={1514764800000},\\n  journal={Comput. Vis. Image Underst.},\\n  volume={170},\\n  pages={51-66},\\n  url={https://doi.org/10.1016/j.cviu.2018.03.003}\\n}\\n\",\"authors\":[\"Huy-Hieu Pham\",\"Louahdi Khoudour\",\"Alain Crouzil\",\"Pablo Zegers\",\"Sergio A. Velastin\"],\"authorids\":[\"https://dblp.org/search/pid/api?q=author:Huy-Hieu_Pham:\",\"https://dblp.org/search/pid/api?q=author:Louahdi_Khoudour:\",\"https://dblp.org/search/pid/api?q=author:Alain_Crouzil:\",\"~Pablo_Zegers1\",\"https://dblp.org/search/pid/api?q=author:Sergio_A._Velastin:\"],\"html\":\"https://doi.org/10.1016/j.cviu.2018.03.003\",\"title\":\"Exploiting deep residual networks for human action recognition from skeletal data\",\"paperhash\":\"pham|exploiting_deep_residual_networks_for_human_action_recognition_from_skeletal_data\",\"abstract\":\"Highlights • An end-to-end learning framework based on Deep Residual Networks (ResNets) has been presented to effectively learn the spatial-temporal dynamics carried in RGB images which encoded from skeleton sequences for 3D human action recognition. • An improved ResNet architecture has been proposed. Experimental results for human action recognition demonstrate that the proposed architecture is able to learn image features better than the original model. • The proposed method achieved the state-of-the-art performance on three benchmark datasets, including the largest skeleton dataset for 3D action recognition. • The proposed deep networks are capable of reducing the effects of the degradation phenomenon. • The proposed method requires less computation to achieve high performance. Abstract The computer vision community is currently focusing on solving action recognition problems in real videos, which contain thousands of samples with many challenges. In this process, Deep Convolutional Neural Networks (D-CNNs) have played a significant role in advancing the state-of-the-art in various vision-based action recognition systems. Recently, the introduction of residual connections in conjunction with a more traditional CNN model in a single architecture called Residual Network (ResNet) has shown impressive performance and great potential for image recognition tasks. In this paper, we investigate and apply deep ResNets for human action recognition using skeletal data provided by depth sensors. Firstly, the 3D coordinates of the human body joints carried in skeleton sequences are transformed into image-based representations and stored as RGB images. These color images are able to capture the spatial-temporal evolutions of 3D motions from skeleton sequences and can be efficiently learned by D-CNNs. We then propose a novel deep learning architecture based on ResNets to learn features from obtained color-based representations and classify them into action classes. The proposed method is evaluated on three challenging benchmark datasets including MSR Action 3D, KARD, and NTU-RGB+D datasets. Experimental results demonstrate that our method achieves state-of-the-art performance for all these benchmarks whilst requiring less computation resource. In particular, the proposed method surpasses previous approaches by a significant margin of 3.4% on MSR Action 3D dataset, 0.67% on KARD dataset, and 2.5% on NTU-RGB+D dataset. Previous article in issue Next article in issue\"},\"signatures\":[\"dblp.org\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"dblp.org\"]}],\"count\":10000,\"fromCache\":true}","https://api.semanticscholar.org/graph/v1/paper/arxiv:1512.03385?fields=references.title,references.authors,references.venue,references.year,references.externalIds,references.citationCount":"{\"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\", \"references\": [{\"paperId\": \"f075f89b4f4026748cbf2fb9f989a9934c42ee8f\", \"externalIds\": {\"MAG\": \"2117287331\", \"DBLP\": \"journals/corr/RenHGZ015\", \"ArXiv\": \"1504.06066\", \"DOI\": \"10.1109/TPAMI.2016.2601099\", \"CorpusId\": 5919483, \"PubMed\": \"27541490\"}, \"title\": \"Object Detection Networks on Convolutional Feature Maps\", \"venue\": \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"year\": 2015, \"citationCount\": 343, \"authors\": [{\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"58b09127761bb83f4761de108eb4d88e92b4f451\", \"externalIds\": {\"CorpusId\": 42316459}, \"title\": \"Deep Residual Learning for Image Recognition Supplementary Materials\", \"venue\": \"\", \"year\": 2016, \"citationCount\": 42, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": \"2032184078\", \"name\": \"Jian Sun\"}]}, {\"paperId\": \"b92aa7024b87f50737b372e5df31ef091ab54e62\", \"externalIds\": {\"MAG\": \"1026270304\", \"DBLP\": \"journals/corr/SrivastavaGS15a\", \"ArXiv\": \"1507.06228\", \"CorpusId\": 2722012}, \"title\": \"Training Very Deep Networks\", \"venue\": \"NIPS\", \"year\": 2015, \"citationCount\": 1436, \"authors\": [{\"authorId\": \"2100612\", \"name\": \"R. Srivastava\"}, {\"authorId\": \"3035541\", \"name\": \"Klaus Greff\"}, {\"authorId\": \"145341374\", \"name\": \"J. Schmidhuber\"}]}, {\"paperId\": \"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\", \"externalIds\": {\"DBLP\": \"conf/nips/RenHGS15\", \"ArXiv\": \"1506.01497\", \"MAG\": \"2953106684\", \"DOI\": \"10.1109/TPAMI.2016.2577031\", \"CorpusId\": 10328909, \"PubMed\": \"27295650\"}, \"title\": \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\", \"venue\": \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"year\": 2015, \"citationCount\": 39236, \"authors\": [{\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"2032184078\", \"name\": \"Jian Sun\"}]}, {\"paperId\": \"e3ea1c7a3c6cc15590ab5e62a171536fe01b9b1c\", \"externalIds\": {\"MAG\": \"2952215797\", \"DBLP\": \"journals/corr/GidarisK15\", \"ArXiv\": \"1505.01749\", \"DOI\": \"10.1109/ICCV.2015.135\", \"CorpusId\": 215824235}, \"title\": \"Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2015, \"citationCount\": 666, \"authors\": [{\"authorId\": \"2475428\", \"name\": \"Spyros Gidaris\"}, {\"authorId\": \"2505902\", \"name\": \"N. Komodakis\"}]}, {\"paperId\": \"e0945081b5b87187a53d4329cf77cd8bff635795\", \"externalIds\": {\"DBLP\": \"journals/corr/SrivastavaGS15\", \"ArXiv\": \"1505.00387\", \"CorpusId\": 14786967}, \"title\": \"Highway Networks\", \"venue\": \"ArXiv\", \"year\": 2015, \"citationCount\": 1251, \"authors\": [{\"authorId\": \"2100612\", \"name\": \"R. Srivastava\"}, {\"authorId\": \"3035541\", \"name\": \"Klaus Greff\"}, {\"authorId\": \"145341374\", \"name\": \"J. Schmidhuber\"}]}, {\"paperId\": \"7ffdbc358b63378f07311e883dddacc9faeeaf4b\", \"externalIds\": {\"DBLP\": \"journals/corr/Girshick15\", \"ArXiv\": \"1504.08083\", \"MAG\": \"1536680647\", \"DOI\": \"10.1109/ICCV.2015.169\", \"CorpusId\": 206770307}, \"title\": \"Fast R-CNN\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2015, \"citationCount\": 16525, \"authors\": [{\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}]}, {\"paperId\": \"4d376d6978dad0374edfa6709c9556b42d3594d3\", \"externalIds\": {\"MAG\": \"2949117887\", \"DBLP\": \"journals/corr/IoffeS15\", \"ArXiv\": \"1502.03167\", \"CorpusId\": 5808102}, \"title\": \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", \"venue\": \"International Conference on Machine Learning\", \"year\": 2015, \"citationCount\": 33395, \"authors\": [{\"authorId\": \"2054165706\", \"name\": \"Sergey Ioffe\"}, {\"authorId\": \"2574060\", \"name\": \"Christian Szegedy\"}]}, {\"paperId\": \"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"externalIds\": {\"DBLP\": \"journals/corr/HeZR015\", \"MAG\": \"2949608135\", \"ArXiv\": \"1502.01852\", \"DOI\": \"10.1109/ICCV.2015.123\", \"CorpusId\": 13740328}, \"title\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2015, \"citationCount\": 14146, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"cd85a549add0c7c7def36aca29837efd24b24080\", \"externalIds\": {\"ArXiv\": \"1412.6550\", \"MAG\": \"2953291111\", \"DBLP\": \"journals/corr/RomeroBKCGB14\", \"CorpusId\": 2723173}, \"title\": \"FitNets: Hints for Thin Deep Nets\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2014, \"citationCount\": 2373, \"authors\": [{\"authorId\": \"2069136633\", \"name\": \"Adriana Romero\"}, {\"authorId\": \"2482072\", \"name\": \"Nicolas Ballas\"}, {\"authorId\": \"3127597\", \"name\": \"S. Kahou\"}, {\"authorId\": \"3186079\", \"name\": \"Antoine Chassang\"}, {\"authorId\": \"143706039\", \"name\": \"C. Gatta\"}, {\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}]}, {\"paperId\": \"8ad35df17ae4064dd174690efb04d347428f1117\", \"externalIds\": {\"MAG\": \"2949847272\", \"DBLP\": \"conf/cvpr/He015\", \"ArXiv\": \"1412.1710\", \"DOI\": \"10.1109/CVPR.2015.7299173\", \"CorpusId\": 2141952}, \"title\": \"Convolutional neural networks at constrained time cost\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2014, \"citationCount\": 1012, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"317aee7fc081f2b137a85c4f20129007fd8e717e\", \"externalIds\": {\"MAG\": \"2395611524\", \"DBLP\": \"journals/pami/ShelhamerLD17\", \"ArXiv\": \"1411.4038\", \"DOI\": \"10.1109/CVPR.2015.7298965\", \"CorpusId\": 1629541, \"PubMed\": \"27244717\"}, \"title\": \"Fully convolutional networks for semantic segmentation\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2014, \"citationCount\": 28026, \"authors\": [{\"authorId\": \"1782282\", \"name\": \"Evan Shelhamer\"}, {\"authorId\": \"2117314646\", \"name\": \"Jonathan Long\"}, {\"authorId\": \"1753210\", \"name\": \"Trevor Darrell\"}]}, {\"paperId\": \"fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd\", \"externalIds\": {\"MAG\": \"2168894214\", \"DBLP\": \"journals/corr/LeeXGZT14\", \"ArXiv\": \"1409.5185\", \"CorpusId\": 1289873}, \"title\": \"Deeply-Supervised Nets\", \"venue\": \"International Conference on Artificial Intelligence and Statistics\", \"year\": 2014, \"citationCount\": 1614, \"authors\": [{\"authorId\": \"50521003\", \"name\": \"Chen-Yu Lee\"}, {\"authorId\": \"1817030\", \"name\": \"Saining Xie\"}, {\"authorId\": \"21561136\", \"name\": \"Patrick W. Gallagher\"}, {\"authorId\": \"2148905709\", \"name\": \"Zhengyou Zhang\"}, {\"authorId\": \"144035504\", \"name\": \"Z. Tu\"}]}, {\"paperId\": \"e15cf50aa89fee8535703b9f9512fca5bfc43327\", \"externalIds\": {\"ArXiv\": \"1409.4842\", \"DBLP\": \"conf/cvpr/SzegedyLJSRAEVR15\", \"MAG\": \"2950179405\", \"DOI\": \"10.1109/CVPR.2015.7298594\", \"CorpusId\": 206592484}, \"title\": \"Going deeper with convolutions\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2014, \"citationCount\": 33545, \"authors\": [{\"authorId\": \"2574060\", \"name\": \"Christian Szegedy\"}, {\"authorId\": \"2157222093\", \"name\": \"Wei Liu\"}, {\"authorId\": \"39978391\", \"name\": \"Yangqing Jia\"}, {\"authorId\": \"3142556\", \"name\": \"P. Sermanet\"}, {\"authorId\": \"144828948\", \"name\": \"Scott E. Reed\"}, {\"authorId\": \"1838674\", \"name\": \"Dragomir Anguelov\"}, {\"authorId\": \"1761978\", \"name\": \"D. Erhan\"}, {\"authorId\": \"2657155\", \"name\": \"Vincent Vanhoucke\"}, {\"authorId\": \"39863668\", \"name\": \"Andrew Rabinovich\"}]}, {\"paperId\": \"eb42cf88027de515750f230b23b1a057dc782108\", \"externalIds\": {\"DBLP\": \"journals/corr/SimonyanZ14a\", \"MAG\": \"2962835968\", \"ArXiv\": \"1409.1556\", \"CorpusId\": 14124313}, \"title\": \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2014, \"citationCount\": 71882, \"authors\": [{\"authorId\": \"34838386\", \"name\": \"K. Simonyan\"}, {\"authorId\": \"1688869\", \"name\": \"Andrew Zisserman\"}]}, {\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"MAG\": \"2117539524\", \"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"citationCount\": 28749, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"cbb19236820a96038d000dc629225d36e0b6294a\", \"externalIds\": {\"DBLP\": \"journals/corr/HeZR014\", \"MAG\": \"2179352600\", \"ArXiv\": \"1406.4729\", \"DOI\": \"10.1007/978-3-319-10578-9_23\", \"CorpusId\": 436933, \"PubMed\": \"26353135\"}, \"title\": \"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\", \"venue\": \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"year\": 2014, \"citationCount\": 7235, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"6bdb186ec4726e00a8051119636d4df3b94043b5\", \"externalIds\": {\"ArXiv\": \"1408.5093\", \"DBLP\": \"conf/mm/JiaSDKLGGD14\", \"MAG\": \"2950094539\", \"DOI\": \"10.1145/2647868.2654889\", \"CorpusId\": 1799558}, \"title\": \"Caffe: Convolutional Architecture for Fast Feature Embedding\", \"venue\": \"ACM Multimedia\", \"year\": 2014, \"citationCount\": 14224, \"authors\": [{\"authorId\": \"39978391\", \"name\": \"Yangqing Jia\"}, {\"authorId\": \"1782282\", \"name\": \"Evan Shelhamer\"}, {\"authorId\": \"7408951\", \"name\": \"Jeff Donahue\"}, {\"authorId\": \"3049736\", \"name\": \"Sergey Karayev\"}, {\"authorId\": \"2117314646\", \"name\": \"Jonathan Long\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"1687120\", \"name\": \"S. Guadarrama\"}, {\"authorId\": \"1753210\", \"name\": \"Trevor Darrell\"}]}, {\"paperId\": \"71b7178df5d2b112d07e45038cb5637208659ff7\", \"externalIds\": {\"DBLP\": \"journals/corr/LinMBHPRDZ14\", \"ArXiv\": \"1405.0312\", \"MAG\": \"2952122856\", \"DOI\": \"10.1007/978-3-319-10602-1_48\", \"CorpusId\": 14113767}, \"title\": \"Microsoft COCO: Common Objects in Context\", \"venue\": \"European Conference on Computer Vision\", \"year\": 2014, \"citationCount\": 24223, \"authors\": [{\"authorId\": \"33493200\", \"name\": \"Tsung-Yi Lin\"}, {\"authorId\": \"145854440\", \"name\": \"M. Maire\"}, {\"authorId\": \"50172592\", \"name\": \"Serge J. Belongie\"}, {\"authorId\": \"48966748\", \"name\": \"James Hays\"}, {\"authorId\": \"1690922\", \"name\": \"P. Perona\"}, {\"authorId\": \"1770537\", \"name\": \"D. Ramanan\"}, {\"authorId\": \"3127283\", \"name\": \"Piotr Doll\\u00e1r\"}, {\"authorId\": \"1699161\", \"name\": \"C. L. Zitnick\"}]}, {\"paperId\": \"b034b5769ab94acf9fb8ae48c7edb560a300bb63\", \"externalIds\": {\"MAG\": \"2161388792\", \"DBLP\": \"journals/corr/MontufarPCB14\", \"ArXiv\": \"1402.1869\", \"CorpusId\": 5941770}, \"title\": \"On the Number of Linear Regions of Deep Neural Networks\", \"venue\": \"NIPS\", \"year\": 2014, \"citationCount\": 1015, \"authors\": [{\"authorId\": \"1784667\", \"name\": \"Guido Mont\\u00fafar\"}, {\"authorId\": \"1996134\", \"name\": \"Razvan Pascanu\"}, {\"authorId\": \"1979489\", \"name\": \"Kyunghyun Cho\"}, {\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}]}, {\"paperId\": \"1109b663453e78a59e4f66446d71720ac58cec25\", \"externalIds\": {\"MAG\": \"2981785444\", \"ArXiv\": \"1312.6229\", \"DBLP\": \"journals/corr/SermanetEZMFL13\", \"CorpusId\": 4071727}, \"title\": \"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2013, \"citationCount\": 4604, \"authors\": [{\"authorId\": \"3142556\", \"name\": \"P. Sermanet\"}, {\"authorId\": \"2060028\", \"name\": \"D. Eigen\"}, {\"authorId\": \"2156004365\", \"name\": \"Xiang Zhang\"}, {\"authorId\": \"143949035\", \"name\": \"Micha\\u00ebl Mathieu\"}, {\"authorId\": \"2276554\", \"name\": \"R. Fergus\"}, {\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}]}, {\"paperId\": \"99c970348b8f70ce23d6641e201904ea49266b6e\", \"externalIds\": {\"DBLP\": \"journals/corr/SaxeMG13\", \"MAG\": \"2963504252\", \"ArXiv\": \"1312.6120\", \"CorpusId\": 17272965}, \"title\": \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2013, \"citationCount\": 1407, \"authors\": [{\"authorId\": \"34927843\", \"name\": \"Andrew M. Saxe\"}, {\"authorId\": \"1701656\", \"name\": \"James L. McClelland\"}, {\"authorId\": \"25769960\", \"name\": \"S. Ganguli\"}]}, {\"paperId\": \"5e83ab70d0cbc003471e87ec306d27d9c80ecb16\", \"externalIds\": {\"MAG\": \"2951718859\", \"DBLP\": \"journals/corr/LinCY13\", \"ArXiv\": \"1312.4400\", \"CorpusId\": 16636683}, \"title\": \"Network In Network\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2013, \"citationCount\": 4693, \"authors\": [{\"authorId\": \"2115913164\", \"name\": \"Min Lin\"}, {\"authorId\": \"35370244\", \"name\": \"Qiang Chen\"}, {\"authorId\": \"143653681\", \"name\": \"Shuicheng Yan\"}]}, {\"paperId\": \"2f4df08d9072fc2ac181b7fced6a245315ce05c8\", \"externalIds\": {\"DBLP\": \"journals/corr/GirshickDDM13\", \"MAG\": \"2951638509\", \"ArXiv\": \"1311.2524\", \"DOI\": \"10.1109/CVPR.2014.81\", \"CorpusId\": 215827080}, \"title\": \"Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\", \"venue\": \"2014 IEEE Conference on Computer Vision and Pattern Recognition\", \"year\": 2013, \"citationCount\": 19766, \"authors\": [{\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"7408951\", \"name\": \"Jeff Donahue\"}, {\"authorId\": \"1753210\", \"name\": \"Trevor Darrell\"}, {\"authorId\": \"153652147\", \"name\": \"J. Malik\"}]}, {\"paperId\": \"fc26b9c1afe81e1b20195123fe6f3ced9520abb6\", \"externalIds\": {\"MAG\": \"2206858481\", \"CorpusId\": 62099841}, \"title\": \"Visualizing and Understanding Convolutional Neural Networks\", \"venue\": \"\", \"year\": 2013, \"citationCount\": 509, \"authors\": [{\"authorId\": \"48799969\", \"name\": \"Matthew D. Zeiler\"}, {\"authorId\": \"2276554\", \"name\": \"R. Fergus\"}]}, {\"paperId\": \"b7b915d508987b73b61eccd2b237e7ed099a2d29\", \"externalIds\": {\"DBLP\": \"conf/icml/GoodfellowWMCB13\", \"MAG\": \"2294059674\", \"ArXiv\": \"1302.4389\", \"CorpusId\": 10600578}, \"title\": \"Maxout Networks\", \"venue\": \"International Conference on Machine Learning\", \"year\": 2013, \"citationCount\": 1936, \"authors\": [{\"authorId\": \"153440022\", \"name\": \"Ian J. Goodfellow\"}, {\"authorId\": \"1393680089\", \"name\": \"David Warde-Farley\"}, {\"authorId\": \"153583218\", \"name\": \"Mehdi Mirza\"}, {\"authorId\": \"1760871\", \"name\": \"Aaron C. Courville\"}, {\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}]}, {\"paperId\": \"f72c079d9179cfaada1135a7e4c77d48b6309a30\", \"externalIds\": {\"ArXiv\": \"1301.3476\", \"MAG\": \"1864754470\", \"DBLP\": \"conf/iconip/VatanenRVL13\", \"DOI\": \"10.1007/978-3-642-42054-2_55\", \"CorpusId\": 8739352}, \"title\": \"Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2013, \"citationCount\": 25, \"authors\": [{\"authorId\": \"1837700\", \"name\": \"T. Vatanen\"}, {\"authorId\": \"2785022\", \"name\": \"T. Raiko\"}, {\"authorId\": \"2132516\", \"name\": \"H. Valpola\"}, {\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}]}, {\"paperId\": \"abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"externalIds\": {\"MAG\": \"2997031122\", \"DBLP\": \"conf/nips/KrizhevskySH12\", \"DOI\": \"10.1145/3065386\", \"CorpusId\": 195908774}, \"title\": \"ImageNet classification with deep convolutional neural networks\", \"venue\": \"Communications of the ACM\", \"year\": 2012, \"citationCount\": 91696, \"authors\": [{\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}, {\"authorId\": \"1701686\", \"name\": \"Ilya Sutskever\"}, {\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}]}, {\"paperId\": \"5183230b706b72f6f6c19415c423d93c79ddde53\", \"externalIds\": {\"DBLP\": \"journals/pami/JegouPDSPS12\", \"MAG\": \"1984309565\", \"DOI\": \"10.1109/TPAMI.2011.235\", \"CorpusId\": 9437674, \"PubMed\": \"22156101\"}, \"title\": \"Aggregating Local Image Descriptors into Compact Codes\", \"venue\": \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"year\": 2012, \"citationCount\": 1488, \"authors\": [{\"authorId\": \"1681054\", \"name\": \"H. J\\u00e9gou\"}, {\"authorId\": \"1723883\", \"name\": \"F. Perronnin\"}, {\"authorId\": \"3271933\", \"name\": \"Matthijs Douze\"}, {\"authorId\": \"143995443\", \"name\": \"Jorge S\\u00e1nchez\"}, {\"authorId\": \"144565371\", \"name\": \"P. P\\u00e9rez\"}, {\"authorId\": \"2462253\", \"name\": \"C. Schmid\"}]}, {\"paperId\": \"1366de5bb112746a555e9c0cd00de3ad8628aea8\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-1207-0580\", \"MAG\": \"1904365287\", \"ArXiv\": \"1207.0580\", \"CorpusId\": 14832074}, \"title\": \"Improving neural networks by preventing co-adaptation of feature detectors\", \"venue\": \"ArXiv\", \"year\": 2012, \"citationCount\": 6656, \"authors\": [{\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}, {\"authorId\": \"2897313\", \"name\": \"Nitish Srivastava\"}, {\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}, {\"authorId\": \"1701686\", \"name\": \"Ilya Sutskever\"}, {\"authorId\": \"145124475\", \"name\": \"R. Salakhutdinov\"}]}, {\"paperId\": \"b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14\", \"externalIds\": {\"DBLP\": \"journals/jmlr/RaikoVL12\", \"MAG\": \"2152424459\", \"CorpusId\": 2204994}, \"title\": \"Deep Learning Made Easier by Linear Transformations in Perceptrons\", \"venue\": \"International Conference on Artificial Intelligence and Statistics\", \"year\": 2012, \"citationCount\": 188, \"authors\": [{\"authorId\": \"2785022\", \"name\": \"T. Raiko\"}, {\"authorId\": \"2132516\", \"name\": \"H. Valpola\"}, {\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}]}, {\"paperId\": \"bc6dff14a130c57a91d5a21339c23471faf1d46f\", \"externalIds\": {\"DOI\": \"10.1136/ebmh.11.4.102\", \"CorpusId\": 11759366, \"PubMed\": \"22365577\"}, \"title\": \"Et al\", \"venue\": \"Archives de p\\u00e9diatrie\", \"year\": 2008, \"citationCount\": 63421, \"authors\": [{\"authorId\": \"2059358552\", \"name\": \"P. Cochat\"}, {\"authorId\": \"13267685\", \"name\": \"L. Vaucoret\"}, {\"authorId\": \"2097644863\", \"name\": \"J. Sarles\"}]}, {\"paperId\": \"b87274e6d9aa4e6ba5148898aa92941617d2b6ed\", \"externalIds\": {\"MAG\": \"2914484425\", \"DBLP\": \"series/lncs/LeCunBOM12\", \"DOI\": \"10.1007/978-3-642-35289-8_3\", \"CorpusId\": 20158889}, \"title\": \"Efficient BackProp\", \"venue\": \"Neural Networks\", \"year\": 2012, \"citationCount\": 2803, \"authors\": [{\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}, {\"authorId\": \"52184096\", \"name\": \"L. Bottou\"}, {\"authorId\": \"34782608\", \"name\": \"G. Orr\"}, {\"authorId\": \"145034054\", \"name\": \"K. M\\u00fcller\"}]}, {\"paperId\": \"7b7908f71188b89adf62ce9126a0466e1a34338f\", \"externalIds\": {\"MAG\": \"1976921161\", \"DBLP\": \"conf/bmvc/ChatfieldLVZ11\", \"DOI\": \"10.5244/C.25.76\", \"CorpusId\": 13126996}, \"title\": \"The devil is in the details: an evaluation of recent feature encoding methods\", \"venue\": \"British Machine Vision Conference\", \"year\": 2011, \"citationCount\": 951, \"authors\": [{\"authorId\": \"1764761\", \"name\": \"K. Chatfield\"}, {\"authorId\": \"1740145\", \"name\": \"V. Lempitsky\"}, {\"authorId\": \"1687524\", \"name\": \"A. Vedaldi\"}, {\"authorId\": \"1688869\", \"name\": \"Andrew Zisserman\"}]}, {\"paperId\": \"4748d22348e72e6e06c2476486afddbc76e5eca7\", \"externalIds\": {\"DBLP\": \"journals/pami/JegouDS11\", \"MAG\": \"2124509324\", \"DOI\": \"10.1109/TPAMI.2010.57\", \"CorpusId\": 5850884, \"PubMed\": \"21088323\"}, \"title\": \"Product Quantization for Nearest Neighbor Search\", \"venue\": \"IEEE Transactions on Pattern Analysis and Machine Intelligence\", \"year\": 2011, \"citationCount\": 2203, \"authors\": [{\"authorId\": \"1681054\", \"name\": \"H. J\\u00e9gou\"}, {\"authorId\": \"3271933\", \"name\": \"Matthijs Douze\"}, {\"authorId\": \"2462253\", \"name\": \"C. Schmid\"}]}, {\"paperId\": \"d720a95e1501922ea17ee31f299f43b2db5e15ef\", \"externalIds\": {\"MAG\": \"2066941820\", \"DBLP\": \"conf/mm/VedaldiF10\", \"DOI\": \"10.1145/1873951.1874249\", \"CorpusId\": 1458265}, \"title\": \"Vlfeat: an open and portable library of computer vision algorithms\", \"venue\": \"ACM Multimedia\", \"year\": 2010, \"citationCount\": 3467, \"authors\": [{\"authorId\": \"1687524\", \"name\": \"A. Vedaldi\"}, {\"authorId\": \"2487006\", \"name\": \"B. Fulkerson\"}]}, {\"paperId\": \"a538b05ebb01a40323997629e171c91aa28b8e2f\", \"externalIds\": {\"DBLP\": \"conf/icml/NairH10\", \"MAG\": \"1665214252\", \"CorpusId\": 15539264}, \"title\": \"Rectified Linear Units Improve Restricted Boltzmann Machines\", \"venue\": \"International Conference on Machine Learning\", \"year\": 2010, \"citationCount\": 14365, \"authors\": [{\"authorId\": \"2073603971\", \"name\": \"Vinod Nair\"}, {\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}]}, {\"paperId\": \"b71ac1e9fb49420d13e084ac67254a0bbd40f83f\", \"externalIds\": {\"DBLP\": \"journals/jmlr/GlorotB10\", \"MAG\": \"1533861849\", \"CorpusId\": 5575601}, \"title\": \"Understanding the difficulty of training deep feedforward neural networks\", \"venue\": \"International Conference on Artificial Intelligence and Statistics\", \"year\": 2010, \"citationCount\": 13917, \"authors\": [{\"authorId\": \"3119801\", \"name\": \"Xavier Glorot\"}, {\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}]}, {\"paperId\": \"82635fb63640ae95f90ee9bdc07832eb461ca881\", \"externalIds\": {\"DBLP\": \"journals/ijcv/EveringhamGWWZ10\", \"MAG\": \"2031489346\", \"DOI\": \"10.1007/s11263-009-0275-4\", \"CorpusId\": 4246903}, \"title\": \"The Pascal Visual Object Classes (VOC) Challenge\", \"venue\": \"International Journal of Computer Vision\", \"year\": 2010, \"citationCount\": 13115, \"authors\": [{\"authorId\": \"3056091\", \"name\": \"M. Everingham\"}, {\"authorId\": \"1681236\", \"name\": \"L. Gool\"}, {\"authorId\": \"145715698\", \"name\": \"Christopher K. I. Williams\"}, {\"authorId\": \"33652486\", \"name\": \"J. Winn\"}, {\"authorId\": \"1688869\", \"name\": \"Andrew Zisserman\"}]}, {\"paperId\": \"5d90f06bb70a0a3dced62413346235c02b1aa086\", \"externalIds\": {\"MAG\": \"2945315962\", \"CorpusId\": 18268744}, \"title\": \"Learning Multiple Layers of Features from Tiny Images\", \"venue\": \"\", \"year\": 2009, \"citationCount\": 20641, \"authors\": [{\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}]}, {\"paperId\": \"23694b6d61668e62bb11f17c1d75dde3b4951948\", \"externalIds\": {\"DBLP\": \"conf/cvpr/PerronninD07\", \"MAG\": \"2147238549\", \"DOI\": \"10.1109/CVPR.2007.383266\", \"CorpusId\": 12795415}, \"title\": \"Fisher Kernels on Visual Vocabularies for Image Categorization\", \"venue\": \"2007 IEEE Conference on Computer Vision and Pattern Recognition\", \"year\": 2007, \"citationCount\": 1661, \"authors\": [{\"authorId\": \"1723883\", \"name\": \"F. Perronnin\"}, {\"authorId\": \"3344005\", \"name\": \"C. Dance\"}]}, {\"paperId\": \"4f04da90218f8ddfa3a758188edade8c7bd95ac1\", \"externalIds\": {\"DBLP\": \"journals/tog/Szeliski06\", \"MAG\": \"2022532533\", \"DOI\": \"10.1145/1179352.1142005\", \"CorpusId\": 835779}, \"title\": \"Locally adapted hierarchical basis preconditioning\", \"venue\": \"International Conference on Computer Graphics and Interactive Techniques\", \"year\": 2006, \"citationCount\": 121, \"authors\": [{\"authorId\": \"1717841\", \"name\": \"R. Szeliski\"}]}, {\"paperId\": \"1bd875676fe49f83d431500cea908da1bdf068da\", \"externalIds\": {\"DBLP\": \"books/daglib/0002128\", \"MAG\": \"1997542937\", \"DOI\": \"10.1137/1.9780898719505\", \"CorpusId\": 58982102}, \"title\": \"A multigrid tutorial, Second Edition\", \"venue\": \"\", \"year\": 2000, \"citationCount\": 1169, \"authors\": [{\"authorId\": \"35180917\", \"name\": \"W. Briggs\"}, {\"authorId\": \"65844783\", \"name\": \"V. E. Henson\"}, {\"authorId\": \"26529098\", \"name\": \"S. McCormick\"}]}, {\"paperId\": \"5763bd6b3f24a01c3bc7cd15d3c916b4840b759d\", \"externalIds\": {\"MAG\": \"2139398462\", \"DOI\": \"10.3929/ETHZ-A-004263473\", \"CorpusId\": 14135490}, \"title\": \"Accelerated Gradient Descent by Factor-Centering Decomposition\", \"venue\": \"\", \"year\": 1998, \"citationCount\": 28, \"authors\": [{\"authorId\": \"1739396\", \"name\": \"N. Schraudolph\"}]}, {\"paperId\": \"44d2abe2175df8153f465f6c39b68b76a0d40ab9\", \"externalIds\": {\"DBLP\": \"journals/neco/HochreiterS97\", \"MAG\": \"2064675550\", \"DOI\": \"10.1162/neco.1997.9.8.1735\", \"CorpusId\": 1915014, \"PubMed\": \"9377276\"}, \"title\": \"Long Short-Term Memory\", \"venue\": \"Neural Computation\", \"year\": 1997, \"citationCount\": 59280, \"authors\": [{\"authorId\": \"3308557\", \"name\": \"S. Hochreiter\"}, {\"authorId\": \"145341374\", \"name\": \"J. Schmidhuber\"}]}, {\"paperId\": \"1f711685bfe9e67d6afe0d5a3cb6675c310237d6\", \"externalIds\": {\"MAG\": \"2797624385\", \"DOI\": \"10.2307/2532871\", \"CorpusId\": 61844108}, \"title\": \"Modern Applied Statistics with S-Plus.\", \"venue\": \"\", \"year\": 1996, \"citationCount\": 4253, \"authors\": [{\"authorId\": \"38883832\", \"name\": \"W. Venables\"}, {\"authorId\": \"2122942\", \"name\": \"B. Ripley\"}]}, {\"paperId\": \"75a026ddfdd9c219d69fe8af816f085ea1b3877d\", \"externalIds\": {\"MAG\": \"2986790716\", \"DBLP\": \"series/lncs/Schraudolph12\", \"DOI\": \"10.1007/3-540-49430-8_11\", \"CorpusId\": 2226276}, \"title\": \"Centering Neural Network Gradient Factors\", \"venue\": \"Neural Networks\", \"year\": 1996, \"citationCount\": 55, \"authors\": [{\"authorId\": \"1739396\", \"name\": \"N. Schraudolph\"}]}, {\"paperId\": \"877a887e7af7daebcb685e4d7b5e80f764035581\", \"externalIds\": {\"MAG\": \"1538142388\", \"CorpusId\": 9584248}, \"title\": \"Pattern Recognition and Neural Networks\", \"venue\": \"\", \"year\": 1995, \"citationCount\": 4201, \"authors\": [{\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}, {\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}]}, {\"paperId\": \"d0be39ee052d246ae99c082a565aba25b811be2d\", \"externalIds\": {\"DBLP\": \"journals/tnn/BengioSF94\", \"MAG\": \"2107878631\", \"DOI\": \"10.1109/72.279181\", \"CorpusId\": 206457500, \"PubMed\": \"18267787\"}, \"title\": \"Learning long-term dependencies with gradient descent is difficult\", \"venue\": \"IEEE Trans. Neural Networks\", \"year\": 1994, \"citationCount\": 6698, \"authors\": [{\"authorId\": \"1751762\", \"name\": \"Yoshua Bengio\"}, {\"authorId\": \"2812486\", \"name\": \"P. Simard\"}, {\"authorId\": \"1688235\", \"name\": \"P. Frasconi\"}]}, {\"paperId\": \"dbc0a468ab103ae29717703d4aa9f682f6a2b664\", \"externalIds\": {\"DBLP\": \"journals/ac/KothariO93\", \"MAG\": \"1567662521\", \"DOI\": \"10.1016/S0065-2458(08)60404-0\", \"CorpusId\": 177751}, \"title\": \"Neural Networks for Pattern Recognition\", \"venue\": \"Advances in Computing\", \"year\": 1993, \"citationCount\": 15934, \"authors\": [{\"authorId\": \"145833095\", \"name\": \"S. Kothari\"}, {\"authorId\": \"2681982\", \"name\": \"H. Oh\"}]}, {\"paperId\": \"3f3d13e95c25a8f6a753e38dfce88885097cbd43\", \"externalIds\": {\"MAG\": \"194249466\", \"CorpusId\": 60091947}, \"title\": \"Untersuchungen zu dynamischen neuronalen Netzen\", \"venue\": \"\", \"year\": 1991, \"citationCount\": 661, \"authors\": [{\"authorId\": \"2066667188\", \"name\": \"Sepp Hochreiter\"}]}, {\"paperId\": \"a8e8f3c8d4418c8d62e306538c9c1292635e9d27\", \"externalIds\": {\"MAG\": \"2147800946\", \"DBLP\": \"journals/neco/LeCunBDHHHJ89\", \"DOI\": \"10.1162/neco.1989.1.4.541\", \"CorpusId\": 41312633}, \"title\": \"Backpropagation Applied to Handwritten Zip Code Recognition\", \"venue\": \"Neural Computation\", \"year\": 1989, \"citationCount\": 8699, \"authors\": [{\"authorId\": \"1688882\", \"name\": \"Yann LeCun\"}, {\"authorId\": \"2219581\", \"name\": \"B. Boser\"}, {\"authorId\": \"1747317\", \"name\": \"J. Denker\"}, {\"authorId\": \"37274089\", \"name\": \"D. Henderson\"}, {\"authorId\": \"2799635\", \"name\": \"R. Howard\"}, {\"authorId\": \"34859193\", \"name\": \"W. Hubbard\"}, {\"authorId\": \"2041866\", \"name\": \"L. Jackel\"}]}, {\"paperId\": \"1efc5a54a4b3f4675bee194ee5842978e45a5bc2\", \"externalIds\": {\"MAG\": \"2159979951\", \"DBLP\": \"conf/cvpr/Szeliski89\", \"DOI\": \"10.1109/CVPR.1989.37853\", \"CorpusId\": 6245009}, \"title\": \"Fast surface interpolation using hierarchical basis functions\", \"venue\": \"Proceedings CVPR '89: IEEE Computer Society Conference on Computer Vision and Pattern Recognition\", \"year\": 1989, \"citationCount\": 209, \"authors\": [{\"authorId\": \"1717841\", \"name\": \"R. Szeliski\"}]}, {\"paperId\": \"8a2384ff41dc2c337e6aaacbcfd13f74e0e2f1ea\", \"externalIds\": {\"MAG\": \"1530872699\", \"DBLP\": \"books/daglib/0000904\", \"CorpusId\": 33704151}, \"title\": \"A multigrid tutorial\", \"venue\": \"\", \"year\": 1987, \"citationCount\": 2567, \"authors\": [{\"authorId\": \"35180917\", \"name\": \"W. Briggs\"}]}]}\n","https://api.semanticscholar.org/graph/v1/paper/search?query=Human-level control through deep reinforcement learning&fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors":"{\"total\": 18088, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\", \"externalIds\": {\"DBLP\": \"journals/nature/MnihKSRVBGRFOPB15\", \"MAG\": \"2145339207\", \"DOI\": \"10.1038/nature14236\", \"CorpusId\": 205242740, \"PubMed\": \"25719670\"}, \"url\": \"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\", \"title\": \"Human-level control through deep reinforcement learning\", \"abstract\": null, \"venue\": \"Nature\", \"year\": 2015, \"referenceCount\": 36, \"citationCount\": 18818, \"influentialCitationCount\": 3000, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\", \"Medicine\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3255983\", \"name\": \"Volodymyr Mnih\"}, {\"authorId\": \"2645384\", \"name\": \"K. Kavukcuoglu\"}, {\"authorId\": \"145824029\", \"name\": \"David Silver\"}, {\"authorId\": \"2228824\", \"name\": \"Andrei A. Rusu\"}, {\"authorId\": \"144056327\", \"name\": \"J. Veness\"}, {\"authorId\": \"1792298\", \"name\": \"Marc G. Bellemare\"}, {\"authorId\": \"1753223\", \"name\": \"A. Graves\"}, {\"authorId\": \"3137672\", \"name\": \"Martin A. Riedmiller\"}, {\"authorId\": \"145600108\", \"name\": \"A. Fidjeland\"}, {\"authorId\": \"2273072\", \"name\": \"Georg Ostrovski\"}, {\"authorId\": \"48348688\", \"name\": \"Stig Petersen\"}, {\"authorId\": \"50388928\", \"name\": \"Charlie Beattie\"}, {\"authorId\": \"49813280\", \"name\": \"A. Sadik\"}, {\"authorId\": \"2460849\", \"name\": \"Ioannis Antonoglou\"}, {\"authorId\": \"143776287\", \"name\": \"Helen King\"}, {\"authorId\": \"2106164\", \"name\": \"D. Kumaran\"}, {\"authorId\": \"1688276\", \"name\": \"Daan Wierstra\"}, {\"authorId\": \"34313265\", \"name\": \"S. Legg\"}, {\"authorId\": \"48987704\", \"name\": \"D. Hassabis\"}]}, {\"paperId\": \"024006d4c2a89f7acacc6e4438d156525b60a98f\", \"externalIds\": {\"ArXiv\": \"1509.02971\", \"MAG\": \"2963864421\", \"DBLP\": \"journals/corr/LillicrapHPHETS15\", \"CorpusId\": 16326763}, \"url\": \"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\", \"title\": \"Continuous control with deep reinforcement learning\", \"abstract\": \"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2015, \"referenceCount\": 34, \"citationCount\": 8216, \"influentialCitationCount\": 1896, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\", \"Mathematics\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Mathematics\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"2542999\", \"name\": \"T. Lillicrap\"}, {\"authorId\": \"2323922\", \"name\": \"Jonathan J. Hunt\"}, {\"authorId\": \"1863250\", \"name\": \"A. Pritzel\"}, {\"authorId\": \"2801204\", \"name\": \"N. Heess\"}, {\"authorId\": \"1968210\", \"name\": \"T. Erez\"}, {\"authorId\": \"2109481\", \"name\": \"Yuval Tassa\"}, {\"authorId\": \"145824029\", \"name\": \"David Silver\"}, {\"authorId\": \"1688276\", \"name\": \"Daan Wierstra\"}]}, {\"paperId\": \"88e488a08dcd629c8ce90099bd8ab0a87f10cafb\", \"externalIds\": {\"PubMedCentral\": \"8850200\", \"DBLP\": \"journals/nature/DegraveFBNTCEHA22\", \"DOI\": \"10.1038/s41586-021-04301-9\", \"CorpusId\": 246904229, \"PubMed\": \"35173339\"}, \"url\": \"https://www.semanticscholar.org/paper/88e488a08dcd629c8ce90099bd8ab0a87f10cafb\", \"title\": \"Magnetic control of tokamak plasmas through deep reinforcement learning\", \"abstract\": null, \"venue\": \"The Naturalist\", \"year\": 2022, \"referenceCount\": 47, \"citationCount\": 144, \"influentialCitationCount\": 2, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"https://www.nature.com/articles/s41586-021-04301-9.pdf\", \"status\": \"HYBRID\"}, \"fieldsOfStudy\": [\"Computer Science\", \"Medicine\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3110620\", \"name\": \"Jonas Degrave\"}, {\"authorId\": \"48532667\", \"name\": \"F. Felici\"}, {\"authorId\": \"1741293\", \"name\": \"J. Buchli\"}, {\"authorId\": \"2366050\", \"name\": \"Michael Neunert\"}, {\"authorId\": \"143613254\", \"name\": \"Brendan D. Tracey\"}, {\"authorId\": \"2154760678\", \"name\": \"Francesco Carpanese\"}, {\"authorId\": \"23988602\", \"name\": \"Timo Ewalds\"}, {\"authorId\": \"49512734\", \"name\": \"Roland Hafner\"}, {\"authorId\": \"2799799\", \"name\": \"A. Abdolmaleki\"}, {\"authorId\": \"2154760863\", \"name\": \"Diego de Las Casas\"}, {\"authorId\": \"2154758319\", \"name\": \"Craig Donner\"}, {\"authorId\": \"2154760967\", \"name\": \"Leslie Fritz\"}, {\"authorId\": \"34766503\", \"name\": \"C. Galperti\"}, {\"authorId\": \"2054151655\", \"name\": \"Andrea Huber\"}, {\"authorId\": \"2058168486\", \"name\": \"James Keeling\"}, {\"authorId\": \"2010057\", \"name\": \"Maria Tsimpoukelli\"}, {\"authorId\": \"2059147422\", \"name\": \"Jackie Kay\"}, {\"authorId\": \"94400492\", \"name\": \"A. Merle\"}, {\"authorId\": \"144212446\", \"name\": \"J. Moret\"}, {\"authorId\": \"30155667\", \"name\": \"Seb Noury\"}, {\"authorId\": \"83757509\", \"name\": \"F. Pesamosca\"}, {\"authorId\": \"30536465\", \"name\": \"D. Pfau\"}, {\"authorId\": \"2211601\", \"name\": \"O. Sauter\"}, {\"authorId\": \"80157448\", \"name\": \"C. Sommariva\"}, {\"authorId\": \"3130796\", \"name\": \"S. Coda\"}, {\"authorId\": \"34659439\", \"name\": \"B. Duval\"}, {\"authorId\": \"66378135\", \"name\": \"A. Fasoli\"}, {\"authorId\": \"143967473\", \"name\": \"Pushmeet Kohli\"}, {\"authorId\": \"2645384\", \"name\": \"K. Kavukcuoglu\"}, {\"authorId\": \"48987704\", \"name\": \"D. Hassabis\"}, {\"authorId\": \"3137672\", \"name\": \"Martin A. Riedmiller\"}]}, {\"paperId\": \"69e76e16740ed69f4dc55361a3d319ac2f1293dd\", \"externalIds\": {\"DBLP\": \"conf/icml/MnihBMGLHSK16\", \"ArXiv\": \"1602.01783\", \"MAG\": \"2951820146\", \"CorpusId\": 6875312}, \"url\": \"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\", \"title\": \"Asynchronous Methods for Deep Reinforcement Learning\", \"abstract\": \"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.\", \"venue\": \"International Conference on Machine Learning\", \"year\": 2016, \"referenceCount\": 43, \"citationCount\": 6174, \"influentialCitationCount\": 1184, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\", \"Mathematics\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Mathematics\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3255983\", \"name\": \"Volodymyr Mnih\"}, {\"authorId\": \"36045539\", \"name\": \"Adri\\u00e0 Puigdom\\u00e8nech Badia\"}, {\"authorId\": \"153583218\", \"name\": \"Mehdi Mirza\"}, {\"authorId\": \"1753223\", \"name\": \"A. Graves\"}, {\"authorId\": \"2542999\", \"name\": \"T. Lillicrap\"}, {\"authorId\": \"3367786\", \"name\": \"Tim Harley\"}, {\"authorId\": \"145824029\", \"name\": \"David Silver\"}, {\"authorId\": \"2645384\", \"name\": \"K. Kavukcuoglu\"}]}, {\"paperId\": \"276204469f838c71ad2e33d8d3ff69c80142630d\", \"externalIds\": {\"DOI\": \"10.1155/2022/7135043\", \"CorpusId\": 248718772}, \"url\": \"https://www.semanticscholar.org/paper/276204469f838c71ad2e33d8d3ff69c80142630d\", \"title\": \"Deep Reinforcement Learning-Based Path Control and Optimization for Unmanned Ships\", \"abstract\": \"Unmanned ship navigates on the water in an autonomous or semiautonomous way, which can be widely used in maritime transportation, intelligence collection, maritime training and testing, reconnaissance, and evidence collection. In this paper, we use deep reinforcement learning to solve the optimization problem in the path planning and management of unmanned ships. Specifically, we take the waiting time (phase and duration) at the corner of the path as the optimization goal to minimize the total travel time of unmanned ships passing through the path. We propose a new reward function, which considers the environment and control delay of unmanned ships at the same time, which can reduce the coordination time between unmanned ships at the same time. In the simulation experiment, through the quantitative and qualitative results of deep reinforcement learning of unmanned ship navigation and path angle waiting, the effectiveness of our solution is verified.\", \"venue\": \"Wireless Communications and Mobile Computing\", \"year\": 2022, \"referenceCount\": 25, \"citationCount\": 55, \"influentialCitationCount\": 2, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"https://downloads.hindawi.com/journals/wcmc/2022/7135043.pdf\", \"status\": \"GOLD\"}, \"fieldsOfStudy\": null, \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"2118290355\", \"name\": \"Dingwei Wu\"}, {\"authorId\": \"2113651719\", \"name\": \"Yin Lei\"}, {\"authorId\": \"1740610090\", \"name\": \"Maoen He\"}, {\"authorId\": \"7923642\", \"name\": \"Chunjiong Zhang\"}, {\"authorId\": \"2147295565\", \"name\": \"Lili Ji\"}]}, {\"paperId\": \"811df72e210e20de99719539505da54762a11c6d\", \"externalIds\": {\"MAG\": \"2932819148\", \"DBLP\": \"conf/icml/HaarnojaZAL18\", \"ArXiv\": \"1801.01290\", \"CorpusId\": 28202810}, \"url\": \"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\", \"title\": \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\", \"abstract\": \"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.\", \"venue\": \"International Conference on Machine Learning\", \"year\": 2018, \"referenceCount\": 41, \"citationCount\": 3741, \"influentialCitationCount\": 1012, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\", \"Mathematics\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Mathematics\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"2587648\", \"name\": \"Tuomas Haarnoja\"}, {\"authorId\": \"35499972\", \"name\": \"Aurick Zhou\"}, {\"authorId\": \"1689992\", \"name\": \"P. Abbeel\"}, {\"authorId\": \"1736651\", \"name\": \"S. Levine\"}]}, {\"paperId\": \"e8847377a090f60759faf927a1c9ac6ebd74480b\", \"externalIds\": {\"DBLP\": \"journals/tnn/WangTWCWTH22\", \"DOI\": \"10.1109/TNNLS.2021.3054402\", \"CorpusId\": 231872921, \"PubMed\": \"33560993\"}, \"url\": \"https://www.semanticscholar.org/paper/e8847377a090f60759faf927a1c9ac6ebd74480b\", \"title\": \"Target Tracking Control of a Biomimetic Underwater Vehicle Through Deep Reinforcement Learning\", \"abstract\": \"In this article, the underwater target tracking control problem of a biomimetic underwater vehicle (BUV) is addressed. Since it is difficult to build an effective mathematic model of a BUV due to the uncertainty of hydrodynamics, target tracking control is converted into the Markov decision process and is further achieved via deep reinforcement learning. The system state and reward function of underwater target tracking control are described. Based on the actor\\u2013critic reinforcement learning framework, the deep deterministic policy gradient actor\\u2013critic algorithm with supervision controller is proposed. The training tricks, including prioritized experience replay, actor network indirect supervision training, target network updating with different periods, and expansion of exploration space by applying random noise, are presented. Indirect supervision training is designed to address the issues of low stability and slow convergence of reinforcement learning in the continuous state and action space. Comparative simulations are performed to show the effectiveness of the training tricks. Finally, the proposed actor\\u2013critic reinforcement learning algorithm with supervision controller is applied to the physical BUV. Swimming pool experiments of underwater object tracking of the BUV are conducted in multiple scenarios to verify the effectiveness and robustness of the proposed method.\", \"venue\": \"IEEE Transactions on Neural Networks and Learning Systems\", \"year\": 2021, \"referenceCount\": 0, \"citationCount\": 8, \"influentialCitationCount\": 0, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\", \"Medicine\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Engineering\", \"source\": \"s2-fos-model\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"2153603607\", \"name\": \"Yu Wang\"}, {\"authorId\": \"2801760\", \"name\": \"Chong Tang\"}, {\"authorId\": \"1731827\", \"name\": \"Shuo Wang\"}, {\"authorId\": \"143778813\", \"name\": \"L. Cheng\"}, {\"authorId\": \"2151036128\", \"name\": \"Rui Wang\"}, {\"authorId\": \"49720667\", \"name\": \"M. Tan\"}, {\"authorId\": \"143719145\", \"name\": \"Zengguang Hou\"}]}, {\"paperId\": \"2319a491378867c7049b3da055c5df60e1671158\", \"externalIds\": {\"DBLP\": \"journals/corr/MnihKSGAWR13\", \"ArXiv\": \"1312.5602\", \"MAG\": \"1757796397\", \"CorpusId\": 15238391}, \"url\": \"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\", \"title\": \"Playing Atari with Deep Reinforcement Learning\", \"abstract\": \"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\", \"venue\": \"ArXiv\", \"year\": 2013, \"referenceCount\": 32, \"citationCount\": 8152, \"influentialCitationCount\": 1319, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3255983\", \"name\": \"Volodymyr Mnih\"}, {\"authorId\": \"2645384\", \"name\": \"K. Kavukcuoglu\"}, {\"authorId\": \"145824029\", \"name\": \"David Silver\"}, {\"authorId\": \"1753223\", \"name\": \"A. Graves\"}, {\"authorId\": \"2460849\", \"name\": \"Ioannis Antonoglou\"}, {\"authorId\": \"1688276\", \"name\": \"Daan Wierstra\"}, {\"authorId\": \"3137672\", \"name\": \"Martin A. Riedmiller\"}]}, {\"paperId\": \"d50dc64f2ba8be1733a8daad143874daec06b00f\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2012-15427\", \"ArXiv\": \"2012.15427\", \"DOI\": \"10.1109/TNNLS.2022.3153502\", \"CorpusId\": 229923643, \"PubMed\": \"35263262\"}, \"url\": \"https://www.semanticscholar.org/paper/d50dc64f2ba8be1733a8daad143874daec06b00f\", \"title\": \"Curriculum-based Deep Reinforcement Learning for Quantum Control\", \"abstract\": \"Deep reinforcement learning (DRL) has been recognized as an efficient technique to design optimal strategies for different complex systems without prior knowledge of the control landscape. To achieve a fast and precise control for quantum systems, we propose a novel DRL approach by constructing a curriculum consisting of a set of intermediate tasks defined by fidelity thresholds, where the tasks among a curriculum can be statically determined before the learning process or dynamically generated during the learning process. By transferring knowledge between two successive tasks and sequencing tasks according to their difficulties, the proposed curriculum-based DRL (CDRL) method enables the agent to focus on easy tasks in the early stage, then move onto difficult tasks, and eventually approaches the final task. Numerical comparison with the traditional methods [gradient method (GD), genetic algorithm (GA), and several other DRL methods] demonstrates that CDRL exhibits improved control performance for quantum systems and also provides an efficient way to identify optimal strategies with few control pulses.\", \"venue\": \"IEEE Transactions on Neural Networks and Learning Systems\", \"year\": 2020, \"referenceCount\": 50, \"citationCount\": 9, \"influentialCitationCount\": 0, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"http://arxiv.org/pdf/2012.15427\", \"status\": \"GREEN\"}, \"fieldsOfStudy\": [\"Computer Science\", \"Medicine\", \"Physics\", \"Engineering\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Physics\", \"source\": \"external\"}, {\"category\": \"Engineering\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3319957\", \"name\": \"Hailan Ma\"}, {\"authorId\": \"3053987\", \"name\": \"D. Dong\"}, {\"authorId\": \"144558270\", \"name\": \"S. Ding\"}, {\"authorId\": \"4520850\", \"name\": \"Chunlin Chen\"}]}, {\"paperId\": \"d5829752b83a31b539865d140cbaccc55ccb481e\", \"externalIds\": {\"DBLP\": \"journals/tits/YangZW21\", \"MAG\": \"3091266930\", \"DOI\": \"10.1109/TITS.2020.3023788\", \"CorpusId\": 226623447}, \"url\": \"https://www.semanticscholar.org/paper/d5829752b83a31b539865d140cbaccc55ccb481e\", \"title\": \"Urban Traffic Control in Software Defined Internet of Things via a Multi-Agent Deep Reinforcement Learning Approach\", \"abstract\": \"As the growth of vehicles and the acceleration of urbanization, the urban traffic congestion problem becomes a burning issue in our society. Constructing a software defined Internet of things(SD-IoT) with a proper traffic control scheme is a promising solution for this issue. However, existing traffic control schemes do not make the best of the advances of the multi-agent deep reinforcement learning area. Furthermore, existing traffic congestion solutions based on deep reinforcement learning(DRL) only focus on controlling the signal of traffic lights, while ignore controlling vehicles to cooperate traffic lights. So the effect of urban traffic control is not comprehensive enough. In this article, we propose Modified Proximal Policy Optimization (Modified PPO) algorithm. This algorithm is ideally suited as the traffic control scheme of SD-IoT. We adaptively adjust the clip hyperparameter to limit the bound of the distance between the next policy and the current policy. What\\u2019s more, based on the collected data of SD-IoT, the proposed algorithm controls traffic lights and vehicles in a global view to advance the performance of urban traffic control. Experimental results under different vehicle numbers show that the proposed method is more competitive and stable than the original algorithm. Our proposed method improves the performance of SD-IoT to relieve traffic congestion.\", \"venue\": \"IEEE transactions on intelligent transportation systems (Print)\", \"year\": 2021, \"referenceCount\": 61, \"citationCount\": 58, \"influentialCitationCount\": 0, \"isOpenAccess\": false, \"openAccessPdf\": null, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"authors\": [{\"authorId\": \"3021550\", \"name\": \"Jiachen Yang\"}, {\"authorId\": \"2108076196\", \"name\": \"Jipeng Zhang\"}, {\"authorId\": \"2108868841\", \"name\": \"Huihui Wang\"}]}]}\n","https://export.arxiv.org/api/query?id_list=1905.11946&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1905.11946%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1905.11946&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/XCNwg98w84MUW+L7c/lBCLmWT0k</id>\n  <updated>2023-01-15T00:00:00-05:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1905.11946v5</id>\n    <updated>2020-09-11T05:08:01Z</updated>\n    <published>2019-05-28T17:05:32Z</published>\n    <title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>\n    <summary>  Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\n</summary>\n    <author>\n      <name>Mingxing Tan</name>\n    </author>\n    <author>\n      <name>Quoc V. Le</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ICML 2019</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Machine Learning, 2019</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/1905.11946v5\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.11946v5\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n","https://api.semanticscholar.org/graph/v1/paper/search?query=imagenet&offset=0&limit=10&fields=externalIds%2Curl%2Ctitle%2Cvenue%2Cyear%2CcitationCount%2Cauthors%2Cabstract":"{\"total\": 21165, \"offset\": 0, \"next\": 10, \"data\": [{\"paperId\": \"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"externalIds\": {\"DBLP\": \"journals/corr/HeZR015\", \"MAG\": \"2949608135\", \"ArXiv\": \"1502.01852\", \"DOI\": \"10.1109/ICCV.2015.123\", \"CorpusId\": 13740328}, \"url\": \"https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"title\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"abstract\": \"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2015, \"citationCount\": 14251, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}, {\"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"externalIds\": {\"DBLP\": \"journals/corr/RussakovskyDSKSMHKKBBF14\", \"MAG\": \"2117539524\", \"ArXiv\": \"1409.0575\", \"DOI\": \"10.1007/s11263-015-0816-y\", \"CorpusId\": 2930547}, \"url\": \"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\", \"title\": \"ImageNet Large Scale Visual Recognition Challenge\", \"abstract\": null, \"venue\": \"International Journal of Computer Vision\", \"year\": 2014, \"citationCount\": 28947, \"authors\": [{\"authorId\": \"2192178\", \"name\": \"Olga Russakovsky\"}, {\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144914140\", \"name\": \"Hao Su\"}, {\"authorId\": \"2285165\", \"name\": \"J. Krause\"}, {\"authorId\": \"145031342\", \"name\": \"S. Satheesh\"}, {\"authorId\": \"145423516\", \"name\": \"S. Ma\"}, {\"authorId\": \"3109481\", \"name\": \"Zhiheng Huang\"}, {\"authorId\": \"2354728\", \"name\": \"A. Karpathy\"}, {\"authorId\": \"2556428\", \"name\": \"A. Khosla\"}, {\"authorId\": \"145879842\", \"name\": \"Michael S. Bernstein\"}, {\"authorId\": \"39668247\", \"name\": \"A. Berg\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"externalIds\": {\"MAG\": \"2997031122\", \"DBLP\": \"conf/nips/KrizhevskySH12\", \"DOI\": \"10.1145/3065386\", \"CorpusId\": 195908774}, \"url\": \"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\", \"title\": \"ImageNet classification with deep convolutional neural networks\", \"abstract\": \"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \\\"dropout\\\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\", \"venue\": \"Communications of the ACM\", \"year\": 2012, \"citationCount\": 92437, \"authors\": [{\"authorId\": \"2064160\", \"name\": \"A. Krizhevsky\"}, {\"authorId\": \"1701686\", \"name\": \"Ilya Sutskever\"}, {\"authorId\": \"1695689\", \"name\": \"Geoffrey E. Hinton\"}]}, {\"paperId\": \"d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"externalIds\": {\"DBLP\": \"conf/cvpr/DengDSLL009\", \"MAG\": \"2108598243\", \"DOI\": \"10.1109/CVPR.2009.5206848\", \"CorpusId\": 57246310}, \"url\": \"https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e\", \"title\": \"ImageNet: A large-scale hierarchical image database\", \"abstract\": \"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \\u201cImageNet\\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.\", \"venue\": \"2009 IEEE Conference on Computer Vision and Pattern Recognition\", \"year\": 2009, \"citationCount\": 39659, \"authors\": [{\"authorId\": \"153302678\", \"name\": \"Jia Deng\"}, {\"authorId\": \"144847596\", \"name\": \"Wei Dong\"}, {\"authorId\": \"2166511\", \"name\": \"R. Socher\"}, {\"authorId\": \"2040091191\", \"name\": \"Li-Jia Li\"}, {\"authorId\": \"94451829\", \"name\": \"K. Li\"}, {\"authorId\": \"48004138\", \"name\": \"Li Fei-Fei\"}]}, {\"paperId\": \"0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"externalIds\": {\"ArXiv\": \"1706.02677\", \"MAG\": \"2622263826\", \"DBLP\": \"journals/corr/GoyalDGNWKTJH17\", \"CorpusId\": 13905106}, \"url\": \"https://www.semanticscholar.org/paper/0d57ba12a6d958e178d83be4c84513f7e42b24e5\", \"title\": \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\", \"abstract\": \"Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.\", \"venue\": \"ArXiv\", \"year\": 2017, \"citationCount\": 2574, \"authors\": [{\"authorId\": \"47316088\", \"name\": \"Priya Goyal\"}, {\"authorId\": \"3127283\", \"name\": \"Piotr Doll\\u00e1r\"}, {\"authorId\": \"2983898\", \"name\": \"Ross B. Girshick\"}, {\"authorId\": \"34837514\", \"name\": \"P. Noordhuis\"}, {\"authorId\": \"2065373815\", \"name\": \"Lukasz Wesolowski\"}, {\"authorId\": \"1717990\", \"name\": \"Aapo Kyrola\"}, {\"authorId\": \"3609856\", \"name\": \"Andrew Tulloch\"}, {\"authorId\": \"39978391\", \"name\": \"Yangqing Jia\"}, {\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}]}, {\"paperId\": \"b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"externalIds\": {\"DBLP\": \"journals/corr/RastegariORF16\", \"ArXiv\": \"1603.05279\", \"MAG\": \"2951978180\", \"DOI\": \"10.1007/978-3-319-46493-0_32\", \"CorpusId\": 14925907}, \"url\": \"https://www.semanticscholar.org/paper/b649a98ce77ece8cd7638bb74ab77d22d9be77e7\", \"title\": \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\", \"abstract\": null, \"venue\": \"European Conference on Computer Vision\", \"year\": 2016, \"citationCount\": 3057, \"authors\": [{\"authorId\": \"143887493\", \"name\": \"Mohammad Rastegari\"}, {\"authorId\": \"2004053\", \"name\": \"Vicente Ordonez\"}, {\"authorId\": \"40497777\", \"name\": \"Joseph Redmon\"}, {\"authorId\": \"143787583\", \"name\": \"Ali Farhadi\"}]}, {\"paperId\": \"dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-2101-11986\", \"ArXiv\": \"2101.11986\", \"DOI\": \"10.1109/ICCV48922.2021.00060\", \"CorpusId\": 231719476}, \"url\": \"https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9\", \"title\": \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\", \"abstract\": \"Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet.1\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2021, \"citationCount\": 748, \"authors\": [{\"authorId\": \"2087091296\", \"name\": \"Li Yuan\"}, {\"authorId\": \"2144861793\", \"name\": \"Yunpeng Chen\"}, {\"authorId\": \"143988955\", \"name\": \"Tao Wang\"}, {\"authorId\": \"23476952\", \"name\": \"Weihao Yu\"}, {\"authorId\": \"145356288\", \"name\": \"Yujun Shi\"}, {\"authorId\": \"40983412\", \"name\": \"Francis E. H. Tay\"}, {\"authorId\": \"33221685\", \"name\": \"Jiashi Feng\"}, {\"authorId\": \"143653681\", \"name\": \"Shuicheng Yan\"}]}, {\"paperId\": \"20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"externalIds\": {\"DBLP\": \"conf/cvpr/XieLHL20\", \"MAG\": \"2985963903\", \"ArXiv\": \"1911.04252\", \"DOI\": \"10.1109/cvpr42600.2020.01070\", \"CorpusId\": 207853355}, \"url\": \"https://www.semanticscholar.org/paper/20ba55ee3229db5cb190a00e788c59f08d2a767d\", \"title\": \"Self-Training With Noisy Student Improves ImageNet Classification\", \"abstract\": \"We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.\", \"venue\": \"Computer Vision and Pattern Recognition\", \"year\": 2019, \"citationCount\": 1370, \"authors\": [{\"authorId\": \"1912046\", \"name\": \"Qizhe Xie\"}, {\"authorId\": \"144547315\", \"name\": \"E. Hovy\"}, {\"authorId\": \"1707242\", \"name\": \"Minh-Thang Luong\"}, {\"authorId\": \"2827616\", \"name\": \"Quoc V. Le\"}]}, {\"paperId\": \"0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"externalIds\": {\"DBLP\": \"journals/corr/abs-1811-12231\", \"ArXiv\": \"1811.12231\", \"MAG\": \"2902617128\", \"CorpusId\": 54101493}, \"url\": \"https://www.semanticscholar.org/paper/0f50b7483f1b200ebf88c4dd7698de986399a0f3\", \"title\": \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\", \"abstract\": \"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \\\"Stylized-ImageNet\\\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.\", \"venue\": \"International Conference on Learning Representations\", \"year\": 2018, \"citationCount\": 1577, \"authors\": [{\"authorId\": \"1949747\", \"name\": \"Robert Geirhos\"}, {\"authorId\": \"52096618\", \"name\": \"Patricia Rubisch\"}, {\"authorId\": \"40899528\", \"name\": \"Claudio Michaelis\"}, {\"authorId\": \"1731199\", \"name\": \"M. Bethge\"}, {\"authorId\": \"1924112\", \"name\": \"Felix Wichmann\"}, {\"authorId\": \"40634590\", \"name\": \"Wieland Brendel\"}]}, {\"paperId\": \"d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"externalIds\": {\"MAG\": \"2952681253\", \"DBLP\": \"conf/cvpr/HaraKS18\", \"ArXiv\": \"1711.09577\", \"DOI\": \"10.1109/CVPR.2018.00685\", \"CorpusId\": 4539700}, \"url\": \"https://www.semanticscholar.org/paper/d716435f0cb0cac56237f74b1ced940aabce6a2b\", \"title\": \"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\", \"abstract\": \"The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available1.\", \"venue\": \"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\", \"year\": 2017, \"citationCount\": 1318, \"authors\": [{\"authorId\": \"2199251\", \"name\": \"Kensho Hara\"}, {\"authorId\": \"1730200\", \"name\": \"Hirokatsu Kataoka\"}, {\"authorId\": \"1732705\", \"name\": \"Y. Satoh\"}]}]}\n","https://api.semanticscholar.org/graph/v1/paper/arxiv:1502.01852?fields=paperId,externalIds,url,title,abstract,venue,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy,s2FieldsOfStudy,openAccessPdf,authors,tldr":"{\"paperId\": \"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"externalIds\": {\"DBLP\": \"journals/corr/HeZR015\", \"MAG\": \"2949608135\", \"ArXiv\": \"1502.01852\", \"DOI\": \"10.1109/ICCV.2015.123\", \"CorpusId\": 13740328}, \"url\": \"https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23\", \"title\": \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", \"abstract\": \"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.\", \"venue\": \"IEEE International Conference on Computer Vision\", \"year\": 2015, \"referenceCount\": 45, \"citationCount\": 14251, \"influentialCitationCount\": 1113, \"isOpenAccess\": true, \"openAccessPdf\": {\"url\": \"http://arxiv.org/pdf/1502.01852\", \"status\": \"GREEN\"}, \"fieldsOfStudy\": [\"Computer Science\"], \"s2FieldsOfStudy\": [{\"category\": \"Computer Science\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}], \"tldr\": {\"model\": \"tldr@v2.0.0\", \"text\": \"This work proposes a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit and derives a robust initialization method that particularly considers the rectifier nonlinearities.\"}, \"authors\": [{\"authorId\": \"39353098\", \"name\": \"Kaiming He\"}, {\"authorId\": \"1771551\", \"name\": \"X. Zhang\"}, {\"authorId\": \"3080683\", \"name\": \"Shaoqing Ren\"}, {\"authorId\": null, \"name\": \"Jian Sun\"}]}\n","https://export.arxiv.org/api/query?id_list=1502.01852&start=0&max_results=10&sortBy=relevance":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1502.01852%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1502.01852&amp;start=0&amp;max_results=10</title>\n  <id>http://arxiv.org/api/W0Cna2dncsQSLR3Sq7oPiNgKfyI</id>\n  <updated>2023-01-15T00:00:00-05:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1502.01852v1</id>\n    <updated>2015-02-06T10:44:00Z</updated>\n    <published>2015-02-06T10:44:00Z</published>\n    <title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification</title>\n    <summary>  Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge.\n</summary>\n    <author>\n      <name>Kaiming He</name>\n    </author>\n    <author>\n      <name>Xiangyu Zhang</name>\n    </author>\n    <author>\n      <name>Shaoqing Ren</name>\n    </author>\n    <author>\n      <name>Jian Sun</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1502.01852v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1502.01852v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"}